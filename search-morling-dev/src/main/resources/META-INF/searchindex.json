[{"content":"","id":0,"publicationdate":"Nov 3, 2022","section":"blog","summary":"","tags":null,"title":"Blogs","uri":"https://www.morling.dev/blog/"},{"content":"","id":1,"publicationdate":"Nov 3, 2022","section":"","summary":"","tags":null,"title":"Gunnar Morling","uri":"https://www.morling.dev/"},{"content":"It\u0026#8217;s my first week as a software engineer at Decodable, a start-up building a serverless real-time data platform! When I shared this news on social media yesterday, folks were not only super supportive and excited for me (thank you so much for all the nice words and wishes!), but some also asked about the reasons behind my decision for switching jobs and going to a start-up, after having worked for Red Hat for the last few years. That\u0026#8217;s a great question indeed, and I thought I\u0026#8217;d put down some thoughts in this post. To me, it boils down to three key aspects: the general field of work, the environment, and the team. In the following, I\u0026#8217;ll drill a bit further into each of them.\n The Space: Real-Time Stream Processing Over the last five years, I\u0026#8217;ve worked on Debezium, a popular open-source platform for change data capture (CDC). It retrieves change events from the transaction logs of databases such as MySQL and Postgres and emits them in a uniform event format to consumers via data streaming platforms like Apache Kafka, Pulsar or Amazon Kinesis. Reacting to low-latency change events enables all kinds of very interesting use cases, ranging from replication to other databases or (cloud) data warehouses, over updating caches and search indexes, to continuous queries over your operational data, or migrating monolithic architectures to microservices.\n Now, CDC is an important part of data pipelines for implementing such use cases, but it\u0026#8217;s not the only one. You need to reason about the sink side of your pipelines and how to get your data from the streaming platform into your target system. There\u0026#8217;s many critical questions there, such as: How do you propagate type metadata? How do you handle changes to the schema of your data? How to deal with duplicate events? Another concern is processing data, as it flows through your pipelines; you might want to filter records based on specific criteria and patterns, apply format conversions, group and aggregate data, join multiple data streams, and more. Lastly, there\u0026#8217;s many other kinds of data sources besides CDC, such as sensor data in IoT scenarios, click streams from websites, APIs, and more.\n This all is to say that I am really excited about the chance to take things to the next level and explore the field of stream processing at large, helping people to implement their streaming use cases end-to-end. Very often, once people have implemented their first low-latency data streaming use case and for instance observe data in their DWH within a second or two after a change has occurred in their operational database, there\u0026#8217;s no going back, and they want this for everything. Of course it\u0026#8217;s impossible to predict the future, but I think stream processing is at this point on the famous \"hockey stick\" curve right before it\u0026#8217;s massively taking off, and it\u0026#8217;s the perfect time to join this space.\n From a technical perspective, Apache Flink, an open-source \"processing engine for stateful computations over unbounded and bounded data streams\" is an excellent and proven foundation for this, and it\u0026#8217;s a core technology behind Decodable. Getting my feet wet with Flink, learning about it in detail and hopefully contributing to it is one of the first things I\u0026#8217;m planning to do. At the same time, I also think there\u0026#8217;s lots of potential for further improving the user experience here, for instance by processing CDC events in a transactional way, smoothly handling schema changes, and much more. Exciting times!\n   The Environment: A Start-up Up to this point, I have mostly worked at large, established companies and enterprises during my career. Red Hat, where I\u0026#8217;ve been at for the last ten years, grew to more than 20,000 employees during that time. Other places, like the German e-commerce giant Otto Group, had even larger workforce sizes of 50,000 people and more.\n As with everything, being with such a large company has its pros and cons. On the upside, it\u0026#8217;s a relatively safe bet, there\u0026#8217;s brand recognition, you can approach and tackle huge undertakings as part of a big organization. At the same time, there tends to be quite a bit of process overhead, things can take a long time, there can be lots of politics, you need approval and buy-in for many things, etc. Note I am not saying that any of this is necessarily a bad thing (Ok, doing your travel expenses just sucks. Period.), lots of it makes sense and just is a reality in a large organization.\n That all being said, I just felt that I want to gather some experience in a small environment, in a start-up company. I want to find out how it is to work in this kind of setting, being part of a small, hyper-focused team, working jointly towards one common goal and shared vision. Coming up with ideas, giving it a try, seeing what flies, and what doesn\u0026#8217;t. Putting something minimal yet useful out there and quickly gathering user feedback. Having a good sense for your own impact. Seeing how the company grows and evolves. That\u0026#8217;s the kind of sensation I am looking for and which I am hoping to find by working at Decodable.\n I could experience a first taste of the agility even before my first day at the company: \"Would you feel comfortable to just buy a laptop of your choosing by yourself and expense it?\" Sure thing! Some clicks and a few days later I had a very nice MacBook delivered to my doorstep. If you\u0026#8217;ve been at bigger organizations, you\u0026#8217;ll know how complicated such seemingly simple things like getting a new laptop can be.\n At the same time, judging by my impressions during interviews, Decodable is a very mature start-up. Most folks have lots of experience, they are senior, in a very positive sense. Sure, there\u0026#8217;s a ton of things on our plates, but there\u0026#8217;s no expectation to work crazy hours. Many people here have families, and there\u0026#8217;s a very healthy culture where it\u0026#8217;s just normal that people have unforeseen situations where they need to pick up their kids on short notice, things like that. People are treated as the grown-ups they are, with lots of autonomy and trust by the leadership. Another key aspect for me is transparency: it\u0026#8217;s one of the company\u0026#8217;s core values, so everyone has the chance to know what\u0026#8217;s going on (technically, business-wise, etc.), which gave me lots of confidence and trust when making the decision to join the team.\n   The Team: One of a Kind One of the clichés in the industry is: \"It\u0026#8217;s all about the people\". And yes, it is a cliché, but I\u0026#8217;m also 100% convinced that it is true. You could work on the most amazing piece of technology, but if you don\u0026#8217;t get along with the people around you, it won\u0026#8217;t be an awful lot of fun. Or rather, it could be really bad.\n So getting a vibe for the team and the people at Decodable was one of the most important things to me when I interviewed with them. And all I can say is that I was really impressed. Starting with the founder and CEO Eric Sammer, I had the opportunity to speak with about one third of the company\u0026#8217;s employees during the interviewing process (talking to everyone is one of my personal onboarding goals, when do you ever get that chance?). I loved the passion, but also the respectfulness and sincereness of everyone. Needless to say that I\u0026#8217;m deeply impressed with what the team has accomplished so far, since Decodable launched last year. I experienced Eric as a very considerate and mindful person, caring deeply about the concerns of the company\u0026#8217;s employees. Plus, not only is he a legend in the data space, he\u0026#8217;s also super well connected within Silicon Valley, opening up lots of doors for the company. Decodable being his second start-up will surely help us to avoid many mistakes.\n In regards to the hiring process itself, it could probably be a topic for a separate blog post. The experience was nothing but excellent, with everyone being very open and transparent, willing to answer any questions I had. It really wasn\u0026#8217;t that much of a series of interviews, but rather really good two-way conversations which helped us to get to know each other and find out whether I would be a good fit for Decodable, and whether the company would be a good fit for me. All in all, I very quickly had a feeling that this is a group of people I want to work with. I\u0026#8217;m sure the direction of the company and the product can and will be adjusted over time, but this is a team I can\u0026#8217;t wait to work with to make this a success.\n   Outlook So those are the three key reasons which made me join Decodable: the exciting field of data streaming, the start-up environment, and a highly competent and friendly team.\n In case you\u0026#8217;re wondering what exactly I will be doing – that\u0026#8217;s something we\u0026#8217;re still figuring out. I am a member of the engineering organization, so I will get my fingers onto Apache Flink, but of course also on Decodable\u0026#8217;s SaaS product around it. But I\u0026#8217;m also planning to continue my fair share of evangelization work and talk about technology and its applications in blog posts or conference sessions. I hope to share my input on the product, be part of customer conversations, and much more. For the beginning, I\u0026#8217;ll mostly focus on learning and sharing feedback based on my perspective of being the \"new guy\" on the team.\n Fully adhering to the start-up spirit, I\u0026#8217;m sure things will be very much in flux and my responsibilities will shift over time. But that dynamic is exactly what I\u0026#8217;m looking for by joining Decodable. Let\u0026#8217;s do this!\n   ","id":2,"publicationdate":"Nov 3, 2022","section":"blog","summary":"It\u0026#8217;s my first week as a software engineer at Decodable, a start-up building a serverless real-time data platform! When I shared this news on social media yesterday, folks were not only super supportive and excited for me (thank you so much for all the nice words and wishes!), but some also asked about the reasons behind my decision for switching jobs and going to a start-up, after having worked for Red Hat for the last few years.","tags":null,"title":"Why I Joined Decodable","uri":"https://www.morling.dev/blog/why-i-joined-decodable/"},{"content":"Kafka Connect, part of the Apache Kafka project, is a development framework and runtime for connectors which either ingest data into Kafka clusters (source connectors) or propagate data from Kafka into external systems (sink connectors). A diverse ecosystem of ready-made connectors has come to life on top of Kafka Connect, which lets you connect all kinds of data stores, APIs, and other systems to Kafka in a no-code approach.\n With the continued move towards running software in the cloud and on Kubernetes in particular, it\u0026#8217;s just natural that many folks also try to run Kafka Connect on Kubernetes. On first thought, this should be simple enough: just take the Connect binary and some connector(s), put them into a container image, and schedule it for execution on Kubernetes. As so often, the devil is in the details though: should you use Connect\u0026#8217;s standalone or distributed mode? How can you control the lifecycle of specific connectors via the Kubernetes control plane? How to make sure different connectors don\u0026#8217;t compete unfairly on resources such as CPU, RAM, or network bandwidth? In the remainder of this blog post, I\u0026#8217;d like to explore running Kafka Connect on Kubernetes, what some of the challenges are for doing so, and how Kafka Connect could potentially be reimagined to become more \"Kubernetes-friendly\" in the future.\n Standalone or Distributed? If you\u0026#8217;ve used Kafka Connect before, then you\u0026#8217;ll know that it has two modes of execution: standalone and distributed. In the former, you configure Connect via property files which you pass as parameters during launch. There will be a single process which executes all the configured connectors and their tasks. In distributed mode, multiple Kafka Connect worker nodes running on different machines form a cluster onto which the workload of the connectors and their tasks is distributed. Configuration is done via a REST API which is exposed on all the worker nodes. Internally, A Connect-specific protocol (which itself is based on Kafka\u0026#8217;s group membership protocol) is used for the purposes of coordination and task assignment.\n The distributed mode is in general the preferred and recommended mode of operating Connect in production, due to its obvious advantages in regards to scalability (one connector can spawn many tasks which are executed on different machines), reliability (connector configuration and offset state is stored in Kafka topics rather than files in the local file system), and fault tolerance (if one worker node crashes, the tasks which were scheduled on that node can be transparently rebalanced to other members of the Connect cluster).\n That\u0026#8217;s why also Kafka users on Kubernetes typically opt for Connect\u0026#8217;s distributed mode, as for instance it\u0026#8217;s the case with Strimzi\u0026#8217;s operator for Kafka Connect. But that\u0026#8217;s not without its issues either, as now essentially two scheduling systems are competing with each other: Kubernetes itself (scheduling pods to compute nodes), and Connect\u0026#8217;s worker coordination mechanism (scheduling connector tasks to Connect worker nodes). This becomes particularly apparent in case of node failures. Should the Kubernetes scheduler spin up the affected pods on another node in the Kubernetes cluster, or should you rely on Connect to schedule the affected tasks to another Connect worker node? Granted, improvements in this area have been made, for instance in form of Kafka improvement proposal KIP-415 (\"Incremental Cooperative Rebalancing in Kafka Connect\"). It adds a new configuration property scheduled.rebalance.max.delay.ms, allowing you to defer rebalances after worker failures. But such a setting will always be a trade-off, and I think in general it\u0026#8217;s fair to say that if there\u0026#8217;s multiple components in a system which share the same responsibility (placement of workloads), that\u0026#8217;s likely going to be a friction point.\n   Issues with Kafka Connect on Kubernetes So let\u0026#8217;s explore a bit more the challenges users often encounter when running Kafka Connect on Kubernetes. One general problem is the lack of awareness for running on Kubernetes from a Connect perspective.\n For instance, consider the case of a stretched Kubernetes cluster, with Kubernetes nodes running in different regions of a cloud provider, or within different data centers. Let\u0026#8217;s assume you have a source connector which ingests data from a database running within one of the regions. As you\u0026#8217;re only interested in a subset of the records produced by that connector, you use a Kafka Connect single message transformation for filtering out a significant number of records. In that scenario, it makes sense to deploy that connector in local proximity to the database it connects to, so as to limit the data that\u0026#8217;s transferred across network boundaries. But Kafka Connect doesn\u0026#8217;t have any understanding of \"regions\" or related Kubernetes concepts like node selectors or node pools, i.e. you\u0026#8217;ll lack the control needed for making sure that the tasks of that connector get scheduled onto Connect worker nodes running on the right Kubernetes nodes (a mitigation strategy would be to set up multiple Connect clusters, tied to specific Kubernetes node pools in the different regions).\n A second big source of issues is Connect\u0026#8217;s model for the deployment of connectors, which in a way resembles the approach taken by Java application servers in the past: multiple, independent connectors are deployed and executed in shared JVM processes. This results in a lack of isolation between connectors, which can have far-reaching consequences in production scenarios:\n   Connectors compete on resources: one connector or task can use up an unfairly large share of CPU, RAM or networking resources assigned to a pod, so that other connectors running on the same Connect worker will be negatively impacted; this could be caused by bugs or poor programming, but it also can simply be a result of different workload requirements, with one connector requiring more resources than others. While a rate limiting feature for Connect is being proposed via KIP-731 (which may eventually address the issue of distributing network resources more fairly), there\u0026#8217;s no satisfying answer for assigning and limiting CPU and RAM resources when running multiple connectors on one shared JVM, due to its lack of application isolation.\n  Scaling complexities: when increasing the number of tasks of a connector (so as to scale out its load), it\u0026#8217;s likely also necessary to increase the number of Connect workers, unless there were idle workers before; this process seems more complex and at the same time less powerful than it should be. For instance, there\u0026#8217;s no way for ensuring that additional worker nodes would exclusively be used for the tasks of one particularly demanding connector.\n  Security implications: as per the OpenJDK Vulnerability Group, \"speculative execution vulnerabilities (e.g., Meltdown, Spectre, and RowHammer) cannot be addressed in the JDK. These hardware design flaws make complete intra-process isolation impossible\". Malicious connectors could leverage these attack vectors for instance to obtain secrets from other connectors running on the same JVM. Furthermore, some connectors rely on secrets (such as cloud SDK credentials) to be provided in the form of environment variables or Java system properties, which by definition are accessible by all connectors scheduled on the same Connect worker node.\n  Risk of resource leaks : Incorrectly implemented connectors can cause memory and thread leaks after they were stopped, resulting in out-of-memory errors after stopping and restarting them several times, potentially impacting other connectors and tasks running on the same Connect worker node.\n  Can\u0026#8217;t use Kubernetes health checks: as health checks (such as liveness probes) work on the container level, a failed health check would restart the container, and thus Connect worker node with all its connectors, even if only one connector is actually failing. On the other hand, when relying on Connect itself to restart failed connectors and/or tasks, that\u0026#8217;s not visible at the level of the Kubernetes control plane, resulting potentially in a false impression of a good health status of a connector, while it actually is in a restarting loop.\n  Can\u0026#8217;t easily examine logs of a single connector: When examining the logs of a Kafka Connect pod, messages from multiple running connectors will potentially show up in an interweaved way, depending on the specific logger configurations; as log messages can be prefixed with the connector name, that\u0026#8217;s not that much of an issue when analyzing logs in dedicated tools like Logstash or Splunk, but it can be challenging when looking at the raw pod logs on the command line or via a Kubernetes web console.\n  Can\u0026#8217;t run multiple versions of one connector: As connectors are solely identified by their classname, it\u0026#8217;s not possible to set up a connector instance of a specific version in case there\u0026#8217;s multiple versions of that connector present.\n   Lastly, a third category of issues with running Connect on Kubernetes stems from the inherently mutable design of the system and the ability to dynamically instantiate and reconfigure connectors at runtime via a REST API.\n Without proper discipline, this can quickly lead to a lack of insight into the connector configuration applying at a given time (in Strimzi, this is solved by preferrably deploying connectors via custom Kubernetes resources, rather than invoking the REST API directly). In fact, the REST API itself can be a source of issues: access to it needs to be secured in production use cases, also I\u0026#8217;ve come across multiple reports over the years (and witnessed myself) where the REST API became unresponsive, while Connect itself still was running. It\u0026#8217;s not exactly clear why this happened, but one potential course could be a buggy connector, consuming 100% of CPU cycles, leaving not enough resources for the REST API worker threads. Essentially, I think that such a control plane element like a REST API shouldn\u0026#8217;t really be exposed on each member of a data plane, as represented by Connect worker nodes.\n Based on all these challenges, in particular those around lacking isolation between different connectors, many users of Kafka Connect stick to the practice of actually not deploying multiple connectors into shared worker clusters, but instead operate a dedicated cluster of Kafka Connect for each connector. This could be a cluster with a node count equal to the configured number of tasks, essentially resulting in 1:1 mapping of tasks to worker processes. Some users also deploy a number of spare workers for fail-over purposes. In fact, that\u0026#8217;s the recommendation we\u0026#8217;ve been giving to users in the Debezium community for a long time, and it also tends to be a common choice amongst providers of managed Kafka Connect services. Another approach taken by some teams is to deploy specific Connect clusters per connector type , preventing interferences between different kinds of connectors.\n All these strategies can help to run connectors for running connectors safely and reliably, but the operational overhead of running multiple Connect clusters is evident.\n   A Vision for Kubernetes-native Kafka Connect Having explored the potential issues with running Kafka Connect on Kubernetes, let\u0026#8217;s finally discuss how Connect could be reimagined for being more Kubernetes-friendly. What are the parts that could remain? Which things would have to change? Many of the questions and shortcomings raised above – such as workload isolation, applying resource constraints, capability-based scheduling, lifecycle management – have been solved by Kubernetes at the pod level already, so how could that foundation be leveraged for Kafka Connect?\n To put a disclaimer first: this part of this post may be a bit dissatisfying to read for some, as it merely describes an idea, I haven\u0026#8217;t actually implemented any of this. My line of thinking is to hopefully ignite a discussion in the community and gauge the general level of interest, perhaps even motivating someone in the community to follow through and make this a reality. At least, that\u0026#8217;s the plan :)\n The general idea is to keep all the actual runtime bits and pieces of Connect: that\u0026#8217;s key to being able to run all the amazing existing connectors out there, which are implemented against Connect\u0026#8217;s framework interfaces. All the semantics and behaviors, like converters and SMTs, retries, dead-letter queue support, the upcoming exactly-once support for source connectors (KIP-618), all that could just be used as is.\n But the entire layer for forming and coordinating clusters of worker nodes and distributing tasks amongst them would be replaced by a Kubernetes operator. To quote the official docs, \"operators are software extensions to Kubernetes that make use of custom resources to manage applications and their components. Operators follow Kubernetes principles, notably the control loop\". The overall architecture would look like this:\n \n In this envisioned model for Kafka Connect, such an operator  would spin up one separate Kubernetes pod (and thus JVM process) for each connector task  of a connector. Conceptually, those task processes would be somewhat of a mixture between today\u0026#8217;s Connect standalone and distributed modes. Like standalone mode in the sense, that there would be no coordination amongst worker nodes and also no capability to dynamically reconfigure or start and stop a running task; each process/pod would run exactly one task in isolation, coordinated by the operator. Similar to distributed mode in the sense, that there would be a read-only REST API for health information, and that connector offsets would be stored in a Kafka topic, so as to avoid any pod-local state. There wouldn\u0026#8217;t be the need for the configuration topic though, as the configuration would be passed upon start-up to the task pods (again akin to standalone mode today, e.g. by mapping a properties file to the pod), with the custom Kubernetes resources defining the connectors being the \"system of record\" for their configuration.\n For this to work, the connector configuration needs to be pre-sliced into task-specific chunks. This could happen in two different ways, depending on the implementation of the specific connectors. For connectors which have a static set of tasks which doesn\u0026#8217;t change at runtime (that\u0026#8217;s the case for the Debezium connectors, for instance), the operator would deploy a short-lived pod on the Kubernetes cluster which runs the actual Connector implementation class and invoke its taskConfigs(int maxTasks) method . This could be implemented using a Kubernetes job, for instance. Once the operator has received the result (a map with one configuration entry per task), the connector pod can be stopped again and the operator will deploy one pod for each configured task, passing its specific configuration to the pod.\n Things get a bit more tricky if connectors dynamically change the number and/or configuration of tasks at runtime, which also is possible with Connect. For instance, that\u0026#8217;s the case for the MirrorMaker 2 connector. Such a connector typically spins up a dedicated thread upon start-up which monitors some input resource. If that resource\u0026#8217;s state changes (say, a new topic to replicate gets detected by MirrorMaker 2), it invokes the ConnectorContext::requestTaskReconfiguration() method, which in turn lets Connect retrieve the task configuration from the connector. This requires a permanently running pod for that connector class . Right now, there\u0026#8217;d be no way for the operator to know whether that connector pod can be short-lived (static task set) or must be long-lived (dynamic task set). Either Connect itself would define some means of metadata for connectors to declare that information, or it could be part of the Kubernetes custom resource for a connector described in the next section.\n The configuration of connectors would happen\u0026#8201;\u0026#8212;\u0026#8201;the Kubernetes way\u0026#8201;\u0026#8212;\u0026#8201;via custom resources. This could look rather similar to how Connect and connectors are deployed via CRs with Strimzi today; the only difference being that there\u0026#8217;d be one CR which describes both Connect (and the resource limits to apply, the connector archive to run) and the actual connector configuration. Here\u0026#8217;s an example how that could look like (again, that\u0026#8217;s a sketch of how such a CR could look like, this won\u0026#8217;t work with Strimzi right now):\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion: kafka.strimzi.io/v1beta2 kind: KafkaConnector metadata: name: debezium-connect-cluster spec: version: 3.2.0 bootstrapServers: debezium-cluster-kafka-bootstrap:9092 config: config.providers: secrets config.providers.secrets.class: io.strimzi.kafka.KubernetesSecretConfigProvider group.id: connect-cluster offset.storage.topic: connect-cluster-offsets config.storage.topic: connect-cluster-configs status.storage.topic: connect-cluster-status connector: class: io.debezium.connector.mysql.MySqlConnector tasksMax: 1 database.hostname: mysql database.port: 3306 database.user: ${secrets:debezium-example/debezium-secret:username} database.password: ${secrets:debezium-example/debezium-secret:password} database.server.id: 184054 database.server.name: mysql database.include.list: inventory database.history.kafka.bootstrap.servers: debezium-cluster-kafka-bootstrap:9092 database.history.kafka.topic: schema-changes.inventory build: output: type: docker image: 10.110.154.103/debezium-connect-mysql:latest plugins: - name: debezium-mysql-connector artifacts: - type: tgz url: https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/1.9.0.Final/debezium-connector-mysql-1.9.0.Final-plugin.tar.gz    The operator would react to the creation, modification, or deletion of this resource, retrieve the (initial) task configuration as described above and spin up corresponding connector and task pods. To stop or restart a connector or task, the user would update the resource state accordingly, upon which the operator would stop and restart the affected pod(s).\n Such an operator-based design addresses all the concerns for running Connect on Kubernetes identified above:\n   Only one component in charge of workload distribution: by removing Connect\u0026#8217;s own clustering layer from the picture, the scheduling of tasks to compute resources is completely left to one component, the operator; it will determine the number and configuration of tasks to be executed and schedule a pod for each of them; regular health checks can be used for monitoring the state of each task, restarting failed task pods as needed; a degraded health state should be exposed if a connector task is in a retrying loop, so as to make this situation apparent at the Kubernetes level; if a pod crashes, it can be restarted by the operator on the same or another node of the Kubernetes cluster, not requiring any kind of task rebalancing from a Connect perspective. Node selectors could be used to pin a task to specific node groups, e.g. in a specific region or availability zone.\n  One JVM process and Kubernetes pod per task: by launching each task in its own process, all the isolation issues discussed above can be avoided, preventing multiple tasks from negatively impacting each other. If needed, Kubernetes resource limits can be put in place in order to effectively cap the resources available to one particular task, such as CPU and RAM, while also allowing to schedule all the task pods tightly packed onto the compute nodes, making efficient use of the available resources. As each process runs exactly one task, log files are easy to consume and analyze. Scaling out can happen by increasing a single configuration parameter in the CR, and a corresponding number of task pods will be deployed by the operator. Thread leaks become a non-issue too, as there would be no notion of stopping or pausing a task; instead, just the pod itself would be stopped for that purpose, terminating the JVM process running inside of it. On the downside, the overall memory consumption across all the tasks would be increased, as there would be no amortization of Connect classes loaded into JVM processes shared by multiple tasks. Considering the significant advantages of process-based isolation, this seems like an acceptable trade-off, just as Java application developers largely have moved on from the model of co-deploying several applications into shared application server instances.\n  Immutable design: by driving configuration solely through Kubernetes resources and passing the resulting Connect configuration as parameters to the Connect process upon start-up, there\u0026#8217;s no need for exposing a mutating REST API (there\u0026#8217;d still be a REST endpoint exposing health information), making things more secure and potentially less complex internally, as the entire machinery for pausing/resuming, dynamically reconfiguring and stopping tasks could be removed. At any time, a connector\u0026#8217;s configuration would be apparent by examining its CR, which ideally should be sourced from an SCM (GitOps).\n   Looking further out into the future, such a design for making Kafka Connect Kubernetes-native would also allow for other, potentially very interesting explorations: for instance one could compile connectors into native binaries using GraalVM, resulting in a significantly lower consumption of memory and faster start-up times (e.g. when reconfiguring a connector and subsequently restarting the corresponding pod), making that model very interesting for densely packed Kubernetes environments. A buildtime toolkit like Quarkus could be used for producing specifically tailored executables, which run exactly one single connector task on top of the Connect framework infrastructure, a bit similar to how Camel-K works under the hood. Ultimately, such Kubernetes-native design could even open up the door to Kafka connectors being built in languages and runtimes other than Java and the JVM, similar to the route explored by the Conduit project.\n If you think this all sounds exciting and should become a reality, I would love to hear from you. One aspect of specific interest will be which of the proposed changes would have to be implemented within Kafka Connect itself (vs. a separate operator project, for instance under the Strimzi umbrella), without disrupting non-Kubernetes users. In any case, it would be amazing to see the Kafka community at large take its steps towards making Connect truly Kubernetes-native and fully taking advantage of this immensely successful container orchestration platform!\n Many thanks to Tom Bentley, Tom Cooper, Ryanne Dolan, Neil Buesing, Mickael Maison, Mattia Mascia, Paolo Patierno, Jakub Scholz, and Kate Stanley for providing their feedback while writing this post!\n   ","id":3,"publicationdate":"Sep 6, 2022","section":"blog","summary":"Kafka Connect, part of the Apache Kafka project, is a development framework and runtime for connectors which either ingest data into Kafka clusters (source connectors) or propagate data from Kafka into external systems (sink connectors). A diverse ecosystem of ready-made connectors has come to life on top of Kafka Connect, which lets you connect all kinds of data stores, APIs, and other systems to Kafka in a no-code approach.\n With the continued move towards running software in the cloud and on Kubernetes in particular, it\u0026#8217;s just natural that many folks also try to run Kafka Connect on Kubernetes.","tags":null,"title":"An Ideation for Kubernetes-native Kafka Connect","uri":"https://www.morling.dev/blog/ideation-kubernetes-native-kafka-connect/"},{"content":"Kafka Connect is a key factor for the wide-spread adoption of Apache Kafka: a framework and runtime environment for connectors, it makes the task of getting data either into Kafka or out of Kafka solely a matter of configuration, rather than a bespoke programming job. There\u0026#8217;s dozens, if not hundreds, of readymade source and sink connectors, allowing you to create no-code data pipelines between all kinds of databases, APIs, and other systems.\n There may be situations though where there is no existing connector matching your requirements, in which case you can implement your own custom connector using the Kafka Connect framework. Naturally, this raises the question of how to test such a Kafka connector, making sure it propagates the data between the connected external system and Kafka correctly and completely. In this blog post I\u0026#8217;d like to focus on testing approaches for Kafka Connect source connectors, i.e. connectors like Debezium, which ingest data from an external system into Kafka. Very similar strategies can be employed for testing sink connectors, though.\n Unit Tests One first obvious approach is implementing good old unit tests: simply instantiate the class under test (typically, your SourceConnector or SourceTask implementation), invoke its methods (for instance, SourceConnector::taskConfigs(), or SourceTask::poll()), and assert the return values.\n Here\u0026#8217;s an example for such a test from kc-etcd, a simple source connector for etcd, which is a distributed key/value store, most prominently used by Kubernetes as its metadata storage. Note that kc-etcd isn\u0026#8217;t meant to be a production-ready connector; I have written it primarily for learning and teaching purposes.\n This test verifies that the connector produces the correct task configuration, based on a given number of maximum tasks of two:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public class EtcdSourceConnectorTest { @Test public void shouldCreateConfigurationForTasks() throws Exception { EtcdSourceConnector connector = new EtcdSourceConnector(); Map\u0026lt;String, String\u0026gt; config = new HashMap\u0026lt;\u0026gt;(); config.put( \"clusters\", \"etcd-a=http://etcd-a-1:2379,http://etcd-a-2:2379,http://etcd-a-3:2379;etcd-b=http://etcd-b-1:2379;etcd-c=http://etcd-c-1:2379\" ); (1) connector.start(config); List\u0026lt;Map\u0026lt;String, String\u0026gt;\u0026gt; taskConfigs = connector.taskConfigs(2); (2) assertThat(taskConfigs).hasSize(2); (3) taskConfig = taskConfigs.get(0); assertThat(taskConfig).containsEntry(\"clusters\", \"etcd-a=http://etcd-a-1:2379,http://etcd-a-2:2379,http://etcd-a-3:2379;etcd-b=http://etcd-b-1:2379\"); (4) taskConfig = taskConfigs.get(1); assertThat(taskConfig).containsEntry(\"clusters\", \"etcd-c=http://etcd-c-1:2379\"); } }      1 Configure the connector with three etcd clusters   2 Request the configuration for two tasks   3 The first connector task should handle the first two clusters   4 The second task should handle the remaining third cluster    Things look similar when testing the actual polling loop of the connector\u0026#8217;s task class. As this is about testing a source connector, we first need to do some data changes in the configured etcd cluster(s), before we can assert the corresponding SourceRecords that are emitted by the task. As part of kc-etcd, I\u0026#8217;ve implemented a very basic testing harness named kcute (\"Kafka Connect Unit Testing\") which simplifies that process a bit. Here\u0026#8217;s an example test from kc-etcd, based on kcute:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 public class EtcdSourceTaskTest { @RegisterExtension (1) public static final EtcdClusterExtension etcd = EtcdClusterExtension.builder() .withNodes(1) .build(); @RegisterExtension (2) public TaskRunner taskRunner = TaskRunner.forSourceTask(EtcdSourceConnectorTask.class) .with(\"clusters\", \"test-etcd=\" + etcd.clientEndpoints().get(0)) .build(); @Test public void shouldHandleAllTypesOfEvents() throws Exception { Client client = Client.builder() (3) .keepaliveWithoutCalls(false) .endpoints(etcd.clientEndpoints()) .build(); KV kvClient = client.getKVClient(); long currentRevision = getCurrentRevision(kvClient); // insert ByteSequence key = ByteSequence.from(\"key-1\".getBytes()); ByteSequence value = ByteSequence.from(\"value-1\".getBytes()); kvClient.put(key, value).get(); // update key = ByteSequence.from(\"key-1\".getBytes()); value = ByteSequence.from(\"value-1a\".getBytes()); kvClient.put(key, value).get(); // delete key = ByteSequence.from(\"key-1\".getBytes()); kvClient.delete(key).get(); (4) List\u0026lt;SourceRecord\u0026gt; records = taskRunner.take(\"test-etcd\", 3); (5) // insert SourceRecord record = records.get(0); assertThat(record.sourcePartition()).isEqualTo(Collections.singletonMap(\"name\", \"test-etcd\")); assertThat(record.sourceOffset()).isEqualTo(Collections.singletonMap(\"revision\", ++currentRevision)); assertThat(record.keySchema()).isEqualTo(Schema.STRING_SCHEMA); assertThat(record.key()).isEqualTo(\"key-1\"); assertThat(record.valueSchema()).isEqualTo(Schema.STRING_SCHEMA); assertThat(record.value()).isEqualTo(\"value-1\"); // update record = records.get(1); assertThat(record.sourceOffset()).isEqualTo(Collections.singletonMap(\"revision\", ++currentRevision)); assertThat(record.key()).isEqualTo(\"key-1\"); assertThat(record.value()).isEqualTo(\"value-1a\"); // delete record = records.get(2); assertThat(record.sourceOffset()).isEqualTo(Collections.singletonMap(\"revision\", ++currentRevision)); assertThat(record.key()).isEqualTo(\"key-1\"); assertThat(record.value()).isNull(); } }      1 Set up an etcd cluster using the JUnit extension provided by the jetcd client project   2 Set up the task unter test using kcute   3 Obtain a client for etcd and do some data changes   4 Retrieve three records for the specified topic via kcute   5 Assert the emitted SourceRecords corresponding to the data changes done before in etcd    Now one could argue about whether this test is a true unit test or not, given it launches and relies on an instance of an external system in the form of etcd. My personal take on these matters is to be pragmatic here, as a) there\u0026#8217;s a difference to true end-to-end integration tests as discussed in the next section, and b) approaches to mock external systems usually are not worth the effort or, worse, result in poor tests, due to incorrect assumptions when implemening the mocked behavior.\n This testing approach works very well in general; in particular it doesn\u0026#8217;t require you to start Apache Kafka (and ZooKeeper), nor Kafka Connect, resulting in very fast test execution times and a great dev experience when creating and running these tests in your IDE.\n But there are some limitations, too. Essentially, we end up emulating the behavior of the actual Kafka Connect runtime in our testing harness. This can become tedious when more advanced Connect features are required for a given test, for instance retrying/restart logic, the dynamic reconfiguration of connector tasks while the connector is running, etc. Ideally, there\u0026#8217;d be a testing harness with all these capabilities provided as part of Kafka Connect itself (similar in spirit to the TopologyTestDriver of Kafka Streams), but in the absence of that, we may be better off for certain tests by deploying our source connector into an actual Kafka Connect instance and run assertions against the topic(s) it writes to.\n   Integration Tests When it comes to setting up the required infrastructure for integration tests in Java, the go-to solution these days is the excellent Testcontainers project. So let\u0026#8217;s see what it takes to test a custom Kafka connector using Testcontainers.\n As far as Kafka itself is concerned, there\u0026#8217;s a module for that coming with Testcontainers, based on Confluent Platform. Alternatively, you could use the Testcontainers module from the Strimzi project, which provides you with plain upstream Apache Kafka container images. For Kafka Connect, we provide a Testcontainers integration as part of the Debezium project, offering an API for registering connectors and controlling their lifecycle.\n Now, unfortunately, the application server like deployment model of Kafka Connect poses a challenge when it comes to testing a connector which is built as part of the current project itself. For each connector plug-in, Connect expects a directory on its plug-in path which contains all the JARs of the connector itself and its dependencies. I\u0026#8217;m not aware of any kind of \"exploded mode\", where you could point Connect to a directory which contains a connector\u0026#8217;s class files and its dependencies in JAR form.\n This means that that the connector must be packaged into a JAR file as part of the test preparation. In order to make integration tests friendly towards being run from within an IDE, this should happen programmatically within the test itself. That way, any code changes to the connector will be picked up automatically when running the test for the next time, without having to run the project\u0026#8217;s Maven build. The entire code for doing this is a bit too long (and boring) for sharing it in this blog post, but you can find it in the kc-etcd repository on GitHub.\n Here\u0026#8217;s the key parts of an integration test based on that approach, though:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 public class EtcdConnectorIT { private static Network network = Network.newNetwork(); (1) private static KafkaContainer kafkaContainer = new KafkaContainer(DockerImageName.parse(\"confluentinc/cp-kafka:7.2.0\")) .withNetwork(network); (2) public static DebeziumContainer connectContainer = new DebeziumContainer(\"debezium/connect-base:1.9.5.Final\") .withFileSystemBind(\"target/kcetcd-connector\", \"/kafka/connect/kcetcd-connector\") .withNetwork(network) .withKafka(kafkaContainer) .dependsOn(kafkaContainer); (3) public static EtcdContainer etcdContainer = new EtcdContainer(\"gcr.io/etcd-development/etcd:v3.5.4\", \"etcd-a\", Arrays.asList(\"etcd-a\")) .withNetworkAliases(\"etcd\") .withNetwork(network); @BeforeAll public static void startContainers() throws Exception { createConnectorJar(); (4) Startables.deepStart(Stream.of( kafkaContainer, etcdContainer, connectContainer)) .join(); } @Test public void shouldHandleAllTypesOfEvents() throws Exception { Client client = Client.builder() .endpoints(etcdContainer.clientEndpoint()).build(); (5) ConnectorConfiguration connector = ConnectorConfiguration.create() .with(\"connector.class\", \"dev.morling.kcetcd.source.EtcdSourceConnector\") .with(\"clusters\", \"test-etcd=http://etcd:2379\") .with(\"tasks.max\", \"2\") .with(\"key.converter\", \"org.apache.kafka.connect.storage.StringConverter\") .with(\"value.converter\", \"org.apache.kafka.connect.storage.StringConverter\"); (6) connectContainer.registerConnector(\"my-connector\", connector); connectContainer.ensureConnectorTaskState(\"my-connector\", 0, State.RUNNING); KV kvClient = client.getKVClient(); (7) // insert ByteSequence key = ByteSequence.from(\"key-1\".getBytes()); ByteSequence value = ByteSequence.from(\"value-1\".getBytes()); kvClient.put(key, value).get(); // update key = ByteSequence.from(\"key-1\".getBytes()); value = ByteSequence.from(\"value-1a\".getBytes()); kvClient.put(key, value).get(); // delete key = ByteSequence.from(\"key-2\".getBytes()); kvClient.delete(key).get(); (8) List\u0026lt;ConsumerRecord\u0026lt;String, String\u0026gt;\u0026gt; records = drain(getConsumer(kafkaContainer), 3); // insert ConsumerRecord\u0026lt;String, String\u0026gt; record = records.get(0); assertThat(record.key()).isEqualTo(\"key-1\"); assertThat(record.value()).isEqualTo(\"value-1\"); // update record = records.get(1); assertThat(record.key()).isEqualTo(\"key-1\"); assertThat(record.value()).isEqualTo(\"value-1a\"); // delete record = records.get(2); assertThat(record.key()).isEqualTo(\"key-2\"); assertThat(record.value()).isNull(); } }      1 Set up Apache Kafka in a container using the Testcontainers Kafka module   2 Set up Kafka Connect in a container, mounting the target/kcetcd-connector directory onto the plug-in path; as part of the project\u0026#8217;s Maven build, all the dependencies of the kc-etcd connector are copied into that directory   3 Set up etcd in a container   4 Package the connector classes from the target/classes directory into a JAR and add that JAR to the mounted plug-in directory; the complete source code for this can be found here   5 Configure an instance of the etcd source connector, using String as the key and value format   6 Register the connector, then block until its tasks have reached the RUNNING state   7 Do some changes in the source etcd cluster   8 Using a regular Kafka consumer, read three records from the corresponding Kafka topic and assert the keys and values (complete code here)    And that\u0026#8217;s all there is to it; we now have a test which packages our source connector, deploys it into Kafka Connect and asserts the messages it sends to Kafka. While this is definitely more time-consuming to run than the simple test harness shown above, this true end-to-end approach tests the connector in the actual runtime environment, verifying its behavior when executed via Kafka Connect, just as it would be the case when running the connector in production later on.\n   Wrap-Up In this post, we\u0026#8217;ve discussed two approaches for testing Kafka Connect source connectors: plain unit tests, \"manually\" invoking the methods of the connector/task classes under test, and integration tests, deploying a connector into Kafka Connect and verifying its behavior there via Testcontainers.\n The former approach provides you with faster turnaround times and shorter feedback cycles, whereas the latter approach gives you the confidence of testing a connector within the actual Kafka Connect runtime environment, at the cost of a more complex infrastructure set-up and longer test execution times. While we\u0026#8217;ve focused on testing source connectors in this post, both approaches could equally be applied to sink connectors, with the only difference being that you\u0026#8217;d feed records to the connector (either directly or by writing to a Kafka topic) and then observe and assert the expected state changes of the sink system in question.\n You can find the complete source code for this post, including some parts omitted here for brevity, in the kc-etcd repository on GitHub. If you think that having a test harness like kcute for unit testing connectors is a good idea and it\u0026#8217;s something you\u0026#8217;d like to contribute to, then please let me know. Ultimately, this could be extracted into its own project, independent from kc-etcd, or even be upstreamed to the Apache Kafka project proper, reusing as much as possible the actual Connect code, sans the bits for \"deploying\" connectors via a separate process.\n Many thanks to Hans-Peter Grahsl and Kate Stanley for their feedback while writing this blog post!\n  ","id":4,"publicationdate":"Aug 25, 2022","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003e\u003ca href=\"https://kafka.apache.org/documentation/#connect\"\u003eKafka Connect\u003c/a\u003e is a key factor for the wide-spread adoption of Apache Kafka:\na framework and runtime environment for connectors,\nit makes the task of getting data either into Kafka or out of Kafka solely a matter of configuration,\nrather than a bespoke programming job.\nThere\u0026#8217;s dozens, if not hundreds, of readymade source and sink connectors,\nallowing you to create no-code data pipelines between all kinds of databases, APIs, and other systems.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThere may be situations though where there is no existing connector matching your requirements,\nin which case you can \u003ca href=\"https://kafka.apache.org/documentation/#connect_development\"\u003eimplement your own\u003c/a\u003e custom connector using the Kafka Connect framework.\nNaturally, this raises the question of how to test such a Kafka connector,\nmaking sure it propagates the data between the connected external system and Kafka correctly and completely.\nIn this blog post I\u0026#8217;d like to focus on testing approaches for Kafka Connect \u003cem\u003esource\u003c/em\u003e connectors,\ni.e. connectors like \u003ca href=\"https://debezium.io/\"\u003eDebezium\u003c/a\u003e, which ingest data from an external system into Kafka.\nVery similar strategies can be employed for testing sink connectors, though.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Testing Kafka Connectors","uri":"https://www.morling.dev/blog/testing-kafka-connectors/"},{"content":"Every so often, I come across some conference talk which is highly interesting in terms of its actual contents, but which unfortunately is presented in a less than ideal way. I\u0026#8217;m thinking of basic mistakes here, such as the presenter primarily looking at their slides rather than at the audience. I\u0026#8217;m always feeling a bit sorry when this happens, as I firmly believe that everyone can do good and even great talks, just by being aware of\u0026#8201;\u0026#8212;\u0026#8201;and thus avoiding\u0026#8201;\u0026#8212;\u0026#8201;a few common mistakes, and sticking to some simple principles.\n Now, who am I to give any advice on public speaking? Indeed I\u0026#8217;m not a professional full-time speaker, but I do enjoy presenting on technologies which I am working on or with as part of my job. Over time, I\u0026#8217;ve come to learn about a few techniques which I felt helped me to give better talks. A few simple things, which can be easy to get wrong, but which make a big difference for the perception of your talk. Do I always stick to them myself? I try my best, but sometimes, I fail ¯\\_(ツ)_/¯.\n So, without further ado, here\u0026#8217;s ten tips and techniques for making your next conference talk suck a bit less.\n 1. 💦 Rehearse, Rehearse, Rehearse In particular if you have done a few talks already and you start feeling comfortable, it can be tempting to think you could just wing it and skip the rehearsal for your next one. After all, it can feel weird to be alone in your room and speak aloud all by yourself. I highly recommend to not fall for that\u0026#8201;\u0026#8212;\u0026#8201;rehearsing a talk is absolutely vital for making it successful. It will help you to develop a consistent line of argument and identify any things you otherwise may forget to mention. Only proper rehearsing will give you that natural flow you want to have for a talk.\n Also, it will help you with the right timing of your talk: you don\u0026#8217;t want to finish 20 min ahead of time, nor reach the end of your presentation slot with half of your slides remaining. If this happens, it usually means folks haven\u0026#8217;t rehearsed once and it\u0026#8217;s not a good position to be in. For a new talk, I usually will do three rehearsal runs before presenting it at an event. I will also do a rehearsal run if I repeat an earlier talk after some months, as it\u0026#8217;s too easy to forget some important point otherwise.\n When doing a rehearsal, it\u0026#8217;s a good idea to note down some key timestamps, such as when you transition to a demo. This will come in handy for instance for identifying sections you could shorten if you realize the talk is too long in its initial form.\n   2. 🎬 Start With a Mission How to start a talk well could be an entire topic for its own post. After all, the first few seconds decide whether folks will be excited about your talk and pay attention, or rather pack out their laptop and check their emails. What I\u0026#8217;ve found to work well for me is starting with a mission. I.e. I\u0026#8217;ll often present a specific problem and make the case for how listening to that talk will help to address that problem. Needless to say that the problem should be relevant to the audience, i.e. its key to motivate why and how it matters to them, and how learning about the solution will benefit them. Don\u0026#8217;t focus on the thing you want to talk about, focus on a challenge your audience has and how your talk will help them to overcome that.\n Another approach is to present the key learnings (for instance three, see below) which the audience will make during the talk. While this may sound similar to an agenda slide, the framing is different: it\u0026#8217;s taking the perspective of the listener and what\u0026#8217;s in it for them by sticking through your session. Don\u0026#8217;t lead with your personal introduction; if you\u0026#8217;re known in the field, people don\u0026#8217;t care. And if you\u0026#8217;re not, well, they probably also won\u0026#8217;t care. In any case, telling much about yourself is not what will attract people to your talk. I usually have a very brief intro slide after discussing the mission or key learnings.\n   3. 📖 Tell a Story Good talks tell a story, i.e. there\u0026#8217;s a meaningful progression in terms of what you tell, starting with some setting and context, perhaps with some challenge or drama (\"And this is when our main production server failed\"), and of course a happy ending (\"With the new solution we can fail-over to a stand-by in less than a second\").\n Now it doesn\u0026#8217;t literally have to be a story (although it can be, as for instance in my talk To the Moon and Beyond With Java 17 APIs!), but you should make sure that there is a logical order of the things you discuss, for instance in a temporal or causal sense, and you should avoid jumping forth and back between different things. The latter for instance can happen due to insufficient rehearsal, forcing you to make a specific point too late during the talk, as you forgot to bring it at the right moment. Also, for each discussion point and slide there should be a very specific reason for having it in your deck. I.e. it should form a cohesive unit, rather than being a collection of random unrelated talking points.\n Other storytelling techniques can be employed to great effect as well, such as doing a quick wrap-up when finishing a key section of your session, or adding little \"side quests\" for things you really want to mention but which are not strictly related to the main storyline.\n In terms of crafting a story, I try to start early and collect input over a longer period of time, typically using a mind map. This allows you to identify and gather the most interesting aspects of a given topic, also touching on points which perhaps came up in a revelation you had a while ago. You\u0026#8217;ll be less likely to have that breadth of contents at your disposal when starting the day before the presentation. This is not to say that you should use every single bit of information you\u0026#8217;ve collected, but starting from a broad foundation allows you to select the most relevant and insightful bits.\n   4. 👀 Look at the Audience, Not Your Slides As mentioned at the beginning, one of my pet peeves is presenters turning their back (or side) to the audience and looking towards their slides projected next to them. This creates a big disconnect with your audience. The same applies to the slides on the laptop in front of you, avoid looking at them as much as you can. Instead, try to have as much eye contact with the audience as possible, it makes a huge difference in terms of perception and quality of your talk. Putting a sticker onto your screen can be a helpful reminder. Only if you actually speak to the audience, it will be an engaging and immersive experience for them. It\u0026#8217;s extra bad if you don\u0026#8217;t use a microphone, say at a local meet-up, as it means people will be able to understand you much worse.\n Now why are folks actually looking at their slides? I think it\u0026#8217;s generally an expression of feeling a bit insecure or uncomfortable, and in particular the concern to forget to mention an important point. To me, the only viable solution here is that you really need to memorize what you want to say, in which case you\u0026#8217;ll be able to make your points without having to read anything from your slides. Your slides are not your speaker notes!\n   5. 🧹 Put Less Text on Your Slides. Much Less In terms of what should be on slides, this again could be a topic for its own blog post. In general, the less words the better. Note I\u0026#8217;m not suggesting you need to go image-only slides TED talk style, but you should minimize the amount of text on slides as much as possible. The reason being that folks will either listen to you or read what\u0026#8217;s on your slides, but hardly both. Which means that either your effort for putting the text on the slides is wasted (bad), or folks don\u0026#8217;t actually get what you\u0026#8217;re telling them (worse). So if you think you\u0026#8217;ve removed enough, remove some more. And then some more. This also allows you to make the font size big enough, so that folks actually can read those few items which remain.\n What I personally like to have on slides the most is diagrams, charts, sketches, and the like. Anything visual really. Which also brings up one exception to the \"Don\u0026#8217;t look at your slides\" rule: if you actually explain a visual, elaborating a particular part for instance, then shortly turning towards the slide and pointing to some element of it can make sense.\n On a related note, I recommend not relying on having access to your speaker notes during a talk. While technically it may be possible to show the notes on your laptop and the actual slides on the projector, this will fall apart when you do a live demo, where you really need to work with a mirrored set-up. Think of speaker notes as of cheat sheets back in school: the value is in writing them, not in reading them. By the time you\u0026#8217;ll present your talk, you\u0026#8217;ll have memorized what\u0026#8217;s on your notes. Make use of them for developing the story line for each slide, and of course they will also be useful when coming back to a talk after a few months.\n   6. ✂️ Tailor the Talk Towards Your Audience I don\u0026#8217;t see that one done wrong too often, but it\u0026#8217;s worth pointing out: a talk should actually match its audience. So if for instance you talk to users of some technology, focussing on use cases of it makes sense, or on how to run it in production etc. Whereas this audience probably won\u0026#8217;t care as much about implementation details (as much as you may want to talk about how you solved that one tricky technical challenge using some clever approach). If, on the other hand, you present about the same technology to a conference geared towards builders of tech in that space, diving into those gory details would be highly attractive for the audience.\n That\u0026#8217;s why I focus heavily on use cases when talking about Debezium at developer conferences. Whereas when I had the opportunity to present on Debezium and change data capture (CDC) during an online talk series of Carnegie Mellon\u0026#8217;s database group, I centered the talk around implementation challenges and improvements databases could make to better support CDC use cases.\n Key here is expectation management: make sure you know what kind of audience you\u0026#8217;re going to speak to and adjust your talk accordingly. Oftentimes, the same basic talk can work well for different settings and audiences, just with framing things the right way and putting the focus on the right parts, for instance by swapping a few slides in and out.\n   7. 3️⃣ Rule of Three Over time I\u0026#8217;ve become a big believer in the rule of three; for instance, have three main learnings or ideas for a talk. If it\u0026#8217;s a talk about a new product release, share three key features. On one slide, have three main points to discuss. When you share examples, give three of them. And so on.\n Why three? It hits the sweet spot of providing representative information and data, letting you enough time to sufficiently dive into each of them, and not being too extensive or repetitive. Your audience can digest only so much input in a given session, so they\u0026#8217;ll be better served if you tell them about three things which they can take in and remember, instead of telling them about ten things which they all quickly forget or even miss to begin with.\n   8. 🚑 Have a Fallback Plan for Demos Live demos can be a great addition to any technology-centered conference talk. Actually showing how the thing you discuss works can be an eye-opener and be truly impressive. Not so much though if the demo gods aren\u0026#8217;t with you. And we\u0026#8217;ve all been there: poor network at the conference venue doesn\u0026#8217;t let you download that one container image you\u0026#8217;re missing, you have a compile error in your code and in the heat of the moment you can\u0026#8217;t find out what\u0026#8217;s wrong, etc.\n Trying to analyze problems in front of a conference audience can be very stressful, and frankly speaking, it\u0026#8217;s quickly getting boring or even weird for the audience. So you always should have a fallback plan in case things don\u0026#8217;t go as expected with your demo. My go-to strategy is to have a pre-recorded video of the demo which I can play back, instead of wasting minutes trying to solve any issues. I\u0026#8217;ll still live-comment that video, which makes it a bit more interactive rather than collectively listening to my pre-recorded voice. For instance I can pause the video and expand on some specific point.\n   9. 💪 Play to Your Strengths Some personal habits are really hard to change. One example: I tend to speak fast, very fast, during talks. I\u0026#8217;m well aware of that, listeners told me, a coach told me, I saw it myself in recordings. But it\u0026#8217;s somehow impossible for me to change it. If I really force myself hard to speak slower, it will work for a while, but typically I\u0026#8217;ll be back to my usual speed after a while.\n So I\u0026#8217;ve decided to not fight against this any longer and just live with it. The reason being that I feel the high pace also gives me some energy and flow which I hope becomes apparent to the audience. I believe viewers (and I) are better off with me doing a passionate talk which may be a bit too fast, instead of one which has a slower pace but lacks the right amount of energy.\n I think that\u0026#8217;s generally applicable: You don\u0026#8217;t like talking about concepts, but love showing how things work in action? Then shorten the former and make more room for a live demo. You enjoy discussing live questions? Make more time for the Q\u0026amp;A. This all is to say, instead of excessively focussing on things you perceive as your weak sides, rather leverage your strong suites.\n (Yes, the irony of this being part of a post focussing on avoiding basic mistakes is not lost on me.)\n   10. 🔄 Circle Back I\u0026#8217;ve found it works great if you circle back to a point you made earlier during a talk. The most apparent way of doing this is coming back to the mission statement you set out for the talk at the beginning. You should be able to make the point that the things you presented actually satisfy that original mission. Or you have some sort of catch phrase to which you cycle back a few times, repetition can help to drive home a point. Just don\u0026#8217;t overdo it, as it can become annoying otherwise. Personally, I like the notion of circling back as it provides some means of closure which is a pleasant sensation.\n And that\u0026#8217;s it, ten basic tips for making your next talk suck a bit less. You probably won\u0026#8217;t get an invitation for doing your first TED talk just by applying them, but they may help you with your next tech conference or meet-up presentation. As a presenter, you should think of yourself as a service provider to the audience: they pay with their time (and usually a fair amount of money) to attend your talk, so you should put in the effort to make sure they have a great time and experience.\n What are your presentation tips and tricks? Let me know in the comments below!\n Many thanks to Hans-Peter Grahsl, Marta Paes, and Robin Moffatt for their feedback while writing this blog post!\n   ","id":5,"publicationdate":"Jun 23, 2022","section":"blog","summary":"Every so often, I come across some conference talk which is highly interesting in terms of its actual contents, but which unfortunately is presented in a less than ideal way. I\u0026#8217;m thinking of basic mistakes here, such as the presenter primarily looking at their slides rather than at the audience. I\u0026#8217;m always feeling a bit sorry when this happens, as I firmly believe that everyone can do good and even great talks, just by being aware of\u0026#8201;\u0026#8212;\u0026#8201;and thus avoiding\u0026#8201;\u0026#8212;\u0026#8201;a few common mistakes, and sticking to some simple principles.","tags":null,"title":"Ten Tips to Make Conference Talks Suck Less","uri":"https://www.morling.dev/blog/ten-tips-make-conference-talks-suck-less/"},{"content":"Update Jun 3: This post is discussed on Reddit and Hacker News\n Project Loom (JEP 425) is probably amongst the most awaited feature additions to Java ever; its implementation of virtual threads (or \"green threads\") promises developers the ability to create highly concurrent applications, for instance with hundreds of thousands of open HTTP connections, sticking to the well-known thread-per-request programming model, without having to resort to less familiar and often more complex to use reactive approaches.\n Having been in the workings for several years, Loom got merged into the mainline of OpenJDK just recently and is available as a preview feature in the latest Java 19 early access builds. I.e. it\u0026#8217;s the perfect time to get your hands onto virtual threads and explore the new feature. In this post I\u0026#8217;m going to share an interesting aspect I learned about thread scheduling fairness for CPU-bound workloads running on Loom.\n Project Loom First, some context. The problem with the classic thread-per-request model is that only scales up to a certain point. Threads managed by the operating system are a costly resource, which means you can typically have at most a few thousands of them, but not hundreds of thousands, or even millions. Now, if for instance a web application makes a blocking request to a database, the thread making that request is exactly that, blocked. Of course other threads can be scheduled on the CPU in the meantime, but you cannot have more concurrent requests than threads available to you.\n Reactive programming models address this limitation by releasing threads upon blocking operations such as file or network IO, allowing other requests to be processed in the meantime. Once a blocking call has completed, the request in question will be continued, using a thread again. This model makes much more efficient use of the threads resource for IO-bound workloads, unfortunately at the price of a more involved programming model, which doesn\u0026#8217;t feel familiar to many developers. Also aspects like debuggability or observability can be more challenging with reactive models, as described in the Loom JEP.\n This explains the huge excitement and anticipation of Project Loom within the Java community. Loom introduces a notion of virtual threads which are scheduled onto OS-level carrier threads by the JVM. If application code hits a blocking method, Loom will unmount the virtual thread from its curring carrier, making space for other virtual threads to be scheduled. Virtual threads are cheap and managed by the JVM, i.e. you can have many of them, even millions. The beauty of the model is that developers can stick to the familiar thread-per-request programming model without running into scaling issues due to a limited number of available threads. I highly recommend you to read the JEP of Project Loom, which is very well written and provides much more details and context.\n   Scheduling Now how does Loom\u0026#8217;s scheduler know that a method is blocking? Turns out, it doesn\u0026#8217;t. As I learned from Ron Pressler, the main author of Project Loom, it\u0026#8217;s the other way around: blocking methods in the JDK have been adjusted for Loom, so as to release the OS-level carrier thread when being called by a virtual thread:\n All blocking in Java is done through the JDK (unless you explicitly call native code). We changed the \u0026quot;leaf\u0026quot; blocking methods in the JDK to block the virtual thread rather than the platform thread. E.g. in all of java.util.concurrent there\u0026#39;s just one such method: LockSupport.park\n\u0026mdash; Ron Pressler (@pressron) May 24, 2022   Ron\u0026#8217;s reply triggered a very interesting discussion with Tim Fox (e.g. of Vert.x fame): what happens if code is not IO-bound, but CPU-bound? I.e. if code in a virtual thread runs some heavy calculation without ever calling any of the JDK\u0026#8217;s blocking methods, will that virtual thread ever be unmounted?\n Perhaps surprisingly, the answer currently is: No. Which means that CPU-bound code will actually behave very differently with virtual threads than with OS-level threads. So let\u0026#8217;s take a closer look at that phenomenon with the following example program:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 public class LoomTest { public static long blackHole; public static void main(String[] args) throws Exception { ExecutorService executor = Executors.newCachedThreadPool(); for(int i = 0; i \u0026lt; 64; i++) { final Instant start = Instant.now(); final int id = i; executor.submit(() -\u0026gt; { BigInteger res = BigInteger.ZERO; for(int j = 0; j \u0026lt; 100_000_000; j++) { res = res.add(BigInteger.valueOf(1L)); } blackHole = res.longValue(); System.out.println(id + \";\" + Duration.between(start, Instant.now()).toMillis()); }); } executor.shutdown(); executor.awaitTermination(1, TimeUnit.HOURS); } }    64 threads are started at approximately the same time using a traditional cached thread pool, i.e. OS-level threads. Each thread counts to 100M (using BigInteger to make it a bit more CPU-intensive) and then prints out how long it took from scheduling the thread to the point of its completion. Here are the results from my Mac Mini M1:\n   In wallclock time, it took all the 64 threads roughly 16 seconds to complete. The threads are rather equally scheduled between the available cores of my machine. I.e. we\u0026#8217;re observing a fair scheduling scheme. Now here are the results using virtual threads (by obtaining the executor via Executors::newVirtualThreadPerTaskExecutor()):\n   That chart looks very differently. The first eight threads took a wallclock time of about two seconds to complete, the next eight took about four seconds, etc. As the executed code doesn\u0026#8217;t hit any of the JDK\u0026#8217;s blocking methods, the threads never yield and thus ursurpate their carrier threads until they have run to completion. This represents an unfair scheduling scheme of the threads. While they were all started at the same time, for the first two seconds only eight of them were actually executed, followed by the next eight, and so on.\n Loom\u0026#8217;s scheduler uses by default as many carrier threads as there are CPU cores available; There are eight cores in my M1, so processing happens in chunks of eight virtual threads at a time. Using the jdk.virtualThreadScheduler.parallelism system property, the number of carrier threads can be adjusted, e.g. to 16:\n   For the fun of it, let\u0026#8217;s add a call to Thread::sleep() (i.e. a blocking method) to the processing loop and see what happens:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 ... for(int j = 0; j \u0026lt; 100_000_000; j++) { res = res.add(BigInteger.valueOf(1L)); if (j % 1_000_000 == 0) { try { Thread.sleep(1L); } catch (InterruptedException e) { throw new RuntimeException(e); } } } ...    Surely enough, we\u0026#8217;re back to a fair scheduling, with all threads completing after the roughly same wallclock time:\n   It\u0026#8217;s noteworthy that the actual durations appear more harmonized in comparison to the original results we got from running with 64 OS-level threads. It seems the Loom scheduler can do a slightly better job of distributing the available resources between virtual threads. Surprisingly, a call to Thread::yield() didn\u0026#8217;t have the same result. While a scheduler is free to ignore this intend to yield as per the method\u0026#8217;s JavaDoc, Sundararajan Athijegannathan indicated that this would be applied by Loom. It would surely be interesting to know why that\u0026#8217;s not the case here.\n   Discussion Seeing these results, the big question of course is whether this unfair scheduling of CPU-bound threads in Loom poses a problem in practice or not. Ron and Tim had an expanded debate on that point, which I recommend you to check out to form an opinion yourself. As per Ron, support for yielding at points in program execution other than blocking methods has been implemented in Loom already, but this hasn\u0026#8217;t been merged into the mainline with the initial drop of Loom. It should be easy enough though to bring this back if the current behavior turns out to be problematic.\n Now there\u0026#8217;s not much point in overcommitting to more threads than physically supported by a given CPU anyways for CPU-bound code (nor in using virtual threads to begin with). But in any case it\u0026#8217;s worth pointing out that CPU-bound code may behavior differently with virtual threads than with classic OS-level threads. This may come at a suprise for Java developers, in particular if authors of such code are not in charge of selecting the thread executor/scheduler actually used by an application.\n Time will tell whether yield support also for CPU-bound code will be required or not, either via support for explicit calls to Thread::yield() (which I think should be supported at the very least) or through more implicit means, e.g. by yielding when reaching a safepoint. As I learned, Go\u0026#8217;s goroutines support yielding in similar scenarios since version 1.14, so I wouldn\u0026#8217;t be surprised to see Java and Loom taking the same course eventually.\n  ","id":6,"publicationdate":"May 27, 2022","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003e\u003cem\u003eUpdate Jun 3: This post is discussed on \u003ca href=\"https://www.reddit.com/r/java/comments/v394uh/loom_and_thread_fairness/\"\u003eReddit\u003c/a\u003e and \u003ca href=\"https://news.ycombinator.com/item?id=31600067\"\u003eHacker News\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eProject Loom (\u003ca href=\"https://openjdk.java.net/jeps/425\"\u003eJEP 425\u003c/a\u003e) is probably amongst the most awaited feature additions to Java ever;\nits implementation of virtual threads (or \"green threads\") promises developers the ability to create highly concurrent applications,\nfor instance with hundreds of thousands of open HTTP connections,\nsticking to the well-known thread-per-request programming model,\nwithout having to resort to less familiar and often more complex to use reactive approaches.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eHaving been in the workings for several years, Loom got merged into the mainline of OpenJDK \u003ca href=\"https://github.com/openjdk/jdk/commit/9583e3657e43cc1c6f2101a64534564db2a9bd84\"\u003ejust recently\u003c/a\u003e and is available as a preview feature in the latest \u003ca href=\"https://jdk.java.net/19/\"\u003eJava 19 early access builds\u003c/a\u003e.\nI.e. it\u0026#8217;s the perfect time to get your hands onto virtual threads and explore the new feature.\nIn this post I\u0026#8217;m going to share an interesting aspect I learned about thread scheduling fairness for CPU-bound workloads running on Loom.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Loom and Thread Fairness","uri":"https://www.morling.dev/blog/loom-and-thread-fairness/"},{"content":"JDK Mission Control (JMC) is invaluable for analysing performance data recording using JDK Flight Recorder (JFR). The other day, I ran into a problem when trying to run JMC on my Mac Mini M1. Mostly for my own reference, here\u0026#8217;s what I did to overcome it.\n Upon launching a freshly downloaded JMC (I tried both the upstream build from OpenJDK and the one from the Eclipse Adoptium project), I\u0026#8217;d get the following error message:\n  The JVM shared library \"/Library/Java/JavaVirtualMachines/temurin-17.jdk/Contents/Home/bin/../lib/server/libjvm.dylib\" does not contain the JNI_CreateJavaVM symbol.\n     \"temurin-17.jdk\" is my default JDK; it\u0026#8217;s the Java 17 build provided by the Eclipse Temurin project for macOS/AArch64, i.e. the right one for the ARM chip of the M1 (\"Apple silicon\"). The error message isn\u0026#8217;t overly helpful; after all, that referenced JDK works just fine for all my other applications. The problem though is that JMC itself currently only is shipped as an x64 application:\n 1 2 $ file \"JDK Mission Control.app\"/Contents/MacOS/jmc .../JDK Mission Control.app/Contents/MacOS/jmc: Mach-O 64-bit executable x86_64    So I decided to try with an x64 JDK build instead; thanks to Apple\u0026#8217;s Rosetta project, x64 binaries can be executed on the M1 via a rather efficient emulation.\n After downloading the macOS/x64 Temurin build, it needs to be configured as the JDK to use for JMC. For that, open the file JDK Mission Control.app/Contents/Info.plist in an editor and look for the Eclipse key. Add the -vm parameter with the path to the x64 JDK to the key\u0026#8217;s value. Altogether, it should look like so:\n 1 2 3 4 5 6 7 8 ... \u0026lt;array\u0026gt; \u0026lt;string\u0026gt;-keyring\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;~/.eclipse_keyring\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;-vm\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;/path/to/jdk-17.0.3+7-x86-64/Contents/Home/bin/java\u0026lt;/string\u0026gt; \u0026lt;/array\u0026gt; ...    Et voilà, JMC will now start just fine on the Apple M1. Note that in some cases I got an intermittent permission issue after editing the plist file. Resetting the permissions helped in that case:\n 1 $ sudo chmod -R 755 \"JDK Mission Control.app\"    With the x64 JDK around, it\u0026#8217;s a good idea to make sure it\u0026#8217;s only used for JMC, while sticking to the AArch64 build for all other usages for the sake of performance. Unfortunately, it\u0026#8217;s not quite obvious to see flavour you are running, as the target architecture isn\u0026#8217;t displayed in the output of java --version:\n 1 2 3 4 5 6 7 8 9 10 11 $ export JAVA_HOME=path/to/temurin-17.jdk/Contents/Home $ java --version openjdk 17.0.3 2022-04-19 OpenJDK Runtime Environment Temurin-17.0.3+7 (build 17.0.3+7) OpenJDK 64-Bit Server VM Temurin-17.0.3+7 (build 17.0.3+7, mixed mode) $ export JAVA_HOME=path/to/jdk-17.0.3+7-x86-64/Contents/Home $ jdks java --version openjdk 17.0.3 2022-04-19 OpenJDK Runtime Environment Temurin-17.0.3+7 (build 17.0.3+7) OpenJDK 64-Bit Server VM Temurin-17.0.3+7 (build 17.0.3+7, mixed mode, sharing)    Not sure what \"sharing\" exactly means in the x64 output, perhaps it\u0026#8217;s a hint? In any case, printing the contents of the os.arch system property will tell the truth, e.g. in jshell:\n 1 2 3 4 5 6 7 $ export JAVA_HOME=/Library/Java/JavaVirtualMachines/temurin-17.jdk/Contents/Home $ jdks jshell | Welcome to JShell -- Version 17.0.3 | For an introduction type: /help intro jshell\u0026gt; System.out.println(System.getProperty(\"os.arch\")) aarch64    1 2 3 4 5 6 7 $ export JAVA_HOME=~/Applications/jdks/jdk-17.0.3+7-x86-64/Contents/Home $ jshell | Welcome to JShell -- Version 17.0.3 | For an introduction type: /help intro jshell\u0026gt; System.out.println(System.getProperty(\"os.arch\")) x86_64    If you are aware of a quicker way for identifying the current JDK\u0026#8217;s target platform, I\u0026#8217;d love to learn about it in the comments below. Thanks!\n","id":7,"publicationdate":"May 17, 2022","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003e\u003ca href=\"https://jdk.java.net/jmc/8/\"\u003eJDK Mission Control\u003c/a\u003e (JMC) is invaluable for analysing performance data recording using \u003ca href=\"https://openjdk.java.net/jeps/328\"\u003eJDK Flight Recorder\u003c/a\u003e (JFR).\nThe other day, I ran into a problem when trying to run JMC on my Mac Mini M1.\nMostly for my own reference, here\u0026#8217;s what I did to overcome it.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Running JDK Mission Control on Apple M1","uri":"https://www.morling.dev/blog/running-jmc-on-apple-m1/"},{"content":"When it comes to code reviews, it\u0026#8217;s a common phenomenon that there is much focus and long-winded discussions around mundane aspects like code formatting and style, whereas important aspects (does the code change do what it is supposed to do, is it performant, is it backwards-compatible for existing clients, and many others) tend to get less attention.\n To raise awareness for the issue and providing some guidance on aspects to focus on, I shared a small visual on Twitter the other day, which I called the \"Code Review Pyramid\". Its intention is to help putting focus on those parts which matter the most during a code review (in my opinion, anyways), and also which parts could and should be automated.\n As some folks asked for a permanent, referenceable location of that resource and others wanted to have a high-res printing version, I\u0026#8217;m putting it here again:\n   You can also download the visual as an SVG file.\n FAQ   Why is it a pyramid?\nThe lower parts of the pyramid should be the foundation of a code review and take up the most part of it.\n   Hey, that\u0026#8217;s a triangle!\nYou might think so, but it\u0026#8217;s a pyramid from the side.\n   Which tool did you use for creating the drawing?\nExcalidraw.\n      ","id":8,"publicationdate":"Mar 10, 2022","section":"blog","summary":"When it comes to code reviews, it\u0026#8217;s a common phenomenon that there is much focus and long-winded discussions around mundane aspects like code formatting and style, whereas important aspects (does the code change do what it is supposed to do, is it performant, is it backwards-compatible for existing clients, and many others) tend to get less attention.\n To raise awareness for the issue and providing some guidance on aspects to focus on, I shared a small visual on Twitter the other day, which I called the \"","tags":null,"title":"The Code Review Pyramid","uri":"https://www.morling.dev/blog/the-code-review-pyramid/"},{"content":"The JDK Flight Recorder (JFR) is one of Java\u0026#8217;s secret weapons; deeply integrated into the Hotspot VM, it\u0026#8217;s a high-performance event collection framework, which lets you collect metrics on runtime aspects like object allocation and garbage collection, class loading, file and network I/O, and lock contention, do method profiling, and much more.\n JFR data is persisted in recording files (since Java 14, also \"realtime\" event streaming is supported), which can be loaded for analysis into tools like JDK Mission Control (JMC), or the jfr utility coming with OpenJDK itself.\n While there\u0026#8217;s lots of blog posts, conference talks, and other coverage on JFR itself, information about the format of recording files is surprisingly heard to come by. There is no official specification, so the only way to actually understand the JFR file format is to read the source code for writing recordings in the JDK itself, which is a combination of Java and C++ code. Alternatively, you can study the code for parsing recordings in JMC (an official JDK project). Btw., JMC comes with a pure Java-based JFR file writer implementation too.\n Apart from the source code itself, the only somewhat related resources which I could find are this JavaOne presentation by Staffan Larssan (2013, still referring to the proprietary Oracle JFR), several JFR-related blog posts by Marcus Hirt, and a post about JFR event sizes by Richard Startin. But there\u0026#8217;s no in-depth discussion or explanation of the file format. As it turns out, this by design; the OpenJDK team shied away from creating a spec, \"because of the overhead of maintaining and staying compatible with it\". I.e. the JFR file format is an implementation detail of OpenJDK, and as such the only stable contract for interacting with it are the APIs provided by JFR.\n Now, also if it is an implementation detail, knowing more about the JFR file format would certainly be useful; for instance, you could use this to implement tools for analyzing and visualizing JFR data in non-JVM programming languages, say Python, or to patch corrupted recording files. So my curiosity was piqued and I thought it\u0026#8217;d be fun to try and find out how JFR recording files are structured. In particular, I was curious about which techniques are used for keeping files relatively small, also with hundreds of thousands or even millions of recoreded events.\n I grabbed a hex editor, the source code of JMC\u0026#8217;s recording parser (which I found a bit easier to grasp than the Java/C++ hybrid in the JDK itself), and loaded several example recordings from my JFR Analytics project, stepping through the parser code in debug mode (fun fact: while doing so, I noticed JMC currently fails to parse events with char attributes).\n Just a feeew hours later, and I largely understood how the thing works. As an image says more than a thousand words, and I\u0026#8217;ll never say no to an opportunity to draw something in the fabuluous Excalidraw, so I proudly present to you this visualization of the JFR file format as per my understanding (click to enlarge):\n   It\u0026#8217;s best viewed on a big screen 😎. Alternatively, here\u0026#8217;s a SVG version. Now this doesn\u0026#8217;t go into all the finest aspects, so you probably couldn\u0026#8217;t go off and implement a clean-room JFR file parser solely based on this. But it does show the relevant concepts and mechanisms. I suggest you spend some time going through sections one to five in the picture, and dive into the sections for header, metadata, constant pool, and actual recorded events. Studying the image should give you a good understanding of the JFR file format and its structure.\n Here are some observations I made as I found my way through the file format:\n   JFR recordings are organized in chunks: Chunks are self-contained independent containers of recorded events and all the metadata required for interpreting these events. There\u0026#8217;s no additional content in recordings besides the chunks, i.e. concat several chunk files, and you\u0026#8217;ll have a JFR recording file. A multi-chunk recording file can be split up into the individual chunks using the jfr utility which comes with OpenJDK:\n1 jfr disassemble --output \u0026lt;target-dir\u0026gt; some-recording.jfr    The default chunksize is 12MB, but if needed, you can override this, e.g. using the -XX:FlightRecorderOptions:maxchunksize=1MB option when starting a recording. A smaller chunk size can come in handy if for instance you only want to transmit a specific section of a long-running recording. On the other hand, many small chunks will increase the overall size of a recording, due to the repeatedly stored metadata and constant pools\n   The event format is self-descriptive: The metadata part of each chunk describes the structure of the contained events, all referenced types, their attributes, etc.; by means of JFR metadata annotations, such as @Label, @Description, @Timestamp etc., further metadata like human-readable names and description as well as units of measurements are expressed, allowing to consume and parse an event stream without a-priori knowledge of specific event types. In particular, this allows for the definition of custom event types and displaying them in the generic event browser of JMC (of course, bespoke views such as the \"Memory\" view rely on type-specific interpretations of individual event types)\n  The format is geared towards space efficiency: Integer values are stored in a variable-length encoded way (LEB128), which will safe lots of space when storing small values. A constant pool is used to store repeatedly referenced objects, such as String literals, stack traces, class and method names, etc.; for each usage of such constant in a recorded event, only the constant pool index is stored (a var-length encoded long). Note that Strings can either be stored as raw values within events themselves, or in the constant pool. Unfortunately, no control is provided for choosing between the two; strings with a length between 16 and 128 will be stored in the constant pool, any others as raw value. It could be a nice extension to give event authors more control here, e.g. by means of an annotation on the event attribute definition\n       When using the jdk.OldObjectSample event type, beware of bug JDK-8277919, which may cause a bloat of the constant pool, as the same entry is duplicated in the pool many times. This will be fixed in Java 17.0.3 and 18.       The format is row-based: Events are stored sequentially one after another in recording files; this means that for instance boolean attributes will consume one full byte, also if actually eight boolean values could be stored in a single byte. It could be interesting to explore a columnar format as an alternative, which may help to further reduce recording size, for instance also allowing to efficiently compress event timestamps values using delta-encoding\n  Compression support in JMC reader implementation: The JFR parser implementation of JMC transparently unpacks recording files which are compressed using GZip, ZIP, or LZ4 (Marcus Hirt discusses the compression of JFR recordings in this post). Interestingly, JMC 8.1 still failed to open such compressed recording with an error message. The jfr utility doesn\u0026#8217;t support compressed recording files, and I suppose the JFR writer in the JDK doesn\u0026#8217;t produce compressed recordings either\n  ","id":9,"publicationdate":"Feb 20, 2022","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe \u003ca href=\"https://openjdk.java.net/jeps/328\"\u003eJDK Flight Recorder\u003c/a\u003e (JFR) is one of Java\u0026#8217;s secret weapons;\ndeeply integrated into the Hotspot VM, it\u0026#8217;s a high-performance event collection framework,\nwhich lets you collect metrics on runtime aspects like object allocation and garbage collection,\nclass loading, file and network I/O, and lock contention, do method profiling, and much more.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eJFR data is persisted in recording files\n(since Java 14, also \u003ca href=\"https://openjdk.java.net/jeps/349\"\u003e\"realtime\" event streaming\u003c/a\u003e is supported),\nwhich can be loaded for analysis into tools like JDK Mission Control (JMC),\nor the \u003cem\u003ejfr\u003c/em\u003e utility coming with OpenJDK itself.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"The JDK Flight Recorder File Format","uri":"https://www.morling.dev/blog/jdk-flight-recorder-file-format/"},{"content":"Update Jan 13: This post is discussed on Reddit\n Update Feb 7: This post is discussed on Hacker News\n As software developers, we\u0026#8217;ve all come across those annoying, not-so-useful error messages when using some library or framework: \"Couldn\u0026#8217;t parse config file\", \"Lacking permission for this operation\", etc. Ok, ok, so something went wrong apparently; but what exactly? What config file? Which permissions? And what should you do about it? Error messages lacking this kind of information quickly create a feeling of frustration and helplessness.\n So what makes a good error message then? To me, it boils down to three pieces of information which should be conveyed by an error message:\n   Context: What led to the error? What was the code trying to do when it failed?\n  The error itself: What exactly failed?\n  Mitigation: What needs to be done in order to overcome the error?\n   Let\u0026#8217;s dive into these individidual aspects a bit. Before we start, let me clarify that this is about error messages created by library or framework code, for instance in form of an exception message, or in form of a message written to some log file. This means the consumers of these error messages will typically be either other software developers (encountering errors raised by 3rd party dependencies during application development), or ops folks (encountering errors while running an application).\n That\u0026#8217;s in contrast to user-facing error messages, for which other guidance and rules (in particular in regards to security concerns) should be applied. For instance, you typically should not expose any implementation details in a user-facing message, whereas that\u0026#8217;s not that much of a concern\u0026#8201;\u0026#8212;\u0026#8201;or on the contrary, it can even be desirable\u0026#8201;\u0026#8212;\u0026#8201;for the kind of error messages discussed here.\n Context In a way, an error message tells a story; and as with every good story, you need to establish some context about its general settings. For an error message, this should tell the recipient what the code in question was trying to do when it failed. In that light, the first example above, \"Couldn\u0026#8217;t parse config file\", is addressing this aspect (and only this one) to some degree, but probably it\u0026#8217;s not enough. For instance, it would be very useful to know the exact name of the file:\n  Couldn\u0026#8217;t parse config file: /etc/sample-config.properties\"\n   Using an example from Debezium, the open-source change data capture platform I am working on in my day job, the second message could read like so with some context about what happened:\n  Failed to create an initial snapshot of the data; lacking permission for this operation\n   Coming back to error messages related to the processing of some input or configuration file, it can be a good idea to print the absolute path. In case file system resources are provided as relative paths, this can help to identify wrong assumptions around the current working directory, or whatever else is used as the root for resolving relative paths. On the other hand, in particular in case of multi-tenant or SaaS scenarios, you may consider filesystem layouts as a confidential implementation detail, which you may prefer to not reveal to unknown code you run. What\u0026#8217;s best here depends on your specific situation.\n If some framework supports different kinds of files, the specific kind of the problematic file in question should be part of the message as well: \"Couldn\u0026#8217;t parse entity mapping file\u0026#8230;\u0026#8203;\". If the error is about specific parts of the contents of a file, displaying the line number and/or the line itself is a good idea.\n In terms of how to convey the context of an error, it can be part of messages themselves, as shown above. Many logging frameworks also support the notion of a Mapped Diagnostic Context (MDC), a map for propagating arbitrary key/value pairs into log messages. So if your messages are meant to show up in logs, setting contextual information to the MDC can be very useful. In Debezium this is used for instance to propagate the name of the affected connector, allowing Kafka Connect users to tell apart log messages originating from different connectors deployed to the same Connect cluster.\n     As far as propagating contextual information via log messages is concerned (as opposed to, say, error messages printed by a CLI tool), structured logging, typically in form of JSON, simplifies any downstream processing. By putting contextual information into separate attributes of a structured log entry, consumers can easily filter messages, ingest only specific sub-sets of messages based on their contents, etc.     In case of exceptions, the chain of exceptions leading to the root cause is an important contextual information, too. So I\u0026#8217;d recommend to always log the entire exception chain, rather than catching exceptions and only logging some substitute message instead.\n   The Error Itself On to the next part then, the description of the actual error itself. That\u0026#8217;s where you should describe what exactly happened in a concise way. Sticking to the examples above, the first message, including context and error description could read like so:\n  Couldn\u0026#8217;t parse config file: /etc/sample-config.properties; given snapshot mode 'nevr' isn\u0026#8217;t valid\n   And for the second one:\n  Failed to create an initial snapshot of the data; database user 'snapper' is lacking the required permissions\n   Other than that, there\u0026#8217;s not too much to be said here; try to be efficient: make messages as long as needed, and as short as possible. One idea could be to work with different variants of messages for the same kind of error, a shorter and a longer one. Which one is used could be controlled via log levels or some kind of \"verbose\" flag. Java developers may find Cédric Champeau\u0026#8217;s jdoctor library useful for implementing this. Personally, I haven\u0026#8217;t used such an approach yet, but it may be worth the effort for specific situations.\n   Mitigation Having established the context of the failure and what went wrong exactly, the last\u0026#8201;\u0026#8212;\u0026#8201;and oftentimes most interesting\u0026#8201;\u0026#8212;\u0026#8201;part is a description of how the user can overcome the error. What\u0026#8217;s the action they need to take in order to avoid it? This could be as simple as telling the user about the constraints and/or valid values in case of the config file example (i.e. akin to test failure messages, which show both expected and actual values):\n  Couldn\u0026#8217;t parse config file: /etc/sample-config.properties; given snapshot mode 'nevr' isn\u0026#8217;t valid (must be one of 'initial', 'always', 'never')\n   In case of the permission issue, you may clarify which ones are needed:\n  Couldn\u0026#8217;t take database snapshot: database user 'snapper' is lacking the required permissions 'SELECT', 'REPLICATION'\n   Alternatively, if longer mitigation strategies are required, you may point to a (stable!) URL in your reference documention which provides the required information:\n  Couldn\u0026#8217;t take database snapshot: database user 'snapper' is lacking the required permissions. Please see https://example.com/knowledge-base/snapshot-permissions/ for the complete set of necessary permissions\n   If some configuration change is required (for instance database or IAM permissions), your users will love you even more if you share that information in \"executable\" form, for instance as GRANT statements which they can simply copy, or vendor-specific CLI invocations such as aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/SomePolicy --role-name SomeRole.\n Speaking of external resources referenced in error messages, it\u0026#8217;s a great idea to have unique error codes as part of your messages (such as Oracle\u0026#8217;s ORA codes, or the error messages produced by WildFly and its components). Corresponding resources (either provided by yourself, or externally, for instance in answers on StackOverflow) will then be easy to find using your favourite search engine. Bonus points for adding a reference to your own canonical resource right to the error message itself:\n  Couldn\u0026#8217;t take database snapshot: database user 'snapper' is lacking the required permissions (DBZ-42). Please see https://dbz.codes/dbz-42/ for the complete set of necessary permissions\n   (That\u0026#8217;s a made-up example, we don\u0026#8217;t make use of this approach in Debezium currently; but I probably should look into buying the dbz.codes domain 😉).\n The key take-away is that you should not leave your users in the dark about what they need to do in order to address the error they ran into. Nothing is more frustrating than essentially being told \"You did it wrong!\", without getting hinted at what\u0026#8217;s the right thing to do instead.\n   General Best Practices Lastly, some practices in regards to error messages which I try to adhere to, and which I would generally recommend:\n   Uniform voice and style: The specific style chosen doesn\u0026#8217;t matter too much, but you should settle on either active vs. passive voice (\"couldn\u0026#8217;t parse config file\" vs. \"config file couldn\u0026#8217;t be parsed\"), apply consistent casing, either finish or not finishes messages with a dot, etc.; not a big thing, but it will make your messages a bit easier to deal with\n  One concept, one term: Avoid referring to the same concept from your domain using different terms in different error messages; similarly, avoid using the same term for multiple things. Use the same terms as in other places, e.g. your API documentation, reference guides etc.; The more consisent and unambiguous you are, the better\n  Don\u0026#8217;t localize error messages: This one is not as clear cut, but I\u0026#8217;d generally recommend to not translate error messages into other languages than English; Again, this all is not about user-facing error messages, but about messages geared towards software developers and ops folks, who generally should command reasonable English skills; depending on your audience and target market, translations to specific languages might make sense, in which case a common, unambiguous error code should definitely be part of messages, so as to facilitate searching for the error on the internet\n  Don\u0026#8217;t make error messages an API contract: In case consumers of your API should be able to react to different kinds of errors, they should not be required to parse any error messages in order to do so. Instead, raise an exception type which exposes a machine-processable error code, or raise specific exception types which can be caught separately by the caller\n  Be cautious about exposing sensitive data: if your library is in the business of handling and processing sensitive user data, make sure to to not create any privacy concerns; for instance, \"show actual vs. expected value\" may not pose a problem for values provided by an application developer or administrator; but it can pose a problem if the actual value is GDPR protected user data\n  Either raise an exception OR log an error, but not both: A given error should either be communicated by raising an exception or by logging an error. Otherwise, when doing both, as the exception will typically end up being logged via some kind of generic handler anyways, the user would see information about the same error in their logs twice, which only adds confusion\n  Fail early: This one is not so much about how to express error messages, but when to raise them; in general, the earlier, the better; a message at application start-up beats one later at runtime; a message at build time beats one at start-up, etc. Quicker feedback makes for shorter turn-around times for fixes and also helps to provide the context of any failures\n   With that all being said, what\u0026#8217;s your take on the matter? Any best practices you would recommend? Do you have any examples for particularly well (or poorly) crafted messages? Let me know in the comments below!\n  ","id":10,"publicationdate":"Jan 12, 2022","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003e\u003cem\u003eUpdate Jan 13: This post is \u003ca href=\"https://www.reddit.com/r/programming/comments/s2kcp7/whats_in_a_good_error_message/\"\u003ediscussed on Reddit\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003e\u003cem\u003eUpdate Feb 7: This post is \u003ca href=\"https://news.ycombinator.com/item?id=30234572\"\u003ediscussed on Hacker News\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eAs software developers, we\u0026#8217;ve all come across those annoying, not-so-useful error messages when using some library or framework: \u003cem\u003e\"Couldn\u0026#8217;t parse config file\"\u003c/em\u003e, \u003cem\u003e\"Lacking permission for this operation\"\u003c/em\u003e, etc.\nOk, ok, so \u003cem\u003esomething\u003c/em\u003e went wrong apparently; but what exactly? What config file? Which permissions? And what should you do about it?\nError messages lacking this kind of information quickly create a feeling of frustration and helplessness.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eSo what makes a good error message then?\nTo me, it boils down to three pieces of information which should be conveyed by an error message:\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"ulist\"\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eContext:\u003c/em\u003e What led to the error? What was the code trying to do when it failed?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eThe error itself:\u003c/em\u003e What exactly failed?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eMitigation:\u003c/em\u003e What needs to be done in order to overcome the error?\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e","tags":null,"title":"What's in a Good Error Message?","uri":"https://www.morling.dev/blog/whats-in-a-good-error-message/"},{"content":"🧸 It\u0026#8217;s Casey. Casey Cuddle.\n I am very happy to announce the first stable release of kcctl, a modern and intuitive command line client for Apache Kafka Connect!\n Forget about having to memorize and type the right REST API paths and curl flags; with kcctl, managing your Kafka connectors is done via concise and logically structured commands, modeled after the semantics of the kubectl tool known from Kubernetes.\n Starting now, kcctl is available via SDKMan, which means it\u0026#8217;s as easy as running sdk install kcctl for getting the latest kcctl release onto your Linux, macOS, or Windows x86 machine. For the best experience, also install the kcctl shell completion script, which not only \u0026lt;TAB\u0026gt;-completes command names and options, but also dynamic information such as connector, task, and logger names:\n 1 2 wget https://raw.githubusercontent.com/kcctl/kcctl/main/kcctl_completion . kcctl_completion    kcctl offers commands for all the common tasks you\u0026#8217;ll encounter when dealing with Kafka Connect, such as listing the available connector plug-ins, registering new connectors, changing their configuration, pausing and resuming them, changing log levels, and much more.\n Similar to kubectl, kcctl works with the notion of named configuration contexts. Contexts allow you to set up multiple named Kafka Connect environments (e.g. \"local\" and \"testing\") and easily switch between them, without having to specify the current Connect cluster URL all the time:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ kcctl config get-contexts NAME KAFKA CONNECT URI local http://localhost:8083 testing* http://localhost:8084 $ kcctl config use-context local Using context 'local' $ kcctl get plugins TYPE CLASS VERSION source io.debezium.connector.db2.Db2Connector 1.8.0.Final source io.debezium.connector.mongodb.MongoDbConnector 1.8.0.Final source io.debezium.connector.mysql.MySqlConnector 1.8.0.Final source org.apache.kafka.connect.file.FileStreamSourceConnector 3.0.0 source org.apache.kafka.connect.mirror.MirrorCheckpointConnector 1 source org.apache.kafka.connect.mirror.MirrorHeartbeatConnector 1 source org.apache.kafka.connect.mirror.MirrorSourceConnector 1 sink org.apache.kafka.connect.file.FileStreamSinkConnector 3.0.0    Once you\u0026#8217;ve set up a kcctl context, you can start using the tool for managing your connectors. Here is a video which shows a typical workflow in kcctl (note this recording shows an earlier version of kcctl, there\u0026#8217;s a few less commands and the notion of contexts has is slightly changed since then):\n    As shown in the video, connectors are registered and updated via kcctl apply. This command can also read input from stdin, which for instance comes in handy when templating connector configuration using Jsonnet and setting up multiple similar connectors at once:\n   To learn more about these and all the other commands available in kcctl, run kcctl --help.\n Discussion and Outlook kcctl offers an easy yet very powerful way for solving your day-to-day tasks with Kafka Connect. In comparison to using the REST API directly via clients such as curl or httpie, kcctl as a dedicated tool offers commands which are more concise and intuitive; also its output is logically organized, using colored formatting to highlight key information. It has become an invaluable tool for my own work on Debezium, e.g. when testing, or doing some demo. These days, I find myself very rarely using the REST API directly any more.\n I hope kcctl becomes useful helper for folks working with Kafka Connect. As such, I see it as a complement to other means of interacting with Kafka Connect. Sometimes a CLI client may be what does the job the best, while at other times you may prefer to work with a graphical user interface such as Debezium UI or the vendor-specific consoles of managed connector services, Kubernetes operators such as Strimzi, Terraform, or perhaps even a Java API. It\u0026#8217;s all about options!\n While all the typical Kafka Connect workflows are supported by kcctl already, there\u0026#8217;s quite a few additional features I\u0026#8217;d love to see. First and foremost, the ability to display (and reset) the offsets of Kafka Connect source connectors. Work on that is well underway, and I expect this to be available very soon. There also should be support for different output formats such as JSON, improving useability in conjunction with other CLI tools such as jq. The restart command should be expanded, so as to take advantage of the API for restarting all (failed) connector tasks added in Kafka Connect 3.0. Going beyond the scope of supporting plain Kafka Connect, there could also be connector specific commands, such as an option for compacting the history topic of Debezium connectors. Of course, your feature requests are welcome, too! Please log an issue in the kcctl project with your proposals for additions to the tool. And while at it, we\u0026#8217;d also love to welcome you as a stargazer 🌟 to the project!\n Lastly, a big thank you to all the amazing people who have contributed to kcctl up to this point:\n Andres Almiray, Guillaume Smet, Hans-Peter Grahsl, Iskandar Abudiab, Jay Patel, Karim ElNaggar, Michael Simons, Mickael Maison, Oliver Weiler, Sergey Nuyanzin, Siddique Ahmad, Thomas Dangleterre, and Tony Foster!\n You\u0026#8217;re the best 🧸!\n  ","id":11,"publicationdate":"Dec 21, 2021","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003e🧸 \u003cem\u003eIt\u0026#8217;s Casey. Casey Cuddle.\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eI am very happy to announce the first stable release of \u003ca href=\"https://github.com/kcctl/kcctl\"\u003ekcctl\u003c/a\u003e,\na modern and intuitive command line client for \u003ca href=\"https://kafka.apache.org/documentation/#connect\"\u003eApache Kafka Connect\u003c/a\u003e!\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eForget about having to memorize and type the right REST API paths and curl flags;\nwith kcctl, managing your Kafka connectors is done via concise and logically structured commands,\nmodeled after the semantics of the kubectl tool known from Kubernetes.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Announcing the First Release of kcctl","uri":"https://www.morling.dev/blog/announcing-first-release-of-kcctl/"},{"content":"This page gives an overview over some talks I have done over the last years. I have spoken at large conferences such as QCon San Francisco, Devoxx and JavaOne, local meet-ups as well as company-internal events, covering topics such as Debezium and Change Data Capture, Bean Validation, NoSQL and more.\n If you\u0026#8217;d like to have me as a speaker at your conference or meet-up, please get in touch.\n 2022   Devoxx (Antwerp): Taming Kafka Connect with kcctl\n  Devoxx (Antwerp): Keep Your Cache Always Fresh with Debezium!\n  Current (Austin): Keep Your Cache Always Fresh with Debezium!\n  code.talks (Hamburg): Change Data Streaming Patterns für Verteilte Systeme\n  Uptime (Amsterdam): Keep Your Cache Always Fresh with Debezium!\n  Java User Group Hamburg: Mit Java-18-APIs zum Mond und weiter\n  JBCONConf (Barcelona): Keep Your Cache Always Fresh with Debezium!\n  Kafka Summit London: Keep Your Cache Always Fresh with Debezium! (recording, slides)\n  Carnegie Mellon University \"Vaccination Database Tech Talks\" (online): Open-source Change Data Capture With Debezium (recording, slides)\n  jChampionsConference (online): Continuous Performance Regression Testing with JfrUnit (recording, slides)\n     2021   DevNation Tech Talk (online): To the Moon and Beyond With Java 17 APIs!\n  Red Hat Summit Connect Developer Experience (online, German): Change Data Streaming Patterns für Verteilte Systeme\n  Flink Forward (joint presentation with Hans-Peter Grahsl; online): Change Data Streaming Patterns in Distributed Systems\n  Voxxed Days Romania (joint presentation with Hans-Peter Grahsl; online): Dissecting our Legacy: The Strangler Fig Pattern with Apache Kafka, Debezium and MongoDB\n  P99 Conf (online): Continuous Performance Regression Testing with JfrUnit\n  Accento (online): To the Moon and Beyond With Java 17 APIs!; Panel The Present and Future of Java (17)\n  Heise betterCode() Java (online): Mit Java-17-APIs zum Mond und weiter\n  Kafka Summit Americas (joint presentation with Hans-Peter Grahsl; online): Dissecting our Legacy: The Strangler Fig Pattern with Debezium, Apache Kafka \u0026amp; MongoDB\n  Apache Pinot Meetup (joint presentation with Kenny Bastani; online): Analyzing Real-time Order Deliveries using CDC with Debezium and Pinot\n  MongoDB World (joint presentation with Hans-Peter Grahsl; online): Dissecting our Legacy: The Strangler Fig Pattern with Apache Kafka, Debezium and MongoDB\n  jLove (joint presentation with Hans-Peter Grahsl; online): Change Data Streaming Patterns in Distributed Systems\n  Berlin Buzzwords (joint presentation with Hans-Peter Grahsl; online): Change Data Streaming Patterns in Distributed Systems (recording, slides)\n  The Developer\u0026#8217;s Conference (joint presentation with Hans-Peter Grahsl; online): Change Data Streaming Patterns in Distributed Systems  (slides)\n  Kafka Summit Europe (joint presentation with Hans-Peter Grahsl; online): Advanced Change Data Streaming Patterns in Distributed Systems (recording, slides)\n  DevNation Tech Talk (online): Continuous performance regression testing with JfrUnit (recording, slides)\n  Bordeaux JUG (joint presentation with Katja Aresti; online): Don\u0026#8217;t fear outdated caches\u0026#8201;\u0026#8212;\u0026#8201;change data capture to the rescue! Let\u0026#8217;s discover Infinispan and Debezium\n  jChampionsConference (joint presentation with Andres Almiray; online): Plug-in Architectures for Java with Layrry \u0026amp; the Java Module System\n     2020   JokerConf (online): Change data capture pipelines with Debezium and Kafka Streams\n  Virtual JUG (joint presentation with Andres Almiray; online): Plug-in Architectures With Layrry and the Java Module System (recording, slides)\n  QConPlus (online): Serverless Search for My Blog With Java, Quarkus, \u0026amp; AWS Lambda\n  JFall (joint presentation with Andres Almiray; online): Plug-in Architectures for Java With Layrry and the Java Module System\n  Java Day Istanbul (online): Change Data Streaming Use Cases With Apache Kafka \u0026amp; Debezium\n  Great International Developer Summit (online): Change Data Capture Pipelines with Debezium and Kafka Streams\n  Kafka Summit (online): Change Data Capture Pipelines with Debezium and Kafka Streams\n  Red Hat Summit Virtual Experience: Data integration patterns for microservices with Debezium and Apache Kafka\n     2019   Nordic Coding, Kiel: Quarkus - Supersonic Subatomic Java\n  Java User Group Paderborn: Change Data Streaming Use Cases mit Debezium und Apache Kafka\n  QCon San Francisco: Practical Change Data Streaming Use Cases With Apache Kafka \u0026amp; Debezium\n  JokerConf, St. Petersburg: Practical change data streaming use cases with Apache Kafka and Debezium\n  JavaZone, Oslo: Change Data Streaming For Microservices With Apache Kafka and Debezium\n  MicroXchg, Berlin: Change Data Streaming Patterns For Microservices With Debezium\n  JavaLand, Brühl\n  Change Data Streaming für Microservices mit Debezium\n  Das Annotation Processing API - Use Cases und Best Practices\n     RivieraDev, Sophia Antipolis: Practical Change Data Streaming Use Cases With Apache Kafka and Debezium\n  Kafka Summit London: Change Data Streaming Patterns For Microservices With Debezium\n  Red Hat Summit, Boston\n  Bridging microservice boundaries with Apache Kafka and Debezium (hands-on lab)\n  Change data streaming patterns for microservices with Debezium\n     Red Hat Modern Integration and Application Development Day, Milano: Data Strategies for Microservices: Change Data Capture with Debezium\n     2018   Devoxx Morocco, Marrakesh\n  Change Data Streaming Patterns for Microservices With Debezium\n  Map me if you can! Painless bean mappings with MapStruct\n     Kafka Summit San Francisco: Change Data Streaming Patterns for Microservices With Debezium\n  VoxxedDays Microservices Paris: Data Streaming for Microservices using Debezium\n  JUG Saxony Day, Dresden: Streaming von Datenbankänderungen mit Debezium\n  Java User Group Darmstadt: Streaming von Datenbankänderungen mit Debezium\n  JavaLand, Brühl: Hibernate - State of the Union; Migrating to Java 9 Modules with ModiTect\n  RivieraDev, Sophia Antipolis: Data Streaming for Microservices using Debezium\n  Red Hat Summit, San Francisco: Running data-streaming applications with Kafka on OpenShift (hands-on lab)\n  Java User Group Münster, Streaming von Datenbankänderungen mit Debezium\n     2017   JavaZone, Oslo: Keeping Your Data Sane with Bean Validation 2.0\n  code.talks, Hamburg: Neues in Bean Validation 2.0 - Support für Java 8 und mehr (recording)\n  JavaOne, San Francisco\n  Keeping Your Data Sane with Bean Validation 2.0\n  NoSQL? Have it Your Way!\n     Devoxx Belgium, Antwerp\n  Streaming Database Changes with Debezium\n  Short talks on Bean Validation 2.0 and MapStruct\n     jdk.io, Copenhagen: Keeping Your Data Sane with Bean Validation 2.0\n  RivieraDev, Sophia Antipolis: Keeping Your Data Sane with Bean Validation 2.0\n  JavaLand, Brühl\n  Bean Validation 2.0\n  Hibernate Search and Elasticsearch\n        2016   JavaZone, Oslo: From Hibernate to Elasticsearch in no time\n     ","id":12,"publicationdate":"Dec 4, 2021","section":"","summary":"This page gives an overview over some talks I have done over the last years. I have spoken at large conferences such as QCon San Francisco, Devoxx and JavaOne, local meet-ups as well as company-internal events, covering topics such as Debezium and Change Data Capture, Bean Validation, NoSQL and more.\n If you\u0026#8217;d like to have me as a speaker at your conference or meet-up, please get in touch.\n 2022   Devoxx (Antwerp): Taming Kafka Connect with kcctl","tags":null,"title":"Conferences","uri":"https://www.morling.dev/conferences/"},{"content":"I am very happy to announce the availability of the OSS Quickstart Archetype!\n Part of the ModiTect family of open-source projects, this is a Maven archetype which makes it very easy to bootstrap new Maven-based open-source projects, satisfying common requirements such as configuring plug-in versions, and adhering to best practices like auto-formatting the source code. Think Maven Quickstart Archetype and friends, but more modern, complete, and opinionated.\n The Challenge When bootstrapping new Maven-based projects, be it long-running ones, a short-lived proof-of-concept projects, or just some quick demo you\u0026#8217;d like to publish on GitHub, there\u0026#8217;s always some boilerplate involved: creating the POM with the right plug-in versions and configurations, preparing CI e.g. on GitHub Actions, providing a license file, etc.\n While you could try and copy (parts of) an existing project you already have, Maven has a better answer to this problem: archetypes, pre-configured project templates which can be parameterized to some degree and which let you create new projects with just a few steps. Unfortunately, the canonical Maven quickstart archetype is rather outdated, creating projects for Java 1.7, using JUnit 4, etc.\n   The OSS Quickstart Archetype The OSS (open-source software) quickstart archetype is meant as a fresh alternative, not only providing more current defaults and dependency versions, but also going beyond what\u0026#8217;s provided by the traditional quickstart archetype. More specifically, it\n   defines up-to-date versions of all plug-ins in use, as well as of JUnit 5 and AssertJ (the opinionated part ;)\n  enforces all plug-in versions to be defined via the Maven enforcer plug-in\n  provides a license file and uses the license Maven plug-in for formatting/checking license headers in all source files\n  defines a basic set up for CI on GitHub Actions, building the project upon each push to the main branch of your repository and for each PR\n  configures plug-ins for auto-formatting code and imports (I told you, it\u0026#8217;s opinionated)\n  defines a -Dquick option for skipping all non-essential plug-ins, allowing you to produce the project\u0026#8217;s JAR as quickly as possible\n  (optionally) provides a module-info.java descriptor\n   And most importantly, opening braces are not on the next line. We all agree nobody likes that, right?! Using the OSS Quickstart Archetype for bootstrapping a new project is as simple as running the following command:\n 1 2 3 4 5 6 7 8 mvn archetype:generate -B \\ -DarchetypeGroupId=org.moditect.ossquickstart \\ -DarchetypeArtifactId=oss-quickstart-simple-archetype \\ -DarchetypeVersion=1.0.0.Alpha1 \\ -DgroupId=com.example.demos \\ -DartifactId=fancy-project \\ -Dversion=1.0.0-SNAPSHOT \\ -DmoduleName=com.example.fancy    Just a few seconds later, and you\u0026#8217;ll have a new project applying all the configuration above, ready for you to start some open-source awesomeness.\n   Outlook Version 1.0.0.Alpha1 of the OSS Quickstart Archetype is available today on Maven Central, i.e. you can starting using it for bootstrapping new projects right now. It already contains most of the things I wanted it to have, but there\u0026#8217;s also a few more improvements I would like to make:\n   Add the Maven wrapper (#1)\n  Make the license of the generated project configurable; currently, it uses Apache License, version 2. I\u0026#8217;d like to make this an option of the archetype, which would let you choose between this license and a few other key open-source licenses, like MIT and BSD 3-clause (#2)\n  Provide a variant of the archetype for creating multi-module Maven projects (#7)\n  Add basic CheckStyle configuration (also skippable via -Dquick, #10)\n   Any contributions for implementing these, as well as other feature requests are highly welcome. Note the idea is to keep these archetypes lean and mean, i.e. they should only contain widely applicable features, leaving more specific things for the user to add after they created a project with the archetype.\n Happy open-sourcing!\n Many thanks to Andres Almiray for setting up the release pipeline for this project, using the amazing JReleaser tool!\n  ","id":13,"publicationdate":"Dec 2, 2021","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eI am very happy to announce the availability of the \u003ca href=\"https://github.com/moditect/oss-quickstart\"\u003eOSS Quickstart Archetype\u003c/a\u003e!\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003ePart of the \u003ca href=\"https://github.com/moditect/\"\u003eModiTect\u003c/a\u003e family of open-source projects,\nthis is a Maven archetype which makes it very easy to bootstrap new Maven-based open-source projects,\nsatisfying common requirements such as configuring plug-in versions, and adhering to best practices like auto-formatting the source code.\nThink \u003ca href=\"https://maven.apache.org/archetypes/maven-archetype-quickstart/scm.html\"\u003eMaven Quickstart Archetype\u003c/a\u003e and friends, but more modern, complete, and opinionated.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Introducing the OSS Quickstart Archetype","uri":"https://www.morling.dev/blog/introducing-oss-quickstart-archetype/"},{"content":"The other day, I came across an interesting thread in the Java sub-reddit, with someone asking: \"Has anyone attempted to write logs directly to Kafka?\". This triggered a number of thoughts and questions for myself, in particular how one should deal in an application when an attempt to send messages to Kafka fails, for instance due to some network connectivity issue? What do you do when you cannot reach the Kafka broker?\n While the Java Kafka producer buffers requests internally (primarily for performance reasons) and also supports retries, you cannot do so indefinitely (or can you?), so I went to ask the Kafka community on Twitter how they would handle this situation:\n #Kafka users: how do you deal in producers with brokers not being available? Take a use case like sending logs; you don\u0026#39;t want to fail your business process due to Kafka issues here, it\u0026#39;s fine do this later on. Large producer buffer and retries? Some extra buffer (e.g. off-heap)?\n\u0026mdash; Gunnar Morling 🌍 (@gunnarmorling) November 27, 2021   This question spawned a great discussion with tons of insightful replies (thanks a lot to you all!), so I thought I\u0026#8217;d try and give an overview on the different comments and arguments. As with everything, the right strategy and solution depends on the specific requirements of the use case at hand; in particular whether you can or cannot afford for potential inconsistencies between the state of the caller of your application, its own state, and the state in the Kafka cluster.\n As an example, let\u0026#8217;s consider an application which exposes a REST API for placing purchase orders. Acknowledging such a request while actually failing to send a Kafka message with the purchase order to some fulfillment system would be pretty bad: the user would believe their order has been received and will be fulfilled eventually, whereas that\u0026#8217;s actually not the case.\n On the other hand, if the incoming request was savely persisted in a database, and a message is sent to Kafka only for logging purposes, we may be fine to accept this inconsistency between the user\u0026#8217;s state (\"my order has been received\"), the application\u0026#8217;s state (order is stored in the database), and the state in Kafka (log message got lost; not ideal, but not the end of the world either).\n Understanding these different semantics helps to put the replies to the question into context. There\u0026#8217;s one group of replies along the lines of \"buffer indefinitely, block inbound requests until messages are sent\", e.g. by Pere Urbón-Bayes:\n This would certainly depend on the client used and your app use case. Generally speaking, retry forever and block if the buffer is full, leave time for broker to recover, with backpressure.\nif backpressure not possible, cause use case, off-load off-heap for later recovery.\n\u0026mdash; Pere Urbón-Bayes (@purbon) November 28, 2021   This strategy makes a lot of sense if you cannot afford any inconsistency between the state of the different actors at all: e.g. when you\u0026#8217;d rather tell the user that you cannot receive their purchase order right now, instead of being at the risk of telling them that you did, whereas you actually didn\u0026#8217;t.\n What though, if we don\u0026#8217;t want to let the availability of a resource like Apache Kafka\u0026#8201;\u0026#8212;\u0026#8201;which is used for asynchronous message exchanges to begin with\u0026#8201;\u0026#8212;\u0026#8201;impact the availability of our own application? Can we somehow buffer requests in a safe way, if they cannot be sent to Kafka right away? This would allow to complete the inbound request, while hopefully still avoiding any inconsistencies, at least eventually.\n Now simply buffering requests in memory isn\u0026#8217;t reliable in any meaningful sense of the word; if the producing application crashes, any unsent messages will be lost, making this approach not different in terms of reliability from working with ack = 0, i.e. not waiting for any acknowledgements from the Kafka broker. It may be useful for pure fire-and-forget use cases, where you don\u0026#8217;t care about delivery guarantees at at all, but these tend to be rare.\n Multiple folks therefore suggested more reliable means of implementing such buffering, e.g. by storing un-sent messages on disk or by using some local, persistent queuing implementation. Some have built solutions using existing open-source components, as Antón Rodriguez and Josh Reagan suggest:\n I usually retry forever, specially when reading from another topic because we can apply backpressure. In some cases, discard after some time is ok. Very rarely off-heap with ChronicleQueue or MapsDB. I have considered but never used an external service as DLQ or a Kafka mesh\n\u0026mdash; Antón (@antonmry) November 27, 2021   Embedded broker and in-vm protocol. Either ActiveMQ or Artemis work great.\n\u0026mdash; Josh Reagan (@joshdreagan) November 28, 2021   You even could think of having a Kafka cluster close by (which then may have other accessibility characteristics than your \"primary\" cluster e.g. running in another availability zone) and keeping everything in sync via tools such as MirrorMaker 2. Others, like Jonathan Santilli, create their own custom solutions by forking existing projects:\n I forked Apache Flume and modified it to used a WAL on the disk, so, messages are technically sent, but store on disk, when the Broker is available, the local queue gets flushed, all transparent for the producer.\n\u0026mdash; Jonathan Santilli (@pachilo) November 27, 2021   Also ready-made wrappers aound the producer exists, e.g. in Wix' Greyhound Kafka client library, which supports producing via local disk as per Derek Moore:\n I built a proprietary \u0026quot;data refinery\u0026quot; on Kafka for @fanthreesixty and we built ourselves libraries not dissimilar to https://t.co/uQdepGHTzj\n\u0026mdash; Derek Moore (@derekm00r3) November 27, 2021   But there be dragons! Persisting to disk will actually not be any better at all, if it\u0026#8217;s for instance an ephermeral disk of a Kubernetes pod which gets destroyed after an application crash. But even when using persistent volumes, you may end up with an inherently unreliable solution, as Mic Hussey points out:\n These are two contradictory requirements 😉 Sooner or later you will run out of local storage capacity. And unless you are very careful you end up moving from a well understood shared queue to a hacked together implicit queue.\n\u0026mdash; Mic Hussey (@hussey_mic) November 29, 2021   So it shouldn\u0026#8217;t come at a surprise that people in this situation have been looking at alternatives, e.g. by using DynamoDB or S3 as an intermediary buffer; The team around Natan Silnitsky working on Greyhound at Wix are exploring this option currently:\n So instead we want to fallback only on failure to send. In addition we want to skip the disk all together, because recovery mechanism when a pod is killed in #Kubernetes is too complex (involves a daemonset...), So we\u0026#39;re doing a POC, writing to #DynamoDB/#S3 upon failure 2/3 🧵\n\u0026mdash; Natan Silnitsky (@NSilnitsky) November 29, 2021   At this point it\u0026#8217;s worth thinking about failure domains, though. Say your application is in its own network and it cannot write to Kafka due to some network split, chances are that it cannot reach other services like S3 either. So another option could be to use a datastore close by as a buffer, for instance a replicated database running on the same Kubernetes cluster or at least in the same availability zone.\n If this reminds you of change data capture (CDC) and the outbox pattern, you\u0026#8217;re absolutely right; multiple folks made this point as well in the conversation, including Natan Silnitsky and R.J. Lorimer:\n Then a dedicated service will listen to #DynamoDB CDC events and produce to #ApacheKafka including payload, key, headers, etc...\n\u0026mdash; Natan Silnitsky (@NSilnitsky) November 29, 2021   For our event sourcing systems the event being delivered actually is critical. For \u0026quot;pure\u0026quot; cqrs services, Kafka being down is paramount to not having a db so we fail. Other systems use a transactional outbox that persists in the db. If Kafka is down it sits there until ready.\n\u0026mdash; R.J. Lorimer (He/Him) (@realjenius) November 27, 2021   As Kacper Zielinski tells us, this approach is an example of a staged event-driven architecture, or SEDA for short:\n Outbox / SEDA to rescue here. Not sure if any “retry” can guarantee you more than “either you will loose some messages or fail the business logic by eating all resources” :)\n\u0026mdash; Kacper Zielinski (@xkzielinski) November 27, 2021   In this model, a database serves as the buffer for persisting messages before they are sent to Kafka, which makes for for a highly reliable solution, provided the right degree of redundancy is implemented e.g. in form of replicas. In fact, if your application needs to write to a database anyways, \"sending\" messages to Kafka via an outbox table and CDC tools like Debezium is a great way to avoid any inconsistencies between the state in the database and Kafka, without incurring any unsafe dual writes.\n But of course there is a price to pay here too: end-to-end latency will be increased when going through a database first and then to Kafka, rather than going to Kafka directly. You also should keep in mind that the more moving pieces your solution has, the more complex to operate it will become of course, and the more subtle and hard-to-understand failure modes and edge cases it will have.\n An excellent point is made by Adam Kotwasinski by stating that it\u0026#8217;s not a question of whether things will go wrong, but only when they will go wrong, and that you need to have the right policies in place in order to be prepared for that:\n For some of my usecases I have a wrapper for Kafka\u0026#39;s producer that requires users to _explicitly_ set up policies like retry/backoff/drop. It allows my customers to think about outages (that will happen!) up front instead of being surprised. Each usecase is different.\n\u0026mdash; Adam Kotwasinski (@AKotwasinski) November 28, 2021   In the end it\u0026#8217;s all about trade-offs, probabilities and acceptable risks. For instance, would you receive and acknowledge that purchase order request as long as you can store it in a replicated database in the local availability zone, or would you rather reject it, as long as you cannot safely persist it in a multi-AZ Kafka cluster?\n These questions aren\u0026#8217;t merely technical ones any longer, but they require close collaboration with product owners and subject matter experts in the business domain at hand, so to make the most suitable decisions for your specific situation. Managed services with defined SLAs guaranteeing high availability values can make the deciding difference here, as Vikas Sood mentions:\n That’s why we decided to go with a managed offering to avoid disruptions in some critical processes.Some teams still had another decoupling layer (rabbit) between producers and Kafka. Was never a huge fan of that coz it simply meant more points of failure.\n\u0026mdash; Vikas Sood (@Sood1Vikas) November 27, 2021   Thanks a lot again to everyone chiming in and sharing their experiences, this was highly interesting and insightful! You have further ideas and thoughts to share? Let me and the community at large know either by leaving a comment below, or by replying to the thread on Twitter. I\u0026#8217;m also curious about your feedback on this format of putting a Twitter discussion into some expanded context. It\u0026#8217;s the first time I\u0026#8217;ve been doing it, and I\u0026#8217;d be eager to know whether you find it useful or not. Thanks!\n","id":14,"publicationdate":"Nov 29, 2021","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe other day, I came across an \u003ca href=\"https://www.reddit.com/r/java/comments/r2z17a/has_any_one_attempted_to_write_logs_directly_to/\"\u003einteresting thread\u003c/a\u003e in the Java sub-reddit, with someone asking:\n\"Has anyone attempted to write logs directly to Kafka?\".\nThis triggered a number of thoughts and questions for myself,\nin particular how one should deal in an application when an attempt to send messages to Kafka fails,\nfor instance due to some network connectivity issue?\nWhat do you do when you cannot reach the Kafka broker?\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"O Kafka, Where Art Thou?","uri":"https://www.morling.dev/blog/kafka-where-art-thou/"},{"content":"If you work on any kind of software library, ensuring backwards-compatibility is a key concern: if there\u0026#8217;s one thing which users really dislike, it is breaking changes in a new version of a library. The rules of what can (and cannot) be changed in a Java API without breaking existing consumers are well defined in the Java language specification (JLS), but things can get pretty interesting in certain corner cases.\n The Eclipse team provides a comprehensive overview about API evolution guidelines in their wiki. When I shared the link to this great resource on Twitter the other day, I received an interesting reply from Lukas Eder:\n  I wish Java had a few tools to prevent some cases of binary compatibility breakages. E.g. when refining a method return type, I\u0026#8217;d like to keep the old method around in byte code (but not in source code). I think kotlin has such tools?   In the remainder of this post, I\u0026#8217;d like to provide some more insight into that problem mentioned by Lukas, and how it can be addressed using an open-source tool called Bridger.\n The Problem Let\u0026#8217;s assume we have a Java library which provides a public class and method like this:\n 1 2 3 4 5 6 public class SomeService { public Number getSomeNumber() { return 42L; } }    The library is released as open-source and it gets adopted quickly by the community; it\u0026#8217;s a great service after all, providing 42 as the answer, right?\n After some time though, people start to complain: instead of the generic Number return type, they\u0026#8217;d rather prefer a more specific return type of Long, which for instance offers the compareTo() method. Since the returned value is always a long value indeed (and no other Number subtype such as Double), we agree that the initial API definition wasn\u0026#8217;t ideal, and we alter the method definition, now returning Long instead.\n But soon after we\u0026#8217;ve released version 2.0 of the library with that change, users report a new problem: after upgrading to the new version, they suddenly get the following error when running their application:\n 1 2 java.lang.NoSuchMethodError: 'java.lang.Number dev.morling.demos.bridgemethods.SomeService.getSomeNumber()' at dev.morling.demos.bridgemethods.SomeClientTest.shouldReturn42(SomeClientTest.java:27)    That doesn\u0026#8217;t look good! Interestingly, other users don\u0026#8217;t have a problem with version 2.0, so what is going on here? In order to understand that, let\u0026#8217;s take a look at how this method is used, in source code and in Java binary code. First the source code:\n 1 2 3 4 5 6 7 public class SomeClient { public String getSomeNumber() { SomeService service = new SomeService(); return String.valueOf(service.getSomeNumber()); } }    Rather unspectacular; so let\u0026#8217;s use javap to examine the byte code of that class:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  public java.lang.String getSomeNumber(); descriptor: ()Ljava/lang/String; flags: (0x0001) ACC_PUBLIC Code: stack=2, locals=2, args_size=1 0: new #7 // class dev/morling/demos/bridgemethods/SomeService 3: dup 4: invokespecial #9 // Method dev/morling/demos/bridgemethods/SomeService.\"\u0026lt;init\u0026gt;\":()V 7: astore_1 8: aload_1 9: invokevirtual #10 // Method dev/morling/demos/bridgemethods/SomeService.getSomeNumber:()Ljava/lang/Number; 12: invokestatic #14 // Method java/lang/String.valueOf:(Ljava/lang/Object;)Ljava/lang/String; 15: areturn LineNumberTable: line 21: 0 line 22: 8 LocalVariableTable: Start Length Slot Name Signature 0 16 0 this Ldev/morling/demos/bridgemethods/SomeClient; 8 8 1 service Ldev/morling/demos/bridgemethods/SomeService;    The interesting part is the invokevirtual at label 9; that\u0026#8217;s the invocation of the SomeService::getSomeNumber() method, and as we see, the return type of the invoked method is part of the byte code of that invocation, too. As developers writing code in the Java language, this might come at a suprise at first, as we tend to think of just a method\u0026#8217;s names and its parameter types as the method signature. For instance, we may not declare two methods which only differ by their return type in the same Java class. But from the perspective of the Java runtime, the return type of a method is part of method signatures as well.\n This explains the error reports we got from our users: when changing the method return type from Number to Long, we did a change that broke the binary compatibility of our library. The JVM was looking for a method SomeService::getSomeNumber() returning Number, but it couldn\u0026#8217;t find it in the class file of version 2.0 of our service.\n It also explains why not all the users reported that problem: those that recompiled their own application when upgrading to 2.0 would not run into any issues, as the compiler would simply use the new version of the method and put the invocation of that signature into the class files of any callers. Only those users who did not re-compile their code encountered the problem, i.e. the change actually was source-compatible.\n   Bridge Methods to the Rescue At this point you might wonder: Isn\u0026#8217;t it possible to refine method return types in sub-classes? How does that work then? Indeed it\u0026#8217;s true, Java does support co-variant return types, i.e. a sub-class can override a method using a more specific return type than declared in the super-type:\n 1 2 3 4 5 6 7 public class SomeSubService extends SomeService { @Override public Long getSomeNumber() { return 42L; } }    To make this work for a client coded against the super-type, the Java compiler uses a neat trick: it injects a so-called bridge method into the class file of the sub-class, which has the signature of the overridden method and which calls the overriding method. This is how this looks like when disassembling the SomeSubService class file:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public java.lang.Long getSomeNumber(); (1) descriptor: ()Ljava/lang/Long; flags: (0x0001) ACC_PUBLIC Code: stack=2, locals=1, args_size=1 0: ldc2_w #14 // long 42l 3: invokestatic #21 // Method java/lang/Long.valueOf:(J)Ljava/lang/Long; 6: areturn LineNumberTable: line 22: 0 LocalVariableTable: Start Length Slot Name Signature 0 7 0 this Ldev/morling/demos/bridgemethods/SomeSubService; public java.lang.Number getSomeNumber(); (2) descriptor: ()Ljava/lang/Number; flags: (0x1041) ACC_PUBLIC, ACC_BRIDGE, ACC_SYNTHETIC (3) Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokevirtual #24 // Method getSomeNumber:()Ljava/lang/Long; 4: areturn LineNumberTable: line 18: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this Ldev/morling/demos/bridgemethods/SomeSubService;      1 The overriding method as defined in the sub-class   2 The bridge method with the signature from the super-class, invoking the overriding method   3 The injected method has the ACC_BRIDGE and ACC_SYNTHETIC modifiers    That way, a client compiled against the super-type method will first invoke the bridge method, which in turn delegates to the overriding method of the sub-class, providing the late binding semantics we\u0026#8217;d expect from Java.\n     Another situation where the Java compiler relies on bridge methods is compiling sub-types of generic super-classes or interfaces. Refer to the Java Tutorial to learn more about this.       Creating Bridge Methods Ourselves So as we\u0026#8217;ve seen, with bridge methods, there is a tool in the box to ensure compatibility in case of refining return types in sub-classes. Which brings us back to Lukas' question from the beginning: is there a way for using the same trick for ensuring compatibility when evolving our API across library versions?\n Now you can\u0026#8217;t define a bridge method using the Java language, this concept just doesn\u0026#8217;t exist at the language level. So I thought about quickly hacking together a PoC for this using the ASM bytecode manipulation toolkit; but what\u0026#8217;s better than creating open-source? Re-using existing open-source! As it turns out, there\u0026#8217;s a tool for that very purpose exactly: Bridger, created by my fellow Red Hatter David M. Lloyd.\n Bridger lets you create your own bridge methods, using ASM to apply the required class file transformations for turning a method into a bridge method. It comes with a Maven plug-in for integrating this transformation step into your build process. Here\u0026#8217;s the plug-in configuration we need:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.jboss.bridger\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;bridger\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.5.Final\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;weave\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;process-classes\u0026lt;/phase\u0026gt; (1) \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;transform\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; (2) \u0026lt;groupId\u0026gt;org.ow2.asm\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;asm\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;9.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/plugin\u0026gt;      1 Bind the transform goal to the process-classes build lifecycle phase, so as to modify the classes produced by the Java compiler   2 Use the latest version of ASM, so we can work with Java 17    With the plug-in in place, you can define bridge methods like so, using the $$bridge name suffix (seems the syntax highligher doesn\u0026#8217;t like the $ signs in identifiers\u0026#8230;\u0026#8203;):\n 1 2 3 4 5 6 7 8 9 10 11 12 13 public class SomeService { /** * @hidden (1) */ public Number getSomeNumber$$bridge() { (2) return getSomeNumber(); } public Long getSomeNumber() { return 42L; } }      1 By means of the @hidden JavaDoc tag (added in Java 9), this method will be excluded from the JavaDoc generated for our library   2 The bridge method to be; the name suffix will be removed by Bridger, i.e. it will be named getSomeNumber; it will also have the ACC_BRIDGE and ACC_SYNTHETIC modifiers    And that\u0026#8217;s how the byte code of SomeService looks like after Bridger applied the transformation:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public java.lang.Number getSomeNumber(); descriptor: ()Ljava/lang/Number; flags: (0x1041) ACC_PUBLIC, ACC_BRIDGE, ACC_SYNTHETIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokevirtual #16 // Method getSomeNumber:()Ljava/lang/Long; 4: areturn LineNumberTable: line 21: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this Ldev/morling/demos/bridgemethods/SomeService; public java.lang.Long getSomeNumber(); descriptor: ()Ljava/lang/Long; flags: (0x0001) ACC_PUBLIC Code: stack=2, locals=1, args_size=1 0: ldc2_w #17 // long 42l 3: invokestatic #24 // Method java/lang/Long.valueOf:(J)Ljava/lang/Long; 6: areturn LineNumberTable: line 25: 0 LocalVariableTable: Start Length Slot Name Signature 0 7 0 this Ldev/morling/demos/bridgemethods/SomeService;    With that, we have solved the challenge: utilizing a bridge method, we can rectify the glitch in the version 1.0 API and refine the method return type in a new version of our library, without breaking source nor binary compatibility with existing users.\n By means of the @hidden JavaDoc tag, the source of our bridge method won\u0026#8217;t show up in the rendered documentation (which would be rather confusing), and marked as a synthetic bridge method in the class file, it also won\u0026#8217;t show up when looking at the JAR in an IDE.\n If you\u0026#8217;d like to start your own explorations of Java bridge methods, you can find the complete source code of the example in this GitHub repo. Useful tools for tracking API changes and identifying any potential breaking changes include SigTest (we use this one for instance in the Bean Validation specification to ensure backwards compatibility) and Revapi (which we use in Debezium). Lastly, here\u0026#8217;s a great blog post by Stuart Marks, where he describes how even the seemingly innocent addition of a Java default method to a widely used (and implemented) interface may lead to problems in the real world.\n  ","id":15,"publicationdate":"Nov 22, 2021","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIf you work on any kind of software library,\nensuring backwards-compatibility is a key concern:\nif there\u0026#8217;s one thing which users really dislike, it is breaking changes in a new version of a library.\nThe rules of what can (and cannot) be changed in a Java API without breaking existing consumers are well defined in the Java language specification (JLS),\nbut things can get pretty interesting in certain corner cases.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe Eclipse team provides a \u003ca href=\"https://wiki.eclipse.org/Evolving_Java-based_APIs_2\"\u003ecomprehensive overview\u003c/a\u003e about API evolution guidelines in their wiki.\nWhen I shared the link to this great resource on Twitter the other day,\nI received an \u003ca href=\"https://twitter.com/lukaseder/status/1462358911072317440\"\u003einteresting reply\u003c/a\u003e from Lukas Eder:\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"quoteblock\"\u003e\n\u003cblockquote\u003e\nI wish Java had a few tools to prevent some cases of binary compatibility breakages. E.g. when refining a method return type, I\u0026#8217;d like to keep the old method around in byte code (but not in source code).\n\u003cbr\u003e\n\u003cbr\u003e\nI think kotlin has such tools?\n\u003c/blockquote\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIn the remainder of this post,\nI\u0026#8217;d like to provide some more insight into that problem mentioned by Lukas,\nand how it can be addressed using an open-source tool called \u003ca href=\"https://github.com/dmlloyd/bridger\"\u003eBridger\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Refining The Return Type Of Java Methods Without Breaking Backwards-Compatibility","uri":"https://www.morling.dev/blog/refining-return-type-java-methods-without-breaking-backwards-compatibility/"},{"content":"If you have followed this blog for a while, you\u0026#8217;ll know that I am a big fan of JDK Flight Recorder (JFR), the low-overhead diagnostics and profiling framework built into the HotSpot Java virtual machine. And indeed, until recently, this meant only HotSpot: Folks compiling their Java applications into GraalVM native binaries could not benefit from all the JFR goodness so far.\n But luckily, this situation has changed with GraalVM 21.2: Thanks to a collaboration of engineers from Red Hat and Oracle, GraalVM native binaries now also support JDK Flight Recorder. At this point, the JFR recording engine itself has been put in place, there are not many event types actually emitted yet. As Jie Kang wrote recently in a post about this ongoing work, this should change soon, though:\n  The initial merge for JFR infrastructure is complete but there is a long road ahead before the system can provide a view into native executables produced by GraalVM that is similar to what is possible for HotSpot. Up next is the work to add events for garbage collection, threads, exceptions, and other useful locations in SubstrateVM.  \u0026#8212; JDK Flight Recorder support for GraalVM Native Image: The journey so far   What already does work is emitting custom JFR events from your application code. So I took the Quarkus-based todo management application from my earlier post about monitoring REST APIs with JFR and explored what it\u0026#8217;d take to make it work as a native binary. And what should I say, essentially things \"just worked ™️\". All I had to do, was the following:\n   Use a current version of GraalVM (21.3 at the time of writing)\n  Upgrade Quarkus to the current version (2.4.2.Final, released just today); with 2.2.3.Final, which I had been using before, I\u0026#8217;d get an error at image build time about a modifier mismatch with a native method substituted by Quarkus\n  Enable GraalVM\u0026#8217;s AllowVMInspection option when creating the native binary\n   As per the GraalVM documentation, the latter is required in order to use JFR events in native binaries. Unfortunately, failing to do so will only be reported at application runtime with an exception like this:\n 1 2 3 4 5 6 7 8 9 10 11 2021-11-12 15:31:22,456 ERROR [io.qua.run.Application] (main) Failed to start application (with profile prod): java.lang.UnsatisfiedLinkError: jdk.jfr.internal.JVM.getHandler(Ljava/lang/Class;)Ljava/lang/Object; [symbol: Java_jdk_jfr_internal_JVM_getHandler or Java_jdk_jfr_internal_JVM_getHandler__Ljava_lang_Class_2] at com.oracle.svm.jni.access.JNINativeLinkage.getOrFindEntryPoint(JNINativeLinkage.java:153) at com.oracle.svm.jni.JNIGeneratedMethodSupport.nativeCallAddress(JNIGeneratedMethodSupport.java:57) at jdk.jfr.internal.JVM.getHandler(JVM.java) at jdk.jfr.internal.Utils.getHandler(Utils.java:448) at jdk.jfr.internal.MetadataRepository.getHandler(MetadataRepository.java:174) at jdk.jfr.internal.MetadataRepository.register(MetadataRepository.java:135) at jdk.jfr.internal.MetadataRepository.register(MetadataRepository.java:130) at jdk.jfr.FlightRecorder.register(FlightRecorder.java:136) at dev.morling.demos.jfr.Metrics.registerEvent(Metrics.java:27) ...    This is triggered by the application code registering the custom JFR event type:\n 1 2 3 public void registerEvent(@Observes StartupEvent se) { FlightRecorder.register(JaxRsInvocationEvent.class); }    Here I\u0026#8217;d wish that either GraalVM\u0026#8217;s native-image tool or Quarkus would tell me about this situation already upon build time, in particular as the cause of that problem is not readily apparent from the exception above. In any case, the required fix is simple enough, all we need to do is to set the quarkus.native.enable-vm-inspection option in the application.properties file of the Quarkus application:\n 1 quarkus.native.enable-vm-inspection=true    With that configuration in place, the application can be built as a native binary via mvn clean verify -Pnative. Grab a coffee while the build is running (it takes about two minutes on my laptop), and then you can start the resulting native binary with the following options for creating a JFR recording:\n 1 2 3 ./target/flight-recorder-demo-1.0.0-SNAPSHOT-runner \\ -XX:+FlightRecorder \\ -XX:StartFlightRecording=\"filename=my-recording.jfr\"    You can also configure some more of the known JFR options, such as maximum recording size and duration. What is not possible at this point is starting recordings dynamically at runtime e.g. via jcmd or JDK Mission Control, as the JMX-based infrastructure required for this isn\u0026#8217;t present in native binaries (I haven\u0026#8217;t tried to do so programmatically from within the application itself, this may be supported already). JFR Event Streaming (as introduced with JEP 349 in Java 14) also doesn\u0026#8217;t work yet.\n After creating some todos in the web application, we can open the JFR recording in JDK Mission Control and examine the JFR events emitted for each invocation of the REST API:\n   As you see, besides the custom REST invocation events and some system events representing environment variables and system properties, the recording is rather empty. Also note how the thread attribute of the custom event type isn\u0026#8217;t populated.\n I\u0026#8217;ve updated the jfr-custom-events repository on GitHub, so you can get started with your own explorations around JFR events in GraalVM native binaries easily. Just make sure to have a current GraalVM and its native-image tool installed. The initial feature request for adding JFR support to GraalVM native binaries provides some more background information. You also can use JFR with the Mandrel distribution of GraalVM.\n To learn more about JFR in general, have a look at this post by Mario Torre. Finally, if you\u0026#8217;d like to find out how to use JFR for identifying potential performance regressions in your Java applications, check out this talk about JfrUnit which I did at the P99Conf conference a few weeks ago.\n","id":16,"publicationdate":"Nov 12, 2021","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIf you have followed this blog for a while,\nyou\u0026#8217;ll know that I am a big fan of \u003ca href=\"https://openjdk.java.net/jeps/328\"\u003eJDK Flight Recorder\u003c/a\u003e (JFR),\nthe low-overhead diagnostics and profiling framework built into the HotSpot Java virtual machine.\nAnd indeed, until recently, this meant \u003cem\u003eonly\u003c/em\u003e HotSpot:\nFolks compiling their Java applications into \u003ca href=\"https://www.graalvm.org/reference-manual/native-image/\"\u003eGraalVM native binaries\u003c/a\u003e could not benefit from all the JFR goodness so far.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"JDK Flight Recorder Events in GraalVM Native Binaries","uri":"https://www.morling.dev/blog/jfr-events-in-graalvm-native-binaries/"},{"content":"If you love to attend conferences around the world without actually leaving the comfort of your house, 2021 certainly was (and is!) a perfect year for you. Tons of online conferences, many of them available for free, are hosting talks on all kinds of topics, and virtual conference platforms are getting better, too.\n As the year is slowly reaching its end, I thought it might be nice to do a quick recap and gather in one place all the talks on Debezium and change data capture (CDC) which I did in 2021. An overarching theme for these talks was to discuss different CDC usage patterns and putting Debezium into the context of solving common data engineering tasks by combining it with other open-source projects such as Infinispan and Apache Pinot. In order to not feel too lonely in front of the screen and make things a bit more exciting, I decided to team up with some amazing friends from the open-source community for the different talks. A big thank you for these phantastic collaborations to Katia Aresti, Kenny Bastani, and Hans-Peter Grahsl!\n So without further ado, here are four Debezium talks I had the pleasure to co-present in 2021.\n Don\u0026#8217;t Fear Outdated Caches – Change Data Capture to the Rescue!    As per an old saying in software engineering, there\u0026#8217;s only two hard things: cache invalidation and naming things. Well, turns out the first is solved actually ;)\n In this talk at the Bordeaux Java User Group, Katia Aresti from the Infinispan team and I explored how users of an application can benefit from low response times by means of read data models, persisted in distributed caches close to the user. When working with a central database as the authoritative data source\u0026#8201;\u0026#8212;\u0026#8201;thus receiving all the write requests\u0026#8201;\u0026#8212;\u0026#8201;these local caches need to be kept up to date, of course. This is where Debezium comes in: any data changes are captured and propagated to the caches via Apache Kafka.\n And as if the combination of Kafka, Infinispan and Debezium was not already exciting enough, we also threw some Quarkus and Kafka Streams into the mix, joining the data from multiple Debezium change data topics, allowing to retrieve entire aggregate structures via a single key look-up from the local caches. It\u0026#8217;s still on our agenda to describe that archicture in a blog post, so stay tuned for that.\n   📺 Recording on YouTube\n  🖥️ Slides\n  🤖 Demo source code\n     Change Data Streaming Patterns in Distributed Systems    While some folks already might feel something like microservices fatigue, the fact is undeniable that organizing business functionality into multiple, loosely coupled services has been one of the biggest trends in software engineering over the last years.\n Of course these services don\u0026#8217;t exist in isolation, but they need to exchange data and cooperate; Apache Kafka has become the de-facto standard as the backbone for connecting services, facilitating asynchronous event-driven communication between them. In this joint presentation, my dear friend Hans-Peter Grahsl and I set out to explore what role change data capture can play in such architectures, and which patterns there are for applying CDC to solve common problems related to handling data in microservices architectures. We focused on three patterns in particular, each implemented using log-based CDC via Debezium:\n   The outbox pattern for reliable, eventually consistent data exchange between microservices, without incurring unsafe dual writes or tight coupling\n  The strangler fig pattern for gradually extracting microservices from existing monolithic applications\n  The saga pattern for coordinating long-running business transactions across multiple services, ensuring such activity gets consistently applied or aborted by all participating services\n   We presented that talk at several conferences, including Kafka Summit Europe, Berlin BuzzWords, and jLove. We also did a variation of the presentation at Flink Forward, discussing how to implement the different CDC patterns using Debezium and Apache Flink. The recording of this session should be published soon, in the meantime you can find the slides here. I also highly recommend to take a look at this blog post by Bilgin Ibryam, in which he discusses these patterns in depth.\n   📺 Recording on YouTube\n  🖥️ Slides\n  🤖 Demo source code\n     Analyzing Real-time Order Deliveries using CDC with Debezium and Pinot    Traditionally, there has been a chasm between operational databases backing enterprise applications (i.e. OLTP systems), and systems meant for ad-hoc analytics use cases, such as queries run by a business analyst in the back office. (OLAP systems). Data would typically be propagated in batches from the former to the latter, resulting in multi-hour delays until the analytics system would be able to run queries on changed production data.\n With the current shift to user-facing analytics, we are observing nothing less than a revolution: the ability to serve low-latency analytical queries on large data sets to the end users of an application, based on data that is really fresh (seconds old, rather than hours). Compared to response times and freshness guarantees you\u0026#8217;d typically get from earlier generations of data warehouses, this is a game changer.\n In this model, Debezium is used to capture all data changes from the operational database and propagate them into the analytics system. Kenny Bastani of StartTree and I spoke about the opportunities and use cases enabled by combining Debezium with Apache Pinot, a realtime distributed OLAP datastore, at the Pinot meet-up. A massive shout-out to Kenny again for putting together an awesome demo, showing how to use Debezium and the outbox pattern for getting the data into Apache Kafka, transform the data and ingest it into Pinot, and do some really cool visualizations via Apache Superset.\n   📺 Recording on YouTube\n  🤖 Demo source code\n     Dissecting our Legacy: The Strangler Fig Pattern with Apache Kafka, Debezium and MongoDB    After talking about three different CDC patterns, Hans-Peter and I decided to explore one of the patterns in some more depth and did this talk focusing exclusively on the strangler fig pattern. Existing monolithic applications are a reality in many enterprises, and oftentimes it\u0026#8217;s just not feasible to replace them with a microservices architecture all at once in one single migration step.\n This is where the strangler fig pattern comes in: it helps you to gradually extract components from a monolith into separate services, relying on CDC for keeping the data stores of the different systems in sync. A routing component, such as Nginx or Envoy Proxy, in front of all the systems sends each incoming request to that system which is in charge of a specific part of the domain at a given point in time during the migration.\n This talk (which we presented at MongoDB.Live, Kafka Summit Americas, and VoxxedDays Romania), also contains a demo, we show how to implement the strangler fig pattern using Debezium, gradually moving data from a legacy system\u0026#8217;s MySQL database over to the MongoDB instance of a new microservice, which is built using Quarkus.\n   📺 Recording on YouTube\n  🖥️ Slides\n  🤖 Demo source code\n     Bonus: Debezium at the Trino Community Broadcast    This one is not so much a regular conference talk, but more of an informal exchange, so I\u0026#8217;m adding it as a bonus here, hoping you may find it interesting too. Brian Olsen and Manfred Moser of Starburst, the company behind Trino, invited Ashhar Hasan, Ayush Chauhan, and me onto their Trino Community Broadcast.\n We had a great time talking about Debezium and CDC in the context of Trino and its federated query capabilities, learning a lot from Ashhar and Ayush about their real-world experiences from using these technologies in production.\n   Learning More Thanks again to Katia, Kenny, and Hans-Peter for joining the virtual conference stages with me this year! It would not have been half as much fun without you.\n If these talks have piqued your interest in open-source change data capture and Debezium, head over to the Debezium website to learn more. You can also find many more examples in the Debezium examples repo on GitHub, and if you look for reports by folks from the community about their experiences using Debezium, take a look at this currated list of blog posts and other resources.\n   ","id":17,"publicationdate":"Nov 2, 2021","section":"blog","summary":"If you love to attend conferences around the world without actually leaving the comfort of your house, 2021 certainly was (and is!) a perfect year for you. Tons of online conferences, many of them available for free, are hosting talks on all kinds of topics, and virtual conference platforms are getting better, too.\n As the year is slowly reaching its end, I thought it might be nice to do a quick recap and gather in one place all the talks on Debezium and change data capture (CDC) which I did in 2021.","tags":null,"title":"Debezium and Friends – Conference Talks 2021","uri":"https://www.morling.dev/blog/debezium-talks-2021/"},{"content":"  I\u0026#8217;ve been working from home exclusively for the last nine years, but it was only last year that I started to look into ways for expanding my computer set-up and go beyond the usual combination of having a laptop with your regular external screen. The global COVID-19 pandemic, the prospect of having more calls with colleagues than ever (no physical meetings), and the constantly increasing need for recording talks for online conferences and meet-ups made me reevaluate things and steadily improve and fine tune my set-up, in particular in regards to better video and audio quality.\n When I shared a picture of my desk on Twitter recently, a few folks asked for more details on specific parts like the screen, microphone etc, so I thought I\u0026#8217;d provide some insights in this post. Don\u0026#8217;t expect any sophisticated test or evaluation of sorts, I\u0026#8217;m just going to briefly describe the different components, how I use them, things I like about them, and other aspects which still could be improved. Note that I\u0026#8217;m not affiliated in any way with any of the vendors mentioned in this post, so anything positive or negative I\u0026#8217;m going to mention, is solely based on my personal experience from using the discussed items, without any financial incentive to do so. There are also no affiliate links.\n The Screen Let\u0026#8217;s start with the most apparent part of the set-up, the screen. It\u0026#8217;s a curved 49\" 32:9 ultra-widescreen display (Samsung C49RG94SSR, 5120 x 1440 pixels), i.e. it offers the same screen real estate like two 16:9 screens next to each other.\n Whether such a large screen suits your personal preferences is something which you only really can find out by yourself. Curvature of the screen is something you may have to get used to, initially I was slightly put off by (wide) windows not appearing 100% straight, but by now I don\u0026#8217;t even notice this any more. I suggest you have a look at this article by my colleague Emmanuel Bernard, where he compares ultra-wide monitors to the alternatives and discusses the pros and cons of each. Personally, I\u0026#8217;m very happy with this screen and really wouldn\u0026#8217;t want to miss it. I never was a fan of multi-screen set-ups due to the inevitable frames between screens, and in fact, my only regret is that I didn\u0026#8217;t buy it earlier. So thanks a lot for the recommendation, Emmanuel!\n Some folks use window managers to arrange their application windows on large screens (e.g. Rectangle on macOS, a few more alternatives are discussed in this thread by Guillaume Laforge), but I find myself just manually organizing things in roughly three columns: communications (email, chat), editing (documents, shell, IDE, etc.), and preview (e.g. rendered AsciiDoc documents).\n  Figure 1. Reading the source code of HasThisTypePatternTried\u0026#8230;\u0026#8203;Visitor at 300%? No problem!  One very useful feature of this monitor is its picture-by-picture mode (PBP): it lets you connect two sources at once, which then will show up next to each other on the screen. Now I\u0026#8217;m typically not working with two computers simultaneously (although this can be useful when for instance editing a benchmark on one machine and running it on another), but I use PBP when doing presentations, or when recording conference talks; in that case, I\u0026#8217;ll connect the same machine twice, i.e. as primary and secondary screen. This allows me to share one of the screens entirely for the presentation/recording (thus having the commonly expected 16:9 aspect ratio), with other applications being located on the second screen, and without having to manually adjust the size of individually shared windows or tabs. Needless to say that sharing the full screen isn\u0026#8217;t very practical, as viewers with a regular screen would just see a small wide ribbon.\n Are there downsides to this screen? So far, I\u0026#8217;ve found two. One is its energy consumption; with 55 kWh/1,000h, it\u0026#8217;s definitely on the high end of the spectrum. I suppose in parts that\u0026#8217;s just due to its sheer size, but I\u0026#8217;m sure things could be improved here. The other thing to mention is that when using it with a MacBook Pro, you should make sure to have the lid of the laptop closed (implying that you\u0026#8217;ll need an external keyboard and mouse/touchpad), as the fan will be audible substantially more when driving both internal and external screens.\n One last minor annoyance is that the screen\u0026#8217;s software forgets the settings when enabling and disabling the picture-by-picture mode. When switching from single input to PBP, I always need to configure the input sources again. Here I\u0026#8217;d really wish the screen would memorize the settings from the last time I was using PBP.\n   Compute I am using two Apple computers to get things done: a 2019 16\" MacBook Pro (2,6 GHz 6-Core Intel Core i7, 32 GB of RAM) provided by my employer, and a Mac Mini M1 2020 with 16 GB of RAM. Most work stuff is happening on the MackBook Pro, and really there\u0026#8217;s nothing too exciting to share here; it tends to do its job just as it should. There\u0026#8217;s two things I don\u0026#8217;t like about it though:\n   the touch bar; it\u0026#8217;s virtually useless to me, and I wished for physical function keys instead, making it much more reliable to hit the right key combinations, e.g. in the IDE. Granted, I work with an external keyboard most of the time, so it\u0026#8217;s not impacting me that much\n  the only connectivity option being USB-C; while surely elegant, the required zoo of connectors and adapters to actually plug in external hardware, renders that point more than moot\n   Thankfully, Apple finally got that memo too and addressed both things in their latest MacBook Pro edition.\n  Figure 2. Duplo bricks make for a perfect laptop stand; luckily, I could borrow some from my daughter  The Mac Mini is awesome for any kind of video recording and streaming. Recently, I was asked to record two Full HD streams for a talk at AccentoDev: one with my slides, and one with my camera feed, allowing the video editor to freely switch between the two when creating the final recording for publication. The M1 wouldn\u0026#8217;t break a sweat when recording this video with a resolution of 3,840 x 1,080 pixels via OBS, with the fan barely being audible. Whereas when trying to do the same on the MBP, the fan would spin up heavily, and you\u0026#8217;d have a hard time to not capture the fan noise with the microphone.\n  Figure 3. MacMini M1 2020  Originally, I bought the Mac Mini M1 to experiment a bit with running Java applications on the AArch64 architecture. Unfortunately, I didn\u0026#8217;t really find much time yet to do so. One interesting thing I noticed though from running some quick JMH benchmarks against the new Java Vector API is that results tended to be super-stable, with a much smaller standard deviation than running the same benchmark on the x86-based laptop. I hope to find some time to dive a bit more into that area at some point in the future.\n     Cloud Compute Every now and then, I do have the need for running something on Linux rather than macOS, or for spinning up multiple boxes, executing a benchmark for instance. Ok, ok, they are not actually running on my desk, but I thought it still might be interesting to share a few words on that.\n My preferred go-to platform for these scenarios is Hetzner Cloud, as they provide flexible cloud compute options at a really attractive price tag, in particular capped at a fixed limit, so there\u0026#8217;s no potential for surprise bills coming in.\n To make launching and configuring boxes in the Hetzner cloud as easy as possible for me, I have a simple set-up of Terraform and Ansible scripts. The former just launches up the desired number of compute nodes with the chosen spec, using the current version of Fedora as the operating system. The latter installs the tools I commonly need, such as different Java versions, commonly used CLI tools, and such.\n One neat thing about Hetzner Cloud is that you can easily scale up and down single instances. So what I\u0026#8217;ll usually do is to spin up a box in the smallest available configuration (CX11); running this for a full month costs a whopping €4.15. But then, when I actually want to use the node, I\u0026#8217;ll change the Terraform configuration to something more powerful, such as the CCX22 instance type with 4 dedicated vCPU and 16 GB RAM. One quick terraform apply and a few seconds later, I\u0026#8217;ll have a node with the specs I need. Only for the few hours I\u0026#8217;m using it, I\u0026#8217;ll have to pay the increased price for the better spec, before scaling it back down to the CX11 instance again.\n       Cameras So let\u0026#8217;s change topics and talk a bit about my recording set-up. There\u0026#8217;s essentially three scenarios where I need to record myself and/or my screen:\n   Video calls: working 100% from home in a globally distributed development team, there\u0026#8217;s not a single day where I won\u0026#8217;t have to do at least a couple of calls with my co-workers\n  Conference talks: with the global pandemic still going on, all the conferences have gone virtual, requiring either to pre-record or live-stream any talks\n  Demos: lately, I\u0026#8217;ve become a fan of recording short videos introducing new features in the projects I\u0026#8217;m involved with, e.g. the Debezium UI or kcctl\n   Additionally, I\u0026#8217;m joining Nicolai Parlog once per month on his Twitch channel, where we talk about and explore all things Java.\n While I initially used the internal camera and microphone of my laptop, I wasn\u0026#8217;t really satisfied with the outcome, in particular once I saw the high quality of recordings shared by other folks. For a really good video image quality, two things are key: using a \"real\" camera (i.e. not a webcam), and proper lighting. You\u0026#8217;ll also want a good external microphone, more on that below.\n So why not a webcam? Essentially because sensors are too small and lenses are too slow, which means you\u0026#8217;ll quickly have noise in the image and you won\u0026#8217;t get that nice movie-like look with a shallow depth of field (bokeh). Using either a DSLR or a mirrorless system camera will yield a dramatically better image quality. In my case, I am using the Lumix GX80 (sold as GX85 in the US), a mirrorless system camera from Panasonic, using the Micro Four Thirds interchangeable lens standard.\n  Figure 4. Panasonic Lumix GX80 and Logitech StreamCam  I\u0026#8217;m generally happy with it for this purpose: it provides clean HDMI output (i.e. no menu overlays when capturing the live feed via HDMI, as it\u0026#8217;s the case with some cameras), image quality and ergonomics are good overall. On the downside, it doesn\u0026#8217;t provide continuous auto-focus if you\u0026#8217;re not actually recording on the camera. This sounds worse than it actually is in practice: using the \"Quick AF\" option, it will auto-focus when turning on the camera, or when zooming in or out a bit, which is enough to get proper focussing in a relatively static setting such as a screen recording session. If you are planning to move forth and back a lot though, then you should look into other options. Another thing to mention is that the GX80 doesn\u0026#8217;t allow to connect an external microphone to it; in my case, that doesn\u0026#8217;t matter though, as I\u0026#8217;m connecting the mic via a separate audio interface.\n As you\u0026#8217;d quickly run down the camera\u0026#8217;s battery when streaming its video signal for a longer period of time, an external power source should be used. I\u0026#8217;m using a dummy battery similar to this one, which does the job as expected. Just make sure to have an USB power adapter which provides enough output current (2A or more); I had missed that initially and was wondering why the camera would always turn off when pressing the focus button\u0026#8230;\u0026#8203; . For a camera mount, I\u0026#8217;m using this cheap one; it\u0026#8217;s pretty crappy, with lots of wobbling, but once you have the camera in the place where you want it to be, it\u0026#8217;ll stay there. Still, I\u0026#8217;d probably pay a bit more to get a more robust mount, should I ever have to buy a new one.\n As you typically cannot connect a DSLR or a mirrorless system camera like the GX80 via USB, you\u0026#8217;ll also need an HDMI converter which you then can plug into your USB port. Here I\u0026#8217;m using the ubiquitous Elgato Cam Link 4K. Back when I got it, it was pretty much the only (and pricy) option, but I believe by now there are alternatives, which should work equally well but are a bit cheaper.\n Despite my \"no webcam\" mantra, I also have a Logitech StreamCam in addition to the GX80. As you\u0026#8217;d expect, image quality is not really comparable, in particular white balance tends to be quite off for a while after switching it on. I still use it occasionally for video calls, as it\u0026#8217;s a bit quicker to turn on and set up in comparison to the GX80.\n   Work in Progress: Teleprompter One of my pet peeves with modern communication is the lack of \"eye contact\" during virtual conference talks and video calls. As we all want to look onto the screen rather than the camera, the viewer on the other side feels like you are not looking at them, but slightly below or to the side. While I believe I largely manage to look into the camera when doing talk recordings, I find it nearly impossible to do so during calls, as the natural desire to look at the other person\u0026#8217;s image on my screen is just too strong.\n That\u0026#8217;s why I\u0026#8217;ve started to explore how I could build my own teleprompter, which puts the camera behind a two-way mirror. That way, I can look at the screen, while also looking straight into the camera. For this purpose, I bought two-way mirror glass on eBay (Schott beamsplitter glass, which is working amazingly well) as well as a cheap-ish external screen, and built a quick proof-of-concept (again using some of my daughter\u0026#8217;s Duplo bricks, this time for the frame).\n    The result was pretty promising, with one open challenge being that the display contents are mirrored from left to right. So I\u0026#8217;d need to digitally mirror the output of that display; if you are aware of any option to do so on macOS, any pointers would be appreciated. With 11.6\", the screen also is rather small, if you consider building something like this by yourself, I\u0026#8217;d recommend going for a larger one.\n Since then, I\u0026#8217;ve dropped that ball a bit and haven\u0026#8217;t followed through yet to make it \"production-worthy\". I\u0026#8217;d still love to make this useful in practice eventually, perhaps once my daughter lets me keep those Duplo bricks ;)\n   Lighting The best camera won\u0026#8217;t help you much if there isn\u0026#8217;t enough light to work with. Generally, the more light you have, the easier the job will be for the camera. I have a ring light similar to this one, with adjustable brightness and color temperature. I don\u0026#8217;t have much to say about it, other than that it does what I want it to do. Note that the tripod requires some space on the floor, which means you cannot move your desk all the way to the wall if you have the light behind it. It\u0026#8217;s not that much of a problem in my case, but you may consider getting a desk-mount alternatively.\n One problem I do have with the ring light is reflections on my glasses. I haven\u0026#8217;t really found a good solution here (no, I won\u0026#8217;t get contact lenses), other than pushing the ring light a bit higher than ideal, so that there are no reflections when looking into the camera further below. On the downside, this results in the area below my chin becoming a bit shaded. A case of having to choose your poison, I suppose.\n  Figure 5. Background Lights  When doing conference talks, I have two more lights in the backgrounds which make for a nicer atmosphere of the scenery. A vintage light (no-name brand, got it from my local hardware store) which adds a nice highlight, and a Philips Hue Iris lamp which adds a colored note of my choosing. Overall, I\u0026#8217;m like 90% happy with the lighting set-up, the comment by video grandmaster Nicolai about lacking separation of background and foreground still nags me ;)\n   Audio Finally, let\u0026#8217;s talk about my audio recording set-up. This definitely is the area I knew the least about when setting out to improve my computing and recording gear. I don\u0026#8217;t quite remember when and how I got sucked into the audio game, perhaps it was when I learned about scientific research indicating that audio quality impacts the perceived quality of spoken content.\n After a rather disappointing experience with the RØDE NT-USB (perhaps it\u0026#8217;s my lack of audiophile sensitivity, but I didn\u0026#8217;t sense a significant difference compared to using the built-in laptop mic), I decided to look for an external microphone which doesn\u0026#8217;t connect via USB. After some research, I decided to go for the RØDE Procaster, which is a rather professional microphone purpose-built for voice recording. It is a dynamic microphone, which in comparison to a condenser microphone will pick up much less noise from your surroundings (you can learn more about the differences between these two kinds of microphones here). This means that I don\u0026#8217;t have to ask my family to be extra-silent in the house while I am doing a recording.\n  Figure 6. RØDE Procaster Microphone  One thing to keep in mind is that this type of microphone is meant to be put rather close to your mouth, which you may or may not find annoying. Personally, I sort of like how this makes speaking a more conscious act, but I\u0026#8217;d probably not like to have the microphone in front of me when doing a multi-hour call. That\u0026#8217;s why I also have a cheap-ish headset as an alternative for these situations. Yet another\u0026#8201;\u0026#8212;\u0026#8201;and more costly\u0026#8201;\u0026#8212;\u0026#8201;option would be to get a shotgun microphone which you can position further away from you.\n The microphone is rather heavy (and you wouldn\u0026#8217;t want to hold it anyways), so I am using the PSA1 studio boom arm. It lets you move the microphone with a single finger to where you want it to be, and then it will stay exactly there. A really solid piece of engineering, in particular when comparing it to the no-name mount I\u0026#8217;m using for the camera.\n Having an external microphone is just one part of the story, though. You also need to have an audio interface which lets you plug in the microphone (using an XLR cable) and then propagates the audio signal to your computer via USB. I didn\u0026#8217;t do much exploration here, but went for the PreSonus AudioBox USB 96, which was recommended to me by a coworker. In general, it does the job well, there\u0026#8217;s two things I don\u0026#8217;t like about it though.\n  Figure 7. PreSonus AudioBox USB 96 Audio Interface  First, it doesn\u0026#8217;t have a physical power switch, which means its two (rather bright) red LEDs will be lighting up as long as it\u0026#8217;s connected to the USB port. Secondly, I really wished it would have a built-in option to emit the microphone signal on both audio channels, left and right. As a microphone is a mono audio source, you\u0026#8217;ll hear the signal only on one channel (typically the left one) on your computer. When doing recordings, where you have the time and ability to do some post-processing, that\u0026#8217;s not a big problem; you can simply duplicate the audio track to both channels. But when using the microphone in a Zoom call or similar, the one-sided output is not what you want. In absence of hardware support for this kind of upmixing in the AudioBox, I had to go for a software solution, which took me quite some time to figure out.\n On macOS, this requires two programs, LadioCast and Blackhole. The former lets you take the single channel input from the AudioBox and expose it on both channels, left and right. This is then connected to a virtual audio device created using the BlackHole audio driver. In Zoom or similar software, you then use that virtual device for the audio input. This works reliably and without any noticeable latency. Still I wished the AudioBox would just take care of all of this and provide me with the microphone input upmixed to both channels.\n  Figure 8. Setting up a virtual audio device using BlackHole and connecting the mono microphone input to it using both channels via LadioCast; note how channel 1 is used for both L and R in the input configuration in LadioCast  Coming back to the microphone, one thing to be aware of is that it provides a rather low output signal. While you can boost it up far enough with the AudioBox, you\u0026#8217;ll start to hear some noise. And I haven\u0026#8217;t spent hundreds of Euros and multiple hours to get noise, have I?! So I did what every reasonable person would do in that situation: spend some more money.\n  Figure 9. CloudLifter CL-1 Mic Activator  The solution was to add a pre-amplifier. Here I went for the CloudLifter, which you put between the microphone and the audio interface. It takes 48V phantom power (which the AudioBox provides) and adds +25dB of gain, giving me audio with proper volume, without any audible hiss whatsoever. Take that, sunken cost fallacy!\n If you would like to hear (and see) a recording with this set-up, have a look at this session about the JfrUnit project from P99Conf earlier this year.\n   What\u0026#8217;s Next? Overall, I\u0026#8217;m very happy with my computing and recording set-up. One thing that still could be improved is lighting. It\u0026#8217;s a common practice to work with two front lights (or one from the front and one from the side), so I\u0026#8217;ll probably buy another light at some point. I also hope to finish the teleprompter project and put it into daily use.\n Other than that, I am sometimes wondering whether I should get a second mirrorless camera and a video switcher like the Atem Mini and explore a multi-camera set-up. I\u0026#8217;m certain that this would be lots of fun, on the other hand I don\u0026#8217;t really have the need for it\u0026#8230;\u0026#8203; yet?\n Many thanks to Hans-Peter Grahsl for his feedback while writing this blog post!\n  ","id":18,"publicationdate":"Oct 24, 2021","section":"blog","summary":"\u003cdiv class=\"imageblock\"\u003e\n\u003cdiv class=\"content\"\u003e\n\u003cimg src=\"/images/desk_complete.jpg\" alt=\"desk complete\"\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eI\u0026#8217;ve been working from home exclusively for the last nine years,\nbut it was only last year that I started to look into ways for expanding my computer set-up and go beyond the usual combination of having a laptop with your regular external screen.\nThe global COVID-19 pandemic, the prospect of having more calls with colleagues than ever (no physical meetings), and the constantly increasing need for recording talks for online conferences and meet-ups made me reevaluate things and steadily improve and fine tune my set-up, in particular in regards to better video and audio quality.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"What's on My Desk?","uri":"https://www.morling.dev/blog/whats-on-my-desk/"},{"content":"It has been just a few weeks since the release of Java 17, but the first changes scheduled for Java 18 begin to show up in early access builds. One feature in particular that excites me as a maintainer of different Java libraries is JEP 413 (\"Code Snippets in Java API Documentation\").\n So far, JavaDoc has not made it exactly comfortable to include example code which shows how to use an API: you had to escape special characters like \"\u0026lt;\", \"\u0026gt;\", and \"\u0026amp;\", indentation handling was cumbersome. But the biggest problem was that any such code snippet would have to be specified within the actual JavaDoc comment itself, i.e. you did not have proper editor support when creating it, and worse, it was not validated that the shown code actually is correct. This often led to code snippets which wouldn\u0026#8217;t compile if you were to copy them into a Java source file, be it due to an oversight by the author, or simply because APIs changed over time and no one was thinking of updating the corresponding snippets in JavaDoc comments.\n All this is going to change with JEP 413: it does not only improve ergonomics of inline snippets, but it also allows you to include code snippets from external source files. This means that you\u0026#8217;ll be able to edit and refactor any example code using your regular Java toolchain; better yet: you can also compile and test it as part of your build. Welcome to 2021\u0026#8201;\u0026#8212;\u0026#8201;no more wrong or outdated code snippets in JavaDoc!\n Including Snippets From Your Test Directory You could think of different ways for organizing your snippet files with JEP 413, but one particularly intriguing option is to source them straight from the tests of your project, e.g. the src/test/java directory in case of a Maven project. That way, any incorrect snippet code\u0026#8201;\u0026#8212;\u0026#8201;be it due to compilation failures or due to failing test assertions\u0026#8201;\u0026#8212;\u0026#8201;will be directly flagged within your build.\n So let\u0026#8217;s see how to set this up, using the Jakarta Bean Validation API project as an example. The required configuration is refreshingly simple; all we need to do is to specify src/test/java as our \"snippet path\". While the Maven JavaDoc plug-in does not yet provide a bespoke configuration option for this, we can simply pass it using the \u0026lt;additionalOptions\u0026gt; property (make sure to use version 3.0.0 or later):\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-javadoc-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.1\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;attach-javadocs\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;jar\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;additionalOptions\u0026gt; \u0026lt;additionalOption\u0026gt; (1) --snippet-path=${basedir}/src/test/java \u0026lt;/additionalOption\u0026gt; \u0026lt;/additionalOptions\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt;      1 Obtain snippets from src/test/java    And that\u0026#8217;s all there is to it really, you now can start to work with example code as actual source code. Here\u0026#8217;s an example for a snippet to be included into the API documentation of jakarta.validation.Validation, the entry point into the Bean Validation API:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package snippets; (1) import jakarta.validation.Validation; import jakarta.validation.ValidatorFactory; public class CustomProviderSnippet { public void customProvider() { // @start region=\"provider\" (2) ACMEConfiguration configuration = Validation .byProvider(ACMEProvider.class) .providerResolver( new MyResolverStrategy() ) .configure(); ValidatorFactory factory = configuration.buildValidatorFactory(); // @end (2) } }      1 There\u0026#8217;s no specific requirements on the package to be used; I like using a descriptive name snippets, so to easily tell apart snippets from functional tests   2 If you don\u0026#8217;t want to include the entire file, regions allow to specify the exact section(s) to include    While a plain method is shown here, this could of course also be an JUnit test with assertions for making sure that the snippet code does what it is supposed to do (being an API specification, the Bean Validation project itself doesn\u0026#8217;t provide an implementation we could test against). Including the snippet into the JavaDoc in the source file is straight-forward:\n 1 2 3 4 5 6 7 8 9 10 11 /** * ... * \u0026lt;li\u0026gt; * The third approach allows you to specify explicitly and in * a type safe fashion the expected provider. * \u0026lt;p\u0026gt; * Optionally you can choose a custom {@code ValidationProviderResolver}. * {@snippet class=\"snippets.CustomProviderSnippet\" region=\"provider\"} (1) * \u0026lt;/li\u0026gt; * ... */      1 Specify the snippet either using the class or the file attribute; optionally define a specific snippet region to be included    If needed, you also can customize appearance of the rendered snippet, so to add links, highlight key parts (using custom CSS styles if needed), or replace specific parts of the snippet. The latter comes in handy for instance to replace non-critical parts with a placeholder such as \"\u0026#8230;\u0026#8203;\". This is one of the details I really like about this JEP: Even if you did manage example code in separate source files in the past, then manually copying them into JavaDoc, such placeholders made things cumbersome. Naturally, they\u0026#8217;d fail compilation, e.e. you always had to do some manual editing when copying over the snippet into JavaDoc. Getting all this \"for free\" is a very nice improvement.\n Here\u0026#8217;s an example showing these adjustments in source form (scroll to the right to see all the snippet tag attributes, as these lines can become fairly long):\n 1 2 3 4 5 6 7 8 9 public void customProvider() { // @start region=\"provider\" ACMEConfiguration configuration = Validation .byProvider(ACMEProvider.class) // @highlight substring=\"byProvider\" (1) .providerResolver( new MyResolverStrategy() ) // @replace regex=\" new MyResolverStrategy\\(\\) \" replacement=\"...\" (2) .configure(); ValidatorFactory factory = configuration.buildValidatorFactory(); // @link regex=\"^.*?ValidatorFactory\" target=\"jakarta.validation.ValidatorFactory\" (3) // @end }      1 Highlight the byProvider() method   2 Replace the parameter value of the method call with \"\u0026#8230;\u0026#8203;\"   3 Make the ValidatorFactory class name a link to its own JavaDoc    And this is how the snippet will looks like in the rendered documention:\n   Some folks may argue that it might be nice to have proper colored syntax highlighting support. I\u0026#8217;m not sure whether I agree though: your typical code snippets in API docs should be rather short, and simply highlighting key parts like shown above may be more useful than colorizing the entire thing. Note the extra new line at the beginning of the snippet shouldn\u0026#8217;t really be there, it\u0026#8217;s not quite clear to me where it\u0026#8217;s coming from. I\u0026#8217;ll try and get this clarified on the javadoc-dev mailing list.\n   Summary Being able to include code snippets from actual source files into API documentation is a highly welcomed improvement for Java API docs authors and users alike. It\u0026#8217;s great to see Java catching up here with other language eco-systems like Rust, which already support executable documentation examples. I\u0026#8217;m expecting this feature to be used very quickly, with first folks already announcing to build their API docs with Java 18 as soon as it\u0026#8217;s out. Of course you can still ensure compatibility of your code with earlier Java versions also when doing so.\n If you\u0026#8217;d like get your hands on executable JavaDoc code snippets yourself, you can start with this commit showing the required changes for the Bean Validation API. Run mvn clean verify, and you\u0026#8217;ll find the rendered JavaDoc under target/apidocs. Just make sure to build this project using a current Java 18 early access build. Happy snippeting!\n  ","id":19,"publicationdate":"Oct 18, 2021","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIt has been just a few weeks since the \u003ca href=\"https://www.infoq.com/news/2021/09/java17-released/\"\u003erelease of Java 17\u003c/a\u003e, but the first changes scheduled for Java 18 begin to show up in early access builds.\nOne feature in particular that excites me as a maintainer of different Java libraries is \u003ca href=\"https://openjdk.java.net/jeps/413\"\u003eJEP 413\u003c/a\u003e (\"Code Snippets in Java API Documentation\").\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Executable JavaDoc Code Snippets","uri":"https://www.morling.dev/blog/executable-javadoc-code-snippets/"},{"content":"I had the pleasure to do a few podcasts and interviews, mostly around Debezium and change data capture.\n   Trino Community Broadcast: Episode #25: Trino Going Through Changes; together with Ashhar Hasan and Ayush Chauhan\n  The InfoQ Podcast, with Wes Reisz: Gunnar Morling on Change Data Capture and Debezium\n  Data Engineering Podcast by Tobias Macey: Episode 114\u0026#8201;\u0026#8212;\u0026#8201;Change Data Capture For All Of Your Databases With Debezium; together with Randall Hauch\n  Adam Bien\u0026#8217;s airhacks.fm podcast: Episode 39\u0026#8201;\u0026#8212;\u0026#8201;Use the Most Productive Stack You Can Get\n  Adam Bien\u0026#8217;s airhacks.fm podcast: Episode 57\u0026#8201;\u0026#8212;\u0026#8201;CDC, Debezium, streaming and Apache Kafka\n  Streaming Audio: a Confluent podcast about Apache Kafka: Change Data Capture with Debezium ft. Gunnar Morling\n  Interview with Thorben Janssen for heise.de (German): Im Gespräch: Gunnar Morling über Debezium und CDC\n  Thoughts On Java: Interview with Gunnar Morling\n   ","id":20,"publicationdate":"Sep 22, 2021","section":"","summary":"I had the pleasure to do a few podcasts and interviews, mostly around Debezium and change data capture.\n   Trino Community Broadcast: Episode #25: Trino Going Through Changes; together with Ashhar Hasan and Ayush Chauhan\n  The InfoQ Podcast, with Wes Reisz: Gunnar Morling on Change Data Capture and Debezium\n  Data Engineering Podcast by Tobias Macey: Episode 114\u0026#8201;\u0026#8212;\u0026#8201;Change Data Capture For All Of Your Databases With Debezium; together with Randall Hauch","tags":null,"title":"Podcasts and Interviews","uri":"https://www.morling.dev/podcasts/"},{"content":"The ResourceBundle class is Java\u0026#8217;s workhorse for managing and retrieving locale specific resources, such as error messages of internationalized applications. With the advent of the module system in Java 9, specifics around discovering and loading resource bundles have changed quite a bit, in particular when it comes to retrieving resource bundles across the boundaries of named modules.\n In this blog post I\u0026#8217;d like to discuss how resource bundles can be used in a multi-module application (i.e. a \"modular monolith\") for internationalizing error messages. The following requirements should be satisified:\n   The individual modules of the application should contribute bundles with their specific error messages, avoiding the need for developers from the team having to work on one large shared resource bundle\n  One central component (like an error handler) should use these bundles for displaying or logging the error messages in a uniform way\n  There should be no knowledge about the specific modules needed in the central component, i.e. it should be possible to add further modules to the application, each with their own set of resource bundles, without having to modify the central component\n   The rationale of this design is to enable individual development teams to work independently on their respective components, including the error message resource bundles, while ensuring consistent preparation of messages via the central error handler.\n As an example, we\u0026#8217;re going to use Links, a hypothetical management software for golf courses. It is comprised of the following modules (click on image to enlarge):\n   The core module contains common \"framework\" code, such as the error handler class. The modules greenkeeping, tournament, and membership represent different parts of the business domain of the Links application. Normally, this is where we\u0026#8217;d put our business logic, but in the case at hand they\u0026#8217;ll just contain the different resource bundles. Lastly, the app module provides the entry point of the application in form of a simple main class.\n The ResourceBundleProvider Interface If you have worked with resource bundles before, you may have come across approaches for merging multiple bundles into one. While technically still doable when running with named Java modules, it is not adviseable; in order to be found across module boundaries, your bundles would have to reside in open packages. Also, as no package must be contained in more than one module, you\u0026#8217;d have to implement some potentially complex logic for identifying bundles contributed by different modules, whose exact names you don\u0026#8217;t know (see the third requirement above). You may consider to use automatic modules, but then you\u0026#8217;d void some advantages of the Java module system, such as the ability to create modular runtime images.\n The solution to these issues comes in the form of the ResourceBundleProvider API, introduced alongside the module system in Java 9. Based on the Java service loader mechanism, it enables one module to retrieve bundles from other modules in a loosely coupled way; the consuming module neither needs to know about the providing modules themselves, nor about implementation details such as their internally used bundle names and locations.\n So let\u0026#8217;s see how we can use ResourceBundleProvider in the Links application. The first step is to define a bundle-specific service provider interface, derived from ResourceBundleProvider:\n 1 2 3 4 5 6 package dev.morling.links.core.spi; import java.util.spi.ResourceBundleProvider; public interface LinksMessagesProvider extends ResourceBundleProvider { }    The name of bundle provider interfaces must follow the pattern \u0026lt;package of baseName\u0026gt; + \".spi.\" + \u0026lt;simple name of baseName\u0026gt; + \"Provider\". As the base name is dev.morling.links.core.LinksMessages in our case, the provider interface name must be dev.morling.links.core.spi.LinksMessagesProvider. This can be sort of a stumbling stone, as an innocent typo in the package or type name will cause your bundle not to be found, without good means of analyzing the situation, other than double and triple checking that all names are correct.\n Next, we need to declare the usage of this provider interface in the consuming module. Assuming the afore-mentioned error handler class is located in the core module, the module descriptor of the same looks like so:\n 1 2 3 4 5 module dev.morling.links.core { exports dev.morling.links.core; exports dev.morling.links.core.spi; (1) uses dev.morling.links.core.spi.LinksMessagesProvider; (2) }      1 Export the package of the resource bundle provider interface so that implementations can be created in other modules   2 Declare the usage of the LinksMessagesProvider service    Using the resource bundle in the error handler class is rather unexciting; note that not our own application code retrieves the resource bundle provider via the service loader, but instead this is happening in the ResourceBundle::getBundle() factory method:\n 1 2 3 4 5 6 7 8 9 public class ErrorHandler { public String getErrorMessage(String key, UserContext context) { ResourceBundle bundle = ResourceBundle.getBundle( \"dev.morling.links.base.LinksMessages\", context.getLocale()); return \"[User: \" + context.getName() + \"] \" + bundle.getString(key); } }    Here, the error handler simply obtains the message for a given key from the bundle, using the locale of some user context object, and returning a message prefixed with the user\u0026#8217;s name. This implementation just serves for example purposes of course; in an actual application, message keys might for instance be obtained from application specific exception types, raised in the different modules, and logged in a unified way via the error handler.\n   Resource Bundle Providers With the code in the core module in place (mostly, that is, as we\u0026#8217;ll see in a bit), let\u0026#8217;s shift our attention towards the resource bundle providers in the different application modules. Not too suprising, they need to define an implementation of the LinksMessagesProvider contract.\n There is one challenge though: how can the different modules contribute implementations for one and the same bundle base name and locale? Once the look-up code in ResourceBundle has found a provider which returns a bundle for a requested name and locale, it will not query any other bundle providers. In our case though, we need to be able to obtain messages from any of the bundles contributed by the different modules: messages related to green keeping must be obtained from the bundle of the dev.morling.links.greenkeeping module, tournament messages from dev.morling.links.tournament, and so on.\n The idea to address this concern is the following:\n   Prefix each message key with a module specific string, resulting in keys like tournament.fullybooked, greenkeeping.greenclosed, etc.\n  When requesting the bundle for a given key in the error handler class, obtain the key\u0026#8217;s prefix and pass it to bundle providers\n  Let bundle providers react only to their specific message prefix\n   This is where things become a little bit fiddly: there isn\u0026#8217;t a really good way for passing such contextual information from bundle consumers to providers. Our loop hole here will be to squeeze that information into the the requested Locale instance. Besides the well-known language and country attributes, Locale can also carry variant data and even application specific extensions.\n The latter, in form of a private use extension, would actually be pretty much ideal for our purposes. But unfortunately, extensions aren\u0026#8217;t evaluated by the look-up routine in ResourceBundle. So instead we\u0026#8217;ll go with propagating the key namespace information via the locale\u0026#8217;s variant. First, let\u0026#8217;s revisit the code in the ErrorHandler class:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 public class ErrorHandler { public String getErrorMessage(String key, UserContext context) { String prefix = key.split(\"\\\\.\")[0]; (1) Locale locale = new Locale( (2) context.getLocale().getLanguage(), context.getLocale().getCountry(), prefix ); ResourceBundle bundle = ResourceBundle.getBundle( \"dev.morling.links.core.LinksMessages\", locale); (3) return \"[User: \" + context.getName() + \"] \" + bundle.getString(key); (4) } }      1 Extract the key prefix, e.g. \"greenkeeping\"   2 Construct a new Locale, using the language and country information from the current user\u0026#8217;s locale and the key prefix as variant   3 Retrieve the bundle using the adjusted locale   4 Prepare the error message    Based on this approach, the resource bundle provider implementation in the greenkeeping module looks like so:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class GreenKeepingMessagesProvider extends AbstractResourceBundleProvider implements LinksMessagesProvider { @Override public ResourceBundle getBundle(String baseName, Locale locale) { if (locale.getVariant().equals(\"greenkeeping\")) { (1) baseName = baseName.replace(\"core.LinksMessages\", \"greenkeeping.internal.LinksMessages\"); (2) locale = new Locale(locale.getLanguage(), locale.getCountry()); (3) return super.getBundle(baseName), locale); } return null; (4) } }      1 This provider only should return a bundle for \"greenkeeping\" messages   2 Retrieve the bundle, adjusting the name (see below)   3 Create a Locale without the variant   4 Let other providers kick in for messages unrelated to green-keeping    The adjustment of the bundle name deserves some more explanation. The module system forbids so-called \"split packages\", i.e. packages of the same name in several modules of an application. That\u0026#8217;s why we cannot have a bundle named dev.morling.links.core.LinksMessages in multiple modules, even if the package dev.morling.links.core isn\u0026#8217;t exported by any of them. So each module must have its bundles in a specific package, and the bundle provider has to adjust the name accordingly, e.g. into dev.morling.links.greenkeeping.internal.LinksMessages in the greenkeeping module.\n As with the service consumer, the service provider also must be declared in the module\u0026#8217;s descriptor:\n 1 2 3 4 5 6 module dev.morling.links.greenkeeping { requires dev.morling.links.core; provides dev.morling.links.core.spi.LinksMessagesProvider with dev.morling.links.greenkeeping.internal. ↩ GreenKeepingMessagesProvider; }    Note how the package of the provider and the bundle isn\u0026#8217;t exported or opened, solely being exposed via the service loader mechanism. For the sake of completeness, here are two resource bundle files from the greenkeeping module, one for English, and one for German:\n 1 greenkeeping.greenclosed=Green closed due to mowing    1 greenkeeping.greenclosed=Grün wegen Pflegearbeiten gesperrt    Lastly, some test for the ErrorHandler class, making sure it works as expected:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ErrorHandler errorHandler = new ErrorHandler(); String message = errorHandler.getErrorMessage(\"greenkeeping.greenclosed\", new UserContext(\"Bob\", Locale.US)); assert message.equals(\"[User: Bob] Green closed due to mowing\"); message = errorHandler.getErrorMessage(\"greenkeeping.greenclosed\", new UserContext(\"Herbert\", Locale.GERMANY)); assert message.equals(\"[User: Herbert] Grün wegen \" + \"Pflegearbeiten gesperrt\"); message = errorHandler.getErrorMessage(\"tournament.fullybooked\", new UserContext(\"Bob\", Locale.US)); assert message.equals(\"[User: Bob] This tournament is fully booked\");      Running on the Classpath At this point, the design supports cross-module look-ups of resource bundles when running the application on the module path. Can we also make it work when running the same modules on the classpath instead? Indeed we can, but some slight additions to the core module will be needed. The reason being, that ResourceBundleProvider service contract isn\u0026#8217;t considered at all by the the bundle retrieval logic in ResourceBundle when running on the classpath.\n The way out is to provide a custom ResourceBundle.Control implementation which mimicks the logic for adjusting the bundle names based on the requested locale variant, as done by the different providers above:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public class LinksMessagesControl extends Control { @Override public String toBundleName(String baseName, Locale locale) { if (locale.getVariant() != null) { baseName = baseName.replace(\"core.LinksMessages\", locale.getVariant() + \".internal.LinksMessages\"); (1) locale = new Locale(locale.getLanguage(), locale.getCountry()); (2) return super.toBundleName(baseName, locale); } return super.toBundleName(baseName, locale); } }      1 Adjust the requested bundle name so that the module-specific bundles are retrieved   2 Drop the variant name from the locale    Now we could explicitly pass in an instance of that Control implementation when retrieving a resource bundle through ResourceBundle::getBundle(), but there\u0026#8217;s a simpler solution in form of the not overly widely known ResourceBundleControlProvider API:\n 1 2 3 4 5 6 7 8 9 10 11 public class LinksMessagesControlProvider implements ResourceBundleControlProvider { @Override public Control getControl(String baseName) { if (baseName.equals(\"dev.morling.links.core.LinksMessages\")) { (1) return new LinksMessagesControl(); } return null; } }      1 Return the LinksMessagesControl when the LinksMessages bundle is requested    This is another service provider contract; its implementations are retrieved from the classpath when obtaining a resource bundle and no control has been given explicity. Of course, the service implementation still needs to be registered, this time using the traditional approach of specifying the implementation name(s) in the META-INF/services/java.util.spi.ResourceBundleControlProvider file:\n dev.morling.links.core.internal.LinksMessagesControlProvider   With the control and control provider in place, the modular resource bundle look-up will work on the module path as well as the classpath, when running on Java 9+. There\u0026#8217;s one caveat remaining though if we want to enable the application also to be run on the classpath with Java 8.\n In Java 8, ResourceBundleControlProvider implementations are not picked up from the classpath, but only via the Java extension mechanism (now deprecated). This means you\u0026#8217;d have to provide the custom control provider through the lib/ext or jre/lib/ext directory of your JRE or JDK, respectively, which often isn\u0026#8217;t very practical. At this point we might be ready to cave in and just pass in the custom control implementation to ResourceBundle::getBundle(). But we can\u0026#8217;t actually do that: when invoked in a named module on Java 9+ (which is the case when running the application on the module path), the getBundle(String, Locale, Control) method will raise an UnsupportedOperationException!\n To overcome this last obstacle and make the application useable across the different Java versions, we can resort to the multi-release JAR mechanism: two different versions of the ErrorHandler class can be provided within a single JAR, one to be used with Java 8, and another one to be used with Java 9 and later. The latter calls getBundle(String, Locale), i.e. not passing the control, thus using the resource bundle providers (when running on the module path) or the control provider (when running on the classpath). The former invokes getBundle(String, Locale, Control), allowing the custom control to be used on Java 8.\n     Building Multi-Release JARs When multi-release JARs were first introduced in Java 9 with JEP 238, tool support for building them was non-existent, making this task quite a challenging one. Luckily, the situation has improved a lot since then. When using Apache Maven, only two plug-ins need to be configured:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 ... \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; (1) \u0026lt;id\u0026gt;compile-java-9\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;compile\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;compile\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;release\u0026gt;9\u0026lt;/release\u0026gt; (2) \u0026lt;compileSourceRoots\u0026gt; \u0026lt;compileSourceRoot\u0026gt; ${project.basedir}/src/main/java-9 (3) \u0026lt;/compileSourceRoot\u0026gt; \u0026lt;/compileSourceRoots\u0026gt; \u0026lt;multiReleaseOutput\u0026gt;true\u0026lt;/multiReleaseOutput\u0026gt; (4) \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-jar-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;archive\u0026gt; \u0026lt;manifestEntries\u0026gt; \u0026lt;Multi-Release\u0026gt;true\u0026lt;/Multi-Release\u0026gt; (5) \u0026lt;/manifestEntries\u0026gt; \u0026lt;/archive\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; ...      1 Set up another execution of the Maven compiler plug-in for the Java 9 specific sources,   2 using Java 9 bytecode level,   3 picking up the sources from src/main/java-9,   4 and organizing the compilation output in the multi-release structure under META-INF/versions/\u0026#8230;\u0026#8203;   5 Configure the Maven JAR plug-in so that the Multi-Release manifest entry is set, marking the JAR als a multi-release JAR          Discussion and Wrap-Up Let\u0026#8217;s wrap up and evaluate whether the proposed implementation satisfies our original requirements:\n   Modules of the application contribute bundles with their specific error messages: ✅ Each module of the Links application can provide its own bundle(s), using a specific key prefix; we could even take it a step further and provide bundles via separate i18n modules, for instance created by an external translation agency, independent from the development teams\n  Central error handler component can use these bundles for displaying or logging the error messages: ✅ The error handler in the core module can retrieve messages from all the bundles in the different modules, freeing the developers of the application modules from details like adding the user\u0026#8217;s name to the final messages\n  No knowledge about the specific modules in the central component: ✅ Thanks to the different providers (or the custom Control, respectively), there is no need for registering the specific bundles with the error handler in the core module; further modules could be added to the Links application and the error handler would be able to obtain messages from the resource bundles contributed by them\n   With a little bit of extra effort, it also was possible to design the code in the core module in a way that the application can be used with different Java versions and configurations: on the module path with Java 9+, on the classpath with Java 9+, on the classpath with Java 8.\n If you\u0026#8217;d like to explore the complete code by yourself, you can find it in the modular-resource-bundles GitHub repository. To learn more about resource bundle retrieval in named modules, please refer to the extensive documentation of ResourceBundle and ResourceBundleProvider.\n Many thanks to Hans-Peter Grahsl for providing feedback while writing this post!\n  ","id":21,"publicationdate":"Aug 29, 2021","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe \u003ca href=\"https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/ResourceBundle.html\"\u003e\u003ccode\u003eResourceBundle\u003c/code\u003e\u003c/a\u003e class is Java\u0026#8217;s workhorse for managing and retrieving locale specific resources,\nsuch as error messages of internationalized applications.\nWith the advent of the module system in Java 9, specifics around discovering and loading resource bundles have changed quite a bit, in particular when it comes to retrieving resource bundles across the boundaries of named modules.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIn this blog post I\u0026#8217;d like to discuss how resource bundles can be used in a multi-module application\n(i.e. a \"modular monolith\") for internationalizing error messages.\nThe following requirements should be satisified:\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Resource Bundle Look-ups in Modular Java Applications","uri":"https://www.morling.dev/blog/resource-bundle-lookups-in-modular-java-applications/"},{"content":"Unit testing, for performance\n It\u0026#8217;s with great pleasure that I\u0026#8217;m announcing the first official release of JfrUnit today!\n JfrUnit is an extension to JUnit which allows you to assert JDK Flight Recorder events in your unit tests. This capability opens up a number of interesting use cases in the field of testing JVM-based applications:\n   You can use JfrUnit to ensure your application produces the custom JFR events you expect it to emit\n  You can use JfrUnit to identify potential performance regressions of your application by means of tracking JFR events e.g. for garbage collection, memory allocation and network I/O\n  You can use JfrUnit together with JMC Agent for whitebox tests of your application, ensuring specific methods are invoked with the expected parameters and return values\n   Getting Started With JfrUnit JfrUnit is available on Maven Central (a big shout-out to Andres Almiray for setting up a fully automated release pipeline using the excellent JReleaser project!). If you\u0026#8217;re working with Apache Maven, add the following dependency to your pom.xml file:\n ... \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.moditect.jfrunit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jfrunit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.0.Alpha1\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; ...   Alternatively, you can of course build JfrUnit from source yourself, as described in the project\u0026#8217;s README file.\n     What is ModiTect? JfrUnit is part of the ModiTect family of open-source projects. All the ModiTect projects are in some way related to Java infrastructure, such as the Java Module System, or JDK Flight Recorder. Besides JfrUnit, the following project are currently developed under the ModiTect umbrella:\n   ModiTect: this eponymous project provides tooling for the Java Module System, e.g. for adding module descriptors while building with Java 8, creating jlink images, etc.\n  Layrry: a Runner and API for layered Java applications, which lets you use the module system\u0026#8217;s notion of module layers for implementing plug-in architectures, loading multiple versions of one dependency into your application, etc.\n  Deptective 🕵️: a plug-in for the javac compiler for analysing, validating and enforcing well-defined relationships between the packages of a Java application\n       With that dependency in place, the steps of using JfrUnit are the following:\n   Enable the JFR event type(s) you want to assert against\n  Run the application logic under test\n  Assert the emitted JFR events\n   To make things more tangible, here\u0026#8217;s an example that asserts the memory allocation done by a Quarkus-based web application for a specific use case:\n @Test @EnableEvent(\"jdk.ObjectAllocationInNewTLAB\") (1) @EnableEvent(\"jdk.ObjectAllocationOutsideTLAB\") public void retrieveTodoShouldYieldExpectedAllocation() throws Exception { Random r = new Random(); HttpClient client = HttpClient.newBuilder() .build(); // warm-up (2) for (int i = 1; i\u0026lt;= WARMUP_ITERATIONS; i++) { if (i % 1000 == 0) { System.out.println(i); } executeRequest(r.nextInt(20) + 1, client); } jfrEvents.awaitEvents(); jfrEvents.reset(); (3) (4) for (int i = 1; i\u0026lt;= ITERATIONS; i++) { if (i % 1000 == 0) { System.out.println(i); } executeRequest(r.nextInt(20) + 1, client); } jfrEvents.awaitEvents(); (5) long sum = jfrEvents.filter(this::isObjectAllocationEvent) .filter(this::isRelevantThread) .mapToLong(this::getAllocationSize) .sum(); assertThat(sum / ITERATIONS).isLessThan(33_000); (6) }     1 Enable the jdk.ObjectAllocationInNewTLAB and jdk.ObjectAllocationOutsideTLAB JFR event types; on Java 16 and beyond, you could also use the new jdk.ObjectAllocationSample type instead   2 Do some warm-up iterations so to achieve a steady state for the memory allocation rate   3 Reset the JfrUnit event collector after the warm-up   4 Run the code under test, in this case invoking some REST API of the application   5 Wait until all the events from the test have been received   6 Run assertions against the JFR events, in this case summing up all memory allocations and asserting that the value per REST call isn\u0026#8217;t larger than 33K (the exact threshold has been determined upfront)    The general idea behind this testing approach is that a regression in regards to metrics like memory allocation or I/O\u0026#8201;\u0026#8212;\u0026#8201;e.g. with a database\u0026#8201;\u0026#8212;\u0026#8201;can be a hint for a performance degredation. Allocating more memory than anticipated may be an indicator that your application started to do something which it hadn\u0026#8217;t done before, and which may impact its latency and through-put characteristics.\n To learn more about this approach for identifying potential performance regressions, please refer to this post, which introduced JfrUnit originally.\n   Groovier Tests With Spock Thanks to an outstanding contribution by Petr Hejl, instead of the Java-based API, you can also use Groovy and the Spock framework for your JfrUnit tests, which makes for very compact and nicely readable tests. Here\u0026#8217;s an example for asserting two JFR events using the Spock integration:\n class JfrSpec extends Specification { JfrEvents jfrEvents = new JfrEvents() @EnableEvent('jdk.GarbageCollection') (1) @EnableEvent('jdk.ThreadSleep') def 'should Have GC And Sleep Events'() { when: (2) System.gc() sleep(1000) then: (3) jfrEvents['jdk.GarbageCollection'] jfrEvents['jdk.ThreadSleep'].withTime(Duration.ofMillis(1000)) } }     1 Enable the jdk.GarbageCollection and jdk.ThreadSleep event types   2 Run the test code   3 Assert the events; thanks to the integration with Spock, no explicit barrier for awaiting all events is needed    To learn more about the Spock-based approach of using JfrUnit, please refer to the instructions in the README.\n For getting started with JfrUnit yourself, you may take a look at the jfrunit-examples repo, which shows some common usages the project.\n   Outlook This first Alpha release is an important milestone for the JfrUnit project. Since its inception in the December of last year, I\u0026#8217;ve received tons of invaluable feedback, and the project has matured quite a bit.\n In terms of next steps, apart from further expanding and honing the API, one area I\u0026#8217;d like to explore with JfrUnit is keeping track of and analysing historical event data from multiple test runs over a longer period of time.\n For instance, consider a case where your REST call allocates 33 KB today, 40 KB next month, 50 KB the month after, etc. Each increase by itself may not be problematic, but when comparing the results from today to those of a run in six months from now, a substantial regression may have accumulated. For identifying and analysing such trends, loading JfrUnit result data into a time series database, or repository systems like Hyperfoil Horreum, may be a very interesting feature.\n On a related note, John O\u0026#8217;Hara has started work towards automated event analysis using the rules system of JDK Mission Control, so stay tuned for some really exciting developments in this area!\n Last but not least, I\u0026#8217;d like say thank you to all the folks helping with the work on JfrUnit, be it through discussions, raising feature requests or bug reports, or code changes, including the following fine folks who have contributed to the JfrUnit repository at this point: Andres Almiray, Hash Zhang, Leonard Brünings, Manyanda Chitimbo, Matthias Andreas Benkard, Petr Hejl, Sam Brannen, Sullis, Thomas, Tivrfoa, and Tushar Badgu. Onwards and upwards!\n  ","id":22,"publicationdate":"Aug 4, 2021","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003e\u003cem\u003eUnit testing, for performance\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIt\u0026#8217;s with great pleasure that I\u0026#8217;m announcing the first official release of JfrUnit today!\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/moditect/jfrunit\"\u003eJfrUnit\u003c/a\u003e is an extension to JUnit which allows you to assert \u003ca href=\"https://openjdk.java.net/jeps/328\"\u003eJDK Flight Recorder\u003c/a\u003e events in your unit tests.\nThis capability opens up a number of interesting use cases in the field of testing JVM-based applications:\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"ulist\"\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eYou can use JfrUnit to ensure your application produces the \u003ca href=\"blog/rest-api-monitoring-with-custom-jdk-flight-recorder-events/\"\u003ecustom JFR events\u003c/a\u003e you expect it to emit\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eYou can use JfrUnit to identify potential performance regressions of your application by means of tracking JFR events e.g. for garbage collection, memory allocation and network I/O\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eYou can use JfrUnit together with \u003ca href=\"https://wiki.openjdk.java.net/display/jmc/The+JMC+Agent\"\u003eJMC Agent\u003c/a\u003e for whitebox tests of your application, ensuring specific methods are invoked with the expected parameters and return values\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e","tags":null,"title":"Introducing JfrUnit 1.0.0.Alpha1","uri":"https://www.morling.dev/blog/introducing-jfrunit-1-0-0-alpha1/"},{"content":"Over the course of the last few months, I\u0026#8217;ve had the pleasure to serve on the Kafka Summit program committee and review several hundred session abstracts for the three Summits happening this year (Europe, APAC, Americas). That\u0026#8217;s not only a big honour, but also a unique opportunity to learn what excites people currently in the Kafka eco-system (and yes, it\u0026#8217;s a fair amount of work, too ;).\n While voting on the proposals, and also generally aspiring to stay informed of what\u0026#8217;s going on in the Kafka community at large, I noticed a few repeating themes and topics which I thought would be interesting to share (without touching on any specific talks of course). At first I meant to put this out via a Twitter thread, but then it became a bit too long for that, so I decided to write this quick blog post instead. Here it goes!\n Cambrian Explosion of Connectors Apache Kafka is a great commit log and streaming platform, but of course you also need to get data into and out of it. Kafka Connect is vital for doing just that, linking data sources and sinks to the Kafka backbone. Be it integration of legacy apps and databases, external systems (e.g. IoT), data lakes, or DWHs, different CDC options (including Debezium, of course)\u0026#8201;\u0026#8212;\u0026#8201;There\u0026#8217;s connectors for everything.\n The ever-increasing number of connectors is accompanied by growing operational maturity (large-scale deployments, KC on K8s, etc.) and upcoming improvements like KIP-618 (exactly-once source connectors) or KIP-731 (rate limiting). There\u0026#8217;s so much activity within the Kafka connector eco-system, and it really sets Kafka apart from alternatives.\n   Democratization of Data Pipelines Another exciting trend is a move to self-service Kafka environments, with portals and infrastructure aimed at reducing the friction for standing up new deployments of Kafka, Connect, and related components like schema registries, while keeping track of and running everything in a safe way, e.g. when it comes to things like access control, role and schema management, (topic) naming conventions, managing data lineage and quality, ensuring compliance, privacy and operational best-practices, or observability.\n A healthy combination of in-house as well as open-source developments is happening here, and I\u0026#8217;m sure it\u0026#8217;s a field where we\u0026#8217;ll see more tools and solutions appearing in the next months and years.\n   Stream Processing for Everyone Not exactly a new trend, but definitely a growing one: more and more users appreciate the benefits of stream processing for working with their data in Kafka, filtering, transforming, enriching and aggregating it either programmatically using libraries such as Kafka Streams or Apache Flink, or in a declarative fashion, e.g. via ksqlDB or Flink SQL. Either way, small, focused stream processing apps are a true manifestation of the microservices idea\u0026#8201;\u0026#8212;\u0026#8201;have cohesive, independent application units, each focusing on one particular task and loosely coupled to each other, via Apache Kafka in this case.\n It\u0026#8217;s great to see the uptake here, including approaches for dynamic scaling based on end-to-end lag, and innovative new solutions for efficient incremental view materialization.\n   Honorable Mentions Besides these bigger trends, there\u0026#8217;s also a few more specific topics which I saw several times and which I found very interesting:\n   Tools and best practices for testing of Kafka-based applications (e.g. for creating test data or mock producers/consumers)\n  Feeding ML/AI models is becoming a popular Kafka use case; it\u0026#8217;s not my field of experience at all, but it seems like a very logical choice to run ML algorithms on data ingested via Kafka, allowing to gain new insight into business data with a low latency\n  Pushing data to consumers via GraphQL; (still?) even more niche probably, but I love the idea of push updates to browsers based on data from Kafka; this should allow for some interesting use cases\n   Of course there\u0026#8217;s also things like geo-replicated Kafka, the ongoing move towards managed Kafka service offerings (which raises interesting questions around connectivity to on-prem systems and data), architectural trends like data meshes, and so much more.\n If you want to learn more about these and many other facets of Apache Kafka, its use cases, best practices, and latest developments, make sure to register for Kafka Summit (it\u0026#8217;s free and online). The sessions from the Europe run can already be watched, while the APAC (July 27 - 28) and Americas (September 14 - 15) editions are still to come.\n  ","id":23,"publicationdate":"May 28, 2021","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eOver the course of the last few months, I\u0026#8217;ve had the pleasure to serve on the \u003ca href=\"https://www.kafka-summit.org/\"\u003eKafka Summit\u003c/a\u003e program committee and review several hundred session abstracts for the three Summits happening this year (Europe, APAC, Americas).\nThat\u0026#8217;s not only a big honour, but also a unique opportunity to learn what excites people currently in the Kafka eco-system\n(and yes, it\u0026#8217;s a fair amount of work, too ;).\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eWhile voting on the proposals, and also generally aspiring to stay informed of what\u0026#8217;s going on in the Kafka community at large, I noticed a few repeating themes and topics which I thought would be interesting to share\n(without touching on any specific talks of course).\nAt first I meant to put this out via a Twitter thread, but then it became a bit too long for that, so I decided to write this quick blog post instead.\nHere it goes!\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Three Plus Some Lovely Kafka Trends","uri":"https://www.morling.dev/blog/three-plus-some-lovely-kafka-trends/"},{"content":"Sometimes, less is more. One case where that\u0026#8217;s certainly true is dependencies. And so it shouldn\u0026#8217;t come at a surprise that the Apache Kafka community is eagerly awaiting the removal of the dependency to the ZooKeeper service, which currently is used for storing Kafka metadata (e.g. about topics and partitions) as well as for the purposes of leader election in the cluster.\n The Kafka improvement proposal KIP-500 (\"Replace ZooKeeper with a Self-Managed Metadata Quorum\") promises to make life better for users in many regards:\n   Better getting started and operational experience by requiring to run only one system, Kafka, instead of two\n  Removing potential for discrepancies of metadata state between ZooKeeper and the Kafka controller\n  Simplifying configuration, for instance when it comes to security\n  Better scalability, e.g. in terms of number of partitions; faster execution of operations like topic creation\n   With KIP-500, Kafka itself will store all the required metadata in an internal Kafka topic, and controller election will be done amongst (a subset of) the Kafka cluster nodes themselves, based on a variant of the Raft protocol for distributed consensus. Removing the ZooKeeper dependency is great not only for running Kafka clusters in production, also for local development and testing being able to start up a Kafka node with a single process comes in very handy.\n Having been in the works for multiple years, ZK-less Kafka, also known as KRaft (\"Kafka Raft metadata mode\"), was recently published as an early access feature with Kafka 2.8. I.e. the perfect time to get my hands on this and get a first feeling for ZK-less Kafka myself. Note this post isn\u0026#8217;t meant to be a thorough evaluation or systematic testing of the new Kafka deployment mode, rather take it as a description of how to get started with playing with ZK-less Kafka and of a few observations I made while doing so.\n In the world of ZK-less Kafka, there\u0026#8217;s two node roles for nodes: controller and broker. Each node in the cluster can have either one or both roles (\"combined nodes\"). All controller nodes elect the active controller, which is in charge of coordinating the whole cluster, with other controller nodes acting as hot stand-by replicas. In the KRaft KIPs, the active controller sometimes also is simply referred to as leader. This may appear confusing at first, if you are familiar with the existing concept of partition leaders. It started to make sense to me once I realized that the active controller is the leader of the sole partition of the metadata topic. All broker nodes are handling client requests, just as before with ZooKeeper.\n While for smaller clusters it is expected that the majority of, or even all cluster nodes act as controllers, you may have dedicated controller-only nodes in larger clusters, e.g. 3 controller nodes and 7 broker nodes in a cluster of 10 nodes overall. As per the KRaft README, having dedicated controller nodes should increase overall stability, as for instance an out-of-memory error on a broker wouldn\u0026#8217;t impact controllers, or potentially even cause a leader re-election.\n Trying ZK-less Kafka Yourself As a foundation, I\u0026#8217;ve created a variant of the Debezium 1.6 container image, which updates Kafka from 2.7 to Kafka 2.8, and also does the required changes to the entrypoint script for using the KRaft mode. Note this change hasn\u0026#8217;t been merged yet to the upstream Debezium repository, so if you\u0026#8217;d like to try out things by yourself, you\u0026#8217;ll have to clone my repo, and then build the container image yourself like this:\n $ git clone git@github.com:gunnarmorling/docker-images.git $ cd docker-images/kafka/1.6 $ docker build -t debezium/zkless-kafka:1.6 --build-arg DEBEZIUM_VERSION=1.6.0 .   In order to start the image with Kafka in KRaft mode, the CLUSTER_ID environment variable must be set. A value can be obtained using the new bin/kafka-storage.sh script; going forward, we\u0026#8217;ll likely add an option to the Debezium Kafka container image for doing so. If that variable is set, the entrypoint script of the image does the following things:\n   Use config/kraft/server.properties instead of config/server.properties as the Kafka configuration file; this one comes with the Kafka distribution and is meant for nodes which should have both the controller and broker roles; i.e. the container image currently only supports combined nodes\n  Format the node\u0026#8217;s storage directory, if not the case yet\n  Set up a listener for controller communication\n   Based on that, here is what\u0026#8217;s needed in a Docker Compose file for spinning up a Kafka cluster with three nodes:\n version: '2' services: kafka-1: image: debezium/zkless-kafka:1.6 ports: - 19092:9092 - 19093:9093 environment: - CLUSTER_ID=oh-sxaDRTcyAr6pFRbXyzA (1) - BROKER_ID=1 (2) - KAFKA_CONTROLLER_QUORUM_VOTERS=1@kafka-1:9093,2@kafka-2:9093,3@kafka-3:9093 (3) kafka-2: image: debezium/zkless-kafka:1.6 ports: - 29092:9092 - 29093:9093 environment: - CLUSTER_ID=oh-sxaDRTcyAr6pFRbXyzA (1) - BROKER_ID=2 (2) - KAFKA_CONTROLLER_QUORUM_VOTERS=1@kafka-1:9093,2@kafka-2:9093,3@kafka-3:9093 (3) kafka-3: image: debezium/zkless-kafka:1.6 ports: - 39092:9092 - 39093:9093 environment: - CLUSTER_ID=oh-sxaDRTcyAr6pFRbXyzA (1) - BROKER_ID=3 (2) - KAFKA_CONTROLLER_QUORUM_VOTERS=1@kafka-1:9093,2@kafka-2:9093,3@kafka-3:9093 (3)     1 Cluster id; must be the same for all the nodes   2 Broker id; must be unique for each node   3 Addresses of all the controller nodes in the format id1@host1:port1,id2@host2:port2\u0026#8230;\u0026#8203;    No ZooKeeper nodes, yeah :)\n Working on Debezium, and being a Kafka Connect aficionado allaround, I\u0026#8217;m also going to add Connect and a Postgres database for testing purposes (you can find the complete Compose file here):\n version: '2' services: # ... connect: image: debezium/connect:1.6 ports: - 8083:8083 links: - kafka-1 - kafka-2 - kafka-3 - postgres environment: - BOOTSTRAP_SERVERS=kafka-1:9092 - GROUP_ID=1 - CONFIG_STORAGE_TOPIC=my_connect_configs - OFFSET_STORAGE_TOPIC=my_connect_offsets - STATUS_STORAGE_TOPIC=my_connect_statuses postgres: image: debezium/example-postgres:1.6 ports: - 5432:5432 environment: - POSTGRES_USER=postgres - POSTGRES_PASSWORD=postgres   Now let\u0026#8217;s start everything:\n $ docker-compose -f docker-compose-zkless-kafka.yaml up   Let\u0026#8217;s also register an instance of the Debezium Postgres connector, which will connect to the PG database and take an initial snapshot, so we got some topics with a few messages to play with:\n $ curl -0 -v -X POST http://localhost:8083/connectors \\ -H \"Expect:\" \\ -H 'Content-Type: application/json; charset=utf-8' \\ --data-binary @- \u0026lt;\u0026lt; EOF{ \"name\": \"inventory-connector\", \"config\": { \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\", \"tasks.max\": \"1\", \"database.hostname\": \"postgres\", \"database.port\": \"5432\", \"database.user\": \"postgres\", \"database.password\": \"postgres\", \"database.dbname\" : \"postgres\", \"database.server.name\": \"dbserver1\", \"schema.include\": \"inventory\", \"topic.creation.default.replication.factor\": 2, \"topic.creation.default.partitions\": 10 } } EOF   Note how this is using a replication factor of 2 for all the topics created via Kafka Connect, which will come in handy for some experimenting later on.\n The nosy person I am, I first wanted to take a look into that new internal metadata topic, where all the cluster metadata is stored. As per the release announcement, it should be named @metadata. But no such topic shows up when listing the available topics; only the __consumer_offsets topic, the change data topics created by Debezium, and some Kafka Connect specific topics are shown:\n # Get a shell on one of the broker containers $ docker-compose -f docker-compose-zkless-kafka.yaml exec kafka-1 bash # In that shell $ /kafka/bin/kafka-topics.sh --bootstrap-server kafka-3:9092 --list __consumer_offsets dbserver1.inventory.customers dbserver1.inventory.geom dbserver1.inventory.orders dbserver1.inventory.products dbserver1.inventory.products_on_hand dbserver1.inventory.spatial_ref_sys my_connect_configs my_connect_offsets my_connect_statuses   Seems that this topic is truly meant to be internal; also trying to consume messages from the topic with kafka-console-consumer.sh or kafkacat fails due to the invalid topic name. Let\u0026#8217;s see whether things are going to change here, since KIP-595 (\"A Raft Protocol for the Metadata Quorum\") explicitly mentions the ability for consumers to \"read the contents of the metadata log for debugging purposes\".\n In the meantime, we can take a look at the contents of the metadata topic using the kafka-dump-log.sh utility, e.g. filtering out all RegisterBroker records:\n $ /kafka/bin/kafka-dump-log.sh --cluster-metadata-decoder \\ --skip-record-metadata \\ --files /kafka/data//\\@metadata-0/*.log | grep REGISTER_BROKER payload: {\"type\":\"REGISTER_BROKER_RECORD\",\"version\":0,\"data\":{\"brokerId\":3,\"incarnationId\":\"O_PiUrjNTsqVEQv61gB2Vg\",\"brokerEpoch\":0,\"endPoints\":[{\"name\":\"PLAINTEXT\",\"host\":\"172.18.0.2\",\"port\":9092,\"securityProtocol\":0}],\"features\":[],\"rack\":null}} payload: {\"type\":\"REGISTER_BROKER_RECORD\",\"version\":0,\"data\":{\"brokerId\":1,\"incarnationId\":\"FbOZdz9rSZqTyuSKr12JWg\",\"brokerEpoch\":2,\"endPoints\":[{\"name\":\"PLAINTEXT\",\"host\":\"172.18.0.3\",\"port\":9092,\"securityProtocol\":0}],\"features\":[],\"rack\":null}} payload: {\"type\":\"REGISTER_BROKER_RECORD\",\"version\":0,\"data\":{\"brokerId\":2,\"incarnationId\":\"ZF_WQqk_T5q3l1vhiWT_FA\",\"brokerEpoch\":4,\"endPoints\":[{\"name\":\"PLAINTEXT\",\"host\":\"172.18.0.4\",\"port\":9092,\"securityProtocol\":0}],\"features\":[],\"rack\":null}} ...   The individual record formats are described in KIP-631 (\"The Quorum-based Kafka Controller\").\n Another approach would be to use a brand-new tool, kafka-metadata-shell.sh. Also defined in KIP-631, this utility script allows to browse a cluster\u0026#8217;s metadata, similarly to zookeeper-shell.sh known from earlier releases. For instance, you can list all brokers and get the metadata of the registration of node 1 like this:\n $ /kafka/bin/kafka-metadata-shell.sh --snapshot /kafka/data/@metadata-0/00000000000000000000.log Loading... Starting... [ Kafka Metadata Shell ] \u0026gt;\u0026gt; ls brokers configs local metadataQuorum topicIds topics \u0026gt;\u0026gt; ls brokers 1 2 3 \u0026gt;\u0026gt; cd brokers/1 \u0026gt;\u0026gt; cat registration RegisterBrokerRecord(brokerId=1, incarnationId=TmM_u-_cQ2ChbUy9NZ9wuA, brokerEpoch=265, endPoints=[BrokerEndpoint(name='PLAINTEXT', host='172.18.0.3', port=9092, securityProtocol=0)], features=[], rack=null) \u0026gt;\u0026gt;   Or to display the current leader:\n \u0026gt;\u0026gt; cat /metadataQuorum/leader MetaLogLeader(nodeId=1, epoch=12)   Or to show the metadata of a specific topic partition:\n \u0026gt;\u0026gt; cat /topics/dbserver1.inventory.customers/0/data { \"partitionId\" : 0, \"topicId\" : \"8xjqykVRT_WpkqbXHwbeCA\", \"replicas\" : [ 2, 3 ], \"isr\" : [ 2, 3 ], \"removingReplicas\" : null, \"addingReplicas\" : null, \"leader\" : 2, \"leaderEpoch\" : 0, \"partitionEpoch\" : 0 } \u0026gt;\u0026gt;   Those are just a few of the things you can do with kafka-metadata-shell.sh, and it surely will be an invaluable tool in the box of administrators in the ZK-less era. Another new tool is kafka-cluster.sh, which currently can do two things: displaying the unique id of a cluster, and unregistering a broker. While the former worked for me:\n $ /kafka/bin/kafka-cluster.sh cluster-id --bootstrap-server kafka-1:9092 Cluster ID: oh-sxaDRTcyAr6pFRbXyzA   The latter always failed with a NotControllerException, no matter on which node I invoked the command:\n $ /kafka/bin/kafka-cluster.sh unregister --bootstrap-server kafka-1:9092 --id 3 [2021-05-15 20:52:54,626] ERROR [AdminClient clientId=adminclient-1] Unregister broker request for broker ID 3 failed: This is not the correct controller for this cluster.   It\u0026#8217;s not quite clear to me whether I did something wrong, or whether this functionality simply should not be expected to be supported just yet.\n The Raft-based metadata quorum also comes with a set of new metrics (described in KIP-595), allowing to retrieve information like the current active controller, role of the node at hand, and more. Here\u0026#8217;s a screenshot of the metrics invoked on a non-leader node:\n     Taking Brokers Down An essential aspect to any distributed system like Kafka is the fact that invidual nodes of a cluster can disappear at any time, be it due to failures (node crashes, network splits, etc.), or due to controlled shut downs, e.g. for a version upgrade. So I was curious how Kafka in KRaft mode would deal with the situation where nodes in the cluster are stopped and then restarted. Note I\u0026#8217;m stopping nodes gracefully via docker-compose stop, instead of randomly crashing them, Jepsen-style ;)\n The sequence of events I was testing was the following:\n   Stop the current active controller, so two nodes from the original three-node cluster remain\n  Stop the then new active controller node, at which point the majority of cluster nodes isn\u0026#8217;t available any longer\n  Start both nodes again\n   Here\u0026#8217;s a few noteworthy things I observed. As you\u0026#8217;d expect, when stopping the active controller, a new leader was elected (as per the result of cat /metadataQuorum/leader in the Kafka metadata shell), and also all partitions which had the previous active controller as partition leader, got re-assigned (in this case node 1 was the active controller and got stopped):\n $ /kafka/bin/kafka-topics.sh --bootstrap-server kafka-2:9092 --describe --topic dbserver1.inventory.customers Topic: dbserver1.inventory.customers\tTopicId: a6qzjnQwQ2eLNSXL5svW8g\tPartitionCount: 10\tReplicationFactor: 2\tConfigs: segment.bytes=1073741824 Topic: dbserver1.inventory.customers\tPartition: 0\tLeader: 1\tReplicas: 1,3\tIsr: 1,3 Topic: dbserver1.inventory.customers\tPartition: 1\tLeader: 1\tReplicas: 3,1\tIsr: 1,3 Topic: dbserver1.inventory.customers\tPartition: 2\tLeader: 1\tReplicas: 1,2\tIsr: 1,2 Topic: dbserver1.inventory.customers\tPartition: 3\tLeader: 1\tReplicas: 2,1\tIsr: 1,2 Topic: dbserver1.inventory.customers\tPartition: 4\tLeader: 1\tReplicas: 2,1\tIsr: 1,2 Topic: dbserver1.inventory.customers\tPartition: 5\tLeader: 2\tReplicas: 3,2\tIsr: 2,3 Topic: dbserver1.inventory.customers\tPartition: 6\tLeader: 2\tReplicas: 3,2\tIsr: 2,3 Topic: dbserver1.inventory.customers\tPartition: 7\tLeader: 2\tReplicas: 2,3\tIsr: 2,3 Topic: dbserver1.inventory.customers\tPartition: 8\tLeader: 1\tReplicas: 2,1\tIsr: 1,2 Topic: dbserver1.inventory.customers\tPartition: 9\tLeader: 2\tReplicas: 3,2\tIsr: 2,3 # After stopping node 1 $ /kafka/bin/kafka-topics.sh --bootstrap-server kafka-2:9092 --describe --topic dbserver1.inventory.customers Topic: dbserver1.inventory.customers\tTopicId: a6qzjnQwQ2eLNSXL5svW8g\tPartitionCount: 10\tReplicationFactor: 2\tConfigs: segment.bytes=1073741824 Topic: dbserver1.inventory.customers\tPartition: 0\tLeader: 3\tReplicas: 1,3\tIsr: 3 Topic: dbserver1.inventory.customers\tPartition: 1\tLeader: 3\tReplicas: 3,1\tIsr: 3 Topic: dbserver1.inventory.customers\tPartition: 2\tLeader: 2\tReplicas: 1,2\tIsr: 2 Topic: dbserver1.inventory.customers\tPartition: 3\tLeader: 2\tReplicas: 2,1\tIsr: 2 Topic: dbserver1.inventory.customers\tPartition: 4\tLeader: 2\tReplicas: 2,1\tIsr: 2 Topic: dbserver1.inventory.customers\tPartition: 5\tLeader: 2\tReplicas: 3,2\tIsr: 2,3 Topic: dbserver1.inventory.customers\tPartition: 6\tLeader: 2\tReplicas: 3,2\tIsr: 2,3 Topic: dbserver1.inventory.customers\tPartition: 7\tLeader: 2\tReplicas: 2,3\tIsr: 2,3 Topic: dbserver1.inventory.customers\tPartition: 8\tLeader: 2\tReplicas: 2,1\tIsr: 2 Topic: dbserver1.inventory.customers\tPartition: 9\tLeader: 2\tReplicas: 3,2\tIsr: 2,3   Things got interesting though when also stopping the newly elected leader subsequently. At this point, the cluster isn\u0026#8217;t in a healthy state any longer, as no majority of nodes of the cluster is available for leader election. Logs of the remaining node are flooded with an UnknownHostException in this situation:\n kafka-3_1 | 2021-05-16 10:16:45,282 - WARN [kafka-raft-outbound-request-thread:NetworkClient@992] - [RaftManager nodeId=3] Error connecting to node kafka-2:9093 (id: 2 rack: null) kafka-3_1 | java.net.UnknownHostException: kafka-2 kafka-3_1 | at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797) kafka-3_1 | at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1505) kafka-3_1 | at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1364) kafka-3_1 | at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1298) kafka-3_1 | at org.apache.kafka.clients.DefaultHostResolver.resolve(DefaultHostResolver.java:27) kafka-3_1 | at org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:111) kafka-3_1 | at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:512) kafka-3_1 | at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:466) kafka-3_1 | at org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:172) kafka-3_1 | at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:985) kafka-3_1 | at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:311) kafka-3_1 | at kafka.common.InterBrokerSendThread.$anonfun$sendRequests$1(InterBrokerSendThread.scala:103) kafka-3_1 | at kafka.common.InterBrokerSendThread.$anonfun$sendRequests$1$adapted(InterBrokerSendThread.scala:99) kafka-3_1 | at scala.collection.Iterator.foreach(Iterator.scala:943) kafka-3_1 | at scala.collection.Iterator.foreach$(Iterator.scala:943) kafka-3_1 | at scala.collection.AbstractIterator.foreach(Iterator.scala:1431) kafka-3_1 | at scala.collection.IterableLike.foreach(IterableLike.scala:74) kafka-3_1 | at scala.collection.IterableLike.foreach$(IterableLike.scala:73) kafka-3_1 | at scala.collection.AbstractIterable.foreach(Iterable.scala:56) kafka-3_1 | at kafka.common.InterBrokerSendThread.sendRequests(InterBrokerSendThread.scala:99) kafka-3_1 | at kafka.common.InterBrokerSendThread.pollOnce(InterBrokerSendThread.scala:73) kafka-3_1 | at kafka.common.InterBrokerSendThread.doWork(InterBrokerSendThread.scala:94) kafka-3_1 | at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)   Here I think it\u0026#8217;d be great to get a more explicit indication in the logs of what\u0026#8217;s going on, clearly indicating the unhealthy status of the cluster at large.\n What\u0026#8217;s also interesting is that the remaining node claims to be a leader as per its exposed metrics and value of /metadataQuorum/leader in the metadata shell. This seems a bit dubious, as no leader election can happen without the majority of nodes available. Consequently, creation of a topic in this state also times out, so I suspect this is more an artifact of displaying the cluster state rather than of what\u0026#8217;s actually going on.\n Things get a bit more troublesome when restarting the two stopped nodes; Very often I\u0026#8217;d then see a very high CPU consumption on the Kafka nodes as well as the Connect node:\n $ docker stats CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS 642eb697fed6 tutorial_connect_1 122.04% 668.3MiB / 7.775GiB 8.39% 99.7MB / 46.9MB 131kB / 106kB 47 5d9806526f92 tutorial_kafka-1_1 9.24% 386.4MiB / 7.775GiB 4.85% 105kB / 104kB 0B / 877kB 93 767e6c0f6cd3 tutorial_kafka-3_1 176.40% 739.2MiB / 7.775GiB 9.28% 14.5MB / 40.6MB 0B / 1.52MB 120 a0ce8438557f tutorial_kafka-2_1 87.51% 567.8MiB / 7.775GiB 7.13% 6.52MB / 24.9MB 0B / 881kB 95 df978d220132 tutorial_postgres_1 0.00% 36.39MiB / 7.775GiB 0.46% 243kB / 5.49MB 0B / 79.4MB 9   In some cases stopping and restarting the Kafka nodes would help, other times only a restart of the Connect node would mitigate the situation. I didn\u0026#8217;t further explore this issue by taking a thread dump, but I suppose threads are stuck in some kind of busy spin loop at this point. The early access state of KRaft mode seems to be somewhat showing here. After bringing up the issue on the Kafka mailing list, I\u0026#8217;ve logged KAFKA-12801 for this problem, as it seems not to have been tracked before.\n On the bright side, once all brokers were up and running again, the cluster and the Debezium connector would happily continue their work.\n   Wrap-Up Not many features have been awaited by the Kafka community as eagerly as the removal of the ZooKeeper dependency. Rightly so: Kafka-based metadata storage and leader election will greatly simplify the operational burden for running Kafka and also allow for better scalability. Lifting the requirement for running separate ZooKeeper processes or even machines should also help to make things more cost-effective, so you should benefit from this change no matter whether you\u0026#8217;re running Kafka yourself or are using a managed service offering.\n The early access release of ZK-less Kafka in version 2.8 gives a first impression of what will hopefully be the standard way of running Kafka in the not too distant future. As very clearly stated in the KRaft README, you should not use this in production yet; this matches with the observerations made above: while running Kafka without ZooKeeper definitely feels great, there\u0026#8217;s still some rough edges to be sorted out. Also check out the README for a list of currently missing features, such as support of transactions, adding partitions to existing topics, partition reassignment, and more. Lastly, any distributed system should only be fully trusted after going through the grinder of the Jepsen test suite, which I\u0026#8217;m sure will only be a question of time.\n Despite the early state, I would very much recommend to get started testing ZK-less Kafka at this point, so to get a feeling for it and of course to report back any findings and insights. To do so, either download the upstream Kafka distribution, or build the Debezium 1.6 container image for Kafka with preliminary support for KRaft mode, which lets you set up a ZK-less Kafka cluster in no time.\n In order to learn more about ZK-less Kafka, besides diving into the relevant KIPs (which all are linked from the umbrella KIP-500), also check out the QCon talk \"Kafka Needs No Keeper\" by Colin McCabe, one of the main engineers driving this effort.\n  ","id":24,"publicationdate":"May 17, 2021","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eSometimes, less is more.\nOne case where that\u0026#8217;s certainly true is dependencies.\nAnd so it shouldn\u0026#8217;t come at a surprise that the \u003ca href=\"https://kafka.apache.org/\"\u003eApache Kafka\u003c/a\u003e community is eagerly awaiting the removal of the dependency to the \u003ca href=\"https://zookeeper.apache.org/\"\u003eZooKeeper\u003c/a\u003e service,\nwhich currently is used for storing Kafka metadata (e.g. about topics and partitions) as well as for the purposes of leader election in the cluster.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe Kafka improvement proposal \u003ca href=\"https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum\"\u003eKIP-500\u003c/a\u003e\n(\"Replace ZooKeeper with a Self-Managed Metadata Quorum\")\npromises to make life better for users in many regards:\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"ulist\"\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eBetter getting started and operational experience by requiring to run only one system, Kafka, instead of two\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRemoving potential for discrepancies of metadata state between ZooKeeper and the Kafka controller\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSimplifying configuration, for instance when it comes to security\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBetter scalability, e.g. in terms of number of partitions; faster execution of operations like topic creation\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e","tags":null,"title":"Exploring ZooKeeper-less Kafka","uri":"https://www.morling.dev/blog/exploring-zookeeper-less-kafka/"},{"content":"One of the ultimate strengths of Java is its strong notion of backwards compatibility: Java applications and libraries built many years ago oftentimes run without problems on current JVMs, and the compiler of current JDKs can produce byte code, that is executable with earlier Java versions.\n For instance, JDK 16 supports byte code levels going back as far as to Java 1.7; But: hic sunt dracones. The emitted byte code level is just one part of the story. It\u0026#8217;s equally important to consider which APIs of the JDK are used by the compiled code, and whether they are available in the targeted Java runtime version. As an example, let\u0026#8217;s consider this simple \"Hello World\" program:\n package com.example; import java.util.List; public class HelloWorld { public static void main(String... args) { System.out.println(List.of(\"Hello\", \"World!\")); } }   Let\u0026#8217;s assume we\u0026#8217;re using Java 16 for compiling this code, aiming for compatibility with Java 1.8. Historically, the Java compiler has provided the --source and --target options for this purpose, which are well known to most Java developers:\n $ javac --source 1.8 --target 1.8 -d classes HelloWorld.java warning: [options] bootstrap class path not set in conjunction with -source 8 1 warning   This compiles successfully (we\u0026#8217;ll come back on that warning in a bit). But if you actually try to run that class on Java 8, you\u0026#8217;re in for a bad suprise:\n $ java -classpath classes com.example.HelloWorld Exception in thread \"main\" java.lang.NoSuchMethodError: ↩ java.util.List.of(Ljava/lang/Object;Ljava/lang/Object;)Ljava/util/List; at com.example.HelloWorld.main(HelloWorld.java:7)   This makes sense: the List.of() methods were only introduced in Java 9, so they are not present in the Java 8 API. Shouldn\u0026#8217;t the compiler have let us know us about this? Absolutely, and that\u0026#8217;s where this warning about the bootstrap class path is coming in: the compiler recognized our potentially dangerous endavour and essentially suggested to compile against the class library matching the targeted Java version instead of that one of the JDK used for compilation. This is done using the -Xbootclasspath option:\n $ javac --source 1.8 --target 1.8 \\ -d classes \\ -Xbootclasspath:${JAVA_8_HOME}/jre/lib/rt.jar \\ (1) HelloWorld.java HelloWorld.java:7: error: cannot find symbol System.out.println(List.of(\"Hello\", \"World!\")); ^ symbol: method of(String,String) location: interface List 1 error     1 Path to the rt.jar of Java 8    That\u0026#8217;s much better: now the invocation of the List.of() method causes compilation to fail, instead of finding out about this problem only during testing, or worse, in production.\n While this approach works, it\u0026#8217;s not without issues: requiring the target Java version\u0026#8217;s class library complicates things quite a bit; multiple Java versions need to be installed, and the targeted JDK\u0026#8217;s location must be known, which for instance tends to make build processes not portable between different machines and platforms.\n Luckily, Java 9 improved things significantly here; by means of the new --release option, code can be compiled for older Java versions in a fully safe and portable way. Let\u0026#8217;s give this a try:\n $ javac --release 8 -d classes HelloWorld.java HelloWorld.java:7: error: cannot find symbol System.out.println(List.of(\"Hello\", \"World!\")); ^ symbol: method of(String,String) location: interface List 1 error   Very nice, the same compilation error as before, but without the need for any complex configuration besides the --release 8 option. So how does this work? Does the JDK come with full class libraries of all the earlier Java versions which it supports? Considering that the modules file of Java 16 has a size of more than one hundred megabytes (to be precise, 118 MB on macOS), that\u0026#8217;d clearly be not a good idea; We\u0026#8217;d end up with a JDK size of nearly one gigabyte.\n What\u0026#8217;s happening instead is that the JDK ships \"stripped-down class files corresponding to class files from the target platform versions\", as we can read in JEP 247 (\"Compile for Older Platform Versions\"), which introduced the --release option. Details about the implementation are sparse, though. The JEP only mentions a ZIP file named ct.sym which contains those signature files. So I started by taking a look at what\u0026#8217;s in there:\n $ unzip -l $JAVA_HOME/lib/ct.sym Archive: /Library/Java/JavaVirtualMachines/jdk-16.sdk/Contents/Home/lib/ct.sym Length Date Time Name --------- ---------- ----- ---- 0 03-26-2021 18:11 7/java.base/java/awt/peer/ 2557 03-26-2021 18:11 7/java.base/java/awt/peer/ComponentPeer.sig 542 03-26-2021 18:11 7/java.base/java/awt/peer/FramePeer.sig ... 856 03-26-2021 18:11 879A/java.activation/javax/activation/ActivationDataFlavor.sig 491 03-26-2021 18:11 879A/java.activation/javax/activation/CommandInfo.sig 299 03-26-2021 18:11 879A/java.activation/javax/activation/CommandObject.sig ... 1566 03-26-2021 18:11 9ABCDE/java.base/java/lang/Byte.sig 1616 03-26-2021 18:11 9ABCDE/java.base/java/lang/Short.sig ...   That\u0026#8217;s interesting, lots of *.sig files, organized in some at first odd-looking directory structure. So let\u0026#8217;s see what\u0026#8217;s there for the java.util.List class:\n $ unzip -l $JAVA_HOME/lib/ct.sym | grep \"java/util/List.sig\" 1481 03-26-2021 18:11 7/java.base/java/util/List.sig 1771 03-26-2021 18:11 8/java.base/java/util/List.sig 4040 03-26-2021 18:11 9/java.base/java/util/List.sig 4184 03-26-2021 18:11 A/java.base/java/util/List.sig 4097 03-26-2021 18:11 BCDEF/java.base/java/util/List.sig   Five different versions altogether, under the directories 7, 8, 9, A, and BCDEF. It took a few moments until the structure began to make sense to me: the top-level directory names encode Java version(s), and there\u0026#8217;s a new version of the signature file whenever its API changed. I.e. java.util.List changed in Java 7, 8, 9, 10 (A), and 11 (B), and has remained stable since then, i.e. from version 11 to 16, there have been no changes to the public List API.\n So let\u0026#8217;s dive in a bit further and compare the signature files of Java 8 and 9. As JEP 247 states that these files are (stripped-down) class files, we should be able to examine them using javap. In order to so, I had to change the file extensions from *.sig to *.class, though. After that, I could decompile the files using javap, save the result in text files and compare them using git:\n $ javap List8.class \u0026gt; List8.txt $ javap List9.class \u0026gt; List9.txt $ git diff --no-index List8.txt List9.txt diff --git a/List8.txt b/List9.txt index b2ca320..b276286 100644 --- a/List8.txt +++ b/List9.txt @@ -27,4 +27,16 @@ public interface java.util.List\u0026lt;E\u0026gt; extends java.util.Collection\u0026lt;E\u0026gt; { public abstract java.util.ListIterator\u0026lt;E\u0026gt; listIterator(int); public abstract java.util.List\u0026lt;E\u0026gt; subList(int, int); public default java.util.Spliterator\u0026lt;E\u0026gt; spliterator(); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E, E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E, E, E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E, E, E, E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E, E, E, E, E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E, E, E, E, E, E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E, E, E, E, E, E, E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E, E, E, E, E, E, E, E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E, E, E, E, E, E, E, E, E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E, E, E, E, E, E, E, E, E, E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E...); }   As expected, the diff between the two signature files reveals the addition of the different List.of() methods in Java 9, as such exactly the reason why the Hello World example from the beginning cannot be executed on Java 8.\n     Debugging the Java Compiler In order to understand in detail how the ct.sym file is used by the Java compiler, it can be useful to run javac in debug mode. As javac is written in Java itself, this can be done exactly the same way as when remote debugging any other Java application. You only need to start javac using the usual debug switches, which must be prepended with -J in this case:\n $ javac -J-Xdebug \\ -J-Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8000 \\ HelloWorld.java   Make sure to download the right version of the OpenJDK source code and set it up in your IDE, so that you also can step through internal classes whose source code isn\u0026#8217;t distributed with binary builds. An interesting starting point for your explorations could be the JDKPlatformProvider class.\n     To double-check, you could also confirm with the API diffs provided by the Java Version Almanac or the Adopt OpenJDK JDK API diff generator. While doing so, one more thing piqued my curiosity: these reports don\u0026#8217;t show any changes to java.util.List in Java 11, whereas ct.sym contains a new version of the corresponding signature file; To find out what\u0026#8217;s going on, again javap\u0026#8201;\u0026#8212;\u0026#8201;this time with a bit more detail level\u0026#8201;\u0026#8212;\u0026#8201;came in handy:\n $ javap -p -c -s -v -l List10.class \u0026gt; List10.txt $ javap -p -c -s -v -l List11.class \u0026gt; List11.txt $ git diff --no-index -w List10.txt List11.txt ... - #96 = Utf8 RuntimeInvisibleAnnotations - #97 = Utf8 Ljdk/Profile+Annotation; - #98 = Utf8 value - #99 = Integer 1 { public abstract int size(); descriptor: ()I @@ -308,8 +304,3 @@ Constant pool: Signature: #87 // \u0026lt;E:Ljava/lang/Object;\u0026gt;(Ljava/util/Collection\u0026lt;+TE;\u0026gt;;)Ljava/util/List\u0026lt;TE;\u0026gt;; } Signature: #95 // \u0026lt;E:Ljava/lang/Object;\u0026gt;Ljava/lang/Object;Ljava/util/Collection\u0026lt;TE;\u0026gt;; -RuntimeInvisibleAnnotations: - 0: #97(#98=I#99) - jdk.Profile+Annotation( - value=1 - )   An annotation with the interesting name @jdk.Profile+Annotion(1) got removed. Now, if you look at the List.java source file in Java 10, you won\u0026#8217;t find this annotation anywhere. In fact, this annotation type doesn\u0026#8217;t exist at all. By grepping through the OpenJDK source code for ct.sym, I learned that it is a synthetic annotation which gets added during the process of creating the signature files, denoting which compact profile a class belongs to.\n     Compact Profiles Compact Profiles are a notion in Java 8 which defines three specific sub-sets of the Java platform: compact1, compact2, and compact3. Each profile contains a fixed set of JDK packages and build upon each other, allowing for more size-efficient deployments to constrained devices, if such profile is sufficient for a given application. With Java 9, the module system, and the ability to create custom runtime images on a much more granular level (using jlink), compact profiles became pretty much obsolete.\n     So that\u0026#8217;s another purpose of the ct.sym file: it allows the compiler to ensure compatibility with a chosen compact profile. In current JDKs, javac still supports the -profile option, but only when compiling for Java 8. In that light, it\u0026#8217;s not quite clear why that annotation only was removed from the signature file with Java 11.\n Summing up, since Java 9 the javac compiler provides powerful means of ensuring API compatibility with earlier Java versions. With a size of 7.2 MB for Java 16, the ct.sym file contains the JDK API signature versions all the way back to Java 7. Using the --release compiler option, backwards-compatible builds, fully portable, and without the need for actually installing earlier JDKs, are straight foward. With that tool in your box, there\u0026#8217;s really no need any longer for using the -source and -target options. Not only that, --release will also help to spot subtle compatibility issues related to overriding methods with co-variant return types, such as ByteBuffer.position().\n","id":25,"publicationdate":"Apr 26, 2021","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eOne of the ultimate strengths of Java is its strong notion of backwards compatibility:\nJava applications and libraries built many years ago oftentimes run without problems on current JVMs,\nand the compiler of current JDKs can produce byte code, that is executable with earlier Java versions.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eFor instance, JDK 16 supports byte code levels going back as far as to Java 1.7;\nBut: \u003cem\u003ehic sunt dracones\u003c/em\u003e.\nThe emitted byte code level is just one part of the story.\nIt\u0026#8217;s equally important to consider which APIs of the JDK are used by the compiled code,\nand whether they are available in the targeted Java runtime version.\nAs an example, let\u0026#8217;s consider this simple \"Hello World\" program:\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"The Anatomy of ct.sym — How javac Ensures Backwards Compatibility","uri":"https://www.morling.dev/blog/the-anatomy-of-ct-sym-how-javac-ensures-backwards-compatibility/"},{"content":"Java 16 is around the corner, so there\u0026#8217;s no better time than now for learning more about the features which the new version will bring. After exploring the support for Unix domain sockets a while ago, I\u0026#8217;ve lately been really curious about the incubating Vector API, as defined by JEP 338, developed under the umbrella of Project Panama, which aims at \"interconnecting JVM and native code\".\n Vectors?!? Of course this is not about renewing the ancient Java collection types like java.util.Vector (\u0026lt;insert some pun about this here\u0026gt;), but rather about an API which lets Java developers take advantage of the vector calculation capabilities you can find in most CPUs these days. Now I\u0026#8217;m by no means an expert on low-level programming leveraging specific CPU instructions, but exactly that\u0026#8217;s why I hope to make the case with this post that the new Vector API makes these capabilities approachable to a wide audience of Java programmers.\n What\u0026#8217;s SIMD Anyways? Before diving into a specific example, it\u0026#8217;s worth pointing out why that API is so interesting, and what it could be used for. In a nutshell, CPU architectures like x86 or AArch64 provide extensions to their instruction sets which allow you to apply a single operation to multiple data items at once (SIMD\u0026#8201;\u0026#8212;\u0026#8201;single instruction, multiple data). If a specific computing problem can be solved using an algorithm that lends itself to such parallelization, substantial performance improvements can be gained. Examples for such SIMD instruction set extensions include SSE and AVX for x64, and Neon of AArch64 (Arm).\n As such, they complement other means of compute parallelization: scaling out across multiple machines which collaborate in a cluster, and multi-threaded programming. Unlike these though, vectorized computations are done within the scope of an individual method, e.g. operating on multiple elements of an array at once.\n So far, there was no way for Java developers to directly work with such SIMD instructions. While you can use SIMD intrinsics in languages closer to the metal such as C/C++, no such option exists in Java so far. Note this doesn\u0026#8217;t mean Java wouldn\u0026#8217;t take advantage of SIMD at all: the JIT compiler can auto-vectorize code in specific situations, i.e. transforming code from a loop into vectorized code. Whether that\u0026#8217;s possible or not isn\u0026#8217;t easy to determine, though; small changes to a loop which the compiler was able to vectorize before, may lead to scalar execution, resulting in a performance regression.\n JEP 338 aims to improve this situation: introducing a portable vector computation API, it allows Java developers to benefit from SIMD execution by means of explicitly vectorized algorithms. Unlike C/C++ style intrinsics, this API will be mapped automatically by the C2 JIT compiler to the corresponding instruction set of the underlying platform, falling back to scalar execution if the platform doesn\u0026#8217;t provide the required capabilities. A pretty sweet deal, if you ask me!\n Now, why would you be interested in this? Doesn\u0026#8217;t \"vector calculation\" sound an awful lot like mathematics-heavy, low-level algorithms, which you don\u0026#8217;t tend to find that much in your typical Java enterprise applications? I\u0026#8217;d say, yes and no. Indeed it may not be that beneficial for say a CRUD application copying some data from left to right. But there are many interesting applications in areas like image processing, AI, parsing, (SIMD-based JSON parsing being a prominently discussed example), text processing, data type conversions, and many others. In that regard, I\u0026#8217;d expect that JEP 338 will pave the path for using Java in many interesting use cases, where it may not be the first choice today.\n   Vectorizing FizzBuzz To see how the Vector API can help with improving the performance of some calculation, let\u0026#8217;s consider FizzBuzz. Originally, FizzBuzz is a game to help teaching children division; but interestingly, it also serves as entry-level interview question for hiring software engineers in some places. In any case, it\u0026#8217;s a nice example for exploring how some calculation can benefit from vectorization. The rules of FizzBuzz are simple:\n   Numbers are counted and printed out: 1, 2, 3, \u0026#8230;\u0026#8203;\n  If a number if divisible by 3, instead of printing the number, print \"Fizz\"\n  If a number if divisible by 5, print \"Buzz\"\n  If a number if divisible by 3 and 5, print \"FizzBuzz\"\n   As the Vector API concerns itself with numeric values instead of strings, rather than \"Fizz\", \"Buzz\", and \"FizzBuzz\", we\u0026#8217;re going to emit -1, -2, and -3, respectively. The input of the program will be an array with the numbers from 1 \u0026#8230;\u0026#8203; 256, the output an array with the FizzBuzz sequence:\n 1, 2, -1, 4, -2, -1, 7, 8, -1, -2, 11, -1, 13, 14, -3, 16, ...   The task is easily solved using a plain for loop processing scalar values one by one:\n private static final int FIZZ = -1; private static final int BUZZ = -2; private static final int FIZZ_BUZZ = -3; public int[] scalarFizzBuzz(int[] values) { int[] result = new int[values.length]; for (int i = 0; i \u0026lt; values.length; i++) { int value = values[i]; if (value % 3 == 0) { if (value % 5 == 0) { (1) result[i] = FIZZ_BUZZ; } else { result[i] = FIZZ; (2) } } else if (value % 5 == 0) { result[i] = BUZZ; (3) } else { result[i] = value; (4) } } return result; }     1 The current number is divisible by 3 and 5: emit FIZZ_BUZZ (-3)   2 The current number is divisible by 3: emit FIZZ (-1)   3 The current number is divisible by 5: emit BUZZ (-2)   4 The current number is divisible by neither 3 nor 5: emit the number itself    As a baseline, this implementation can be executed ~2.2M times per second in a simple JMH benchmark running on my Macbook Pro 2019, with a 2.6 GHz 6-Core Intel Core i7 CPU:\n Benchmark (arrayLength) Mode Cnt Score Error Units FizzBuzzBenchmark.scalarFizzBuzz 256 thrpt 5 2204774,792 ± 76581,374 ops/s   Now let\u0026#8217;s see how this calculation could be vectorized and what performance improvements can be gained by doing so. When looking at the incubating Vector API, you may be overwhelmed at first by its large API surface. But it\u0026#8217;s becoming manageable once you realize that all the types like IntVector, LongVector, etc. essentially expose the same set of methods, only specific for each of the supported data types (and indeed, as per the JavaDoc, all these classes were not hand-written by some poor soul, but generated, from some sort of parameterized template supposedly).\n Amongst the plethora of API methods, there is no modulo operation, though (which makes sense, as for instance there isn\u0026#8217;t such instruction in any of the x86 SIMD extensions). So what could we do to solve the FizzBuzz task? After skimming through the API for some time, the method blend​(Vector\u0026lt;Integer\u0026gt; v, VectorMask\u0026lt;Integer\u0026gt; m) caught my attention:\n  Replaces selected lanes of this vector with corresponding lanes from a second input vector under the control of a mask. [\u0026#8230;\u0026#8203;]\n   For any lane set in the mask, the new lane value is taken from the second input vector, and replaces whatever value was in the that lane of this vector.\n  For any lane unset in the mask, the replacement is suppressed and this vector retains the original value stored in that lane.\n     This sounds pretty useful; The pattern of expected -1, -2, and -3 values repeats every 15 input values. So we can \"pre-calculate\" that pattern once and persist it in form of vectors and masks for the blend() method. While stepping through the input array, the right vector and mask are obtained based on the current position and are used with blend() in order to mark the values divisible by 3, 5, and 15 (another option could be min(Vector\u0026lt;Integer\u0026gt; v), but I decided against it, as we\u0026#8217;d need some magic value for representing those numbers which should be emitted as-is).\n Here is a visualization of the approach, assuming a vector length of eight elements (\"lanes\"):\n   So let\u0026#8217;s see how we can implement this using the Vector API. The mask and second input vector repeat every 120 elements (least common multiple of 8 and 15), so 15 masks and vectors need to be determined. They can be created like so:\n public class FizzBuzz { private static final VectorSpecies\u0026lt;Integer\u0026gt; SPECIES = IntVector.SPECIES_256; (1) private final List\u0026lt;VectorMask\u0026lt;Integer\u0026gt;\u0026gt; resultMasks = new ArrayList\u0026lt;\u0026gt;(15); private final IntVector[] resultVectors = new IntVector[15]; public FizzBuzz() { List\u0026lt;VectorMask\u0026lt;Integer\u0026gt;\u0026gt; threes = Arrays.asList( (2) VectorMask.\u0026lt;Integer\u0026gt;fromLong(SPECIES, 0b00100100), VectorMask.\u0026lt;Integer\u0026gt;fromLong(SPECIES, 0b01001001), VectorMask.\u0026lt;Integer\u0026gt;fromLong(SPECIES, 0b10010010) ); List\u0026lt;VectorMask\u0026lt;Integer\u0026gt;\u0026gt; fives = Arrays.asList( (3) VectorMask.\u0026lt;Integer\u0026gt;fromLong(SPECIES, 0b00010000), VectorMask.\u0026lt;Integer\u0026gt;fromLong(SPECIES, 0b01000010), VectorMask.\u0026lt;Integer\u0026gt;fromLong(SPECIES, 0b00001000), VectorMask.\u0026lt;Integer\u0026gt;fromLong(SPECIES, 0b00100001), VectorMask.\u0026lt;Integer\u0026gt;fromLong(SPECIES, 0b10000100) ); for(int i = 0; i \u0026lt; 15; i++) { (4) VectorMask\u0026lt;Integer\u0026gt; threeMask = threes.get(i%3); VectorMask\u0026lt;Integer\u0026gt; fiveMask = fives.get(i%5); resultMasks.add(threeMask.or(fiveMask)); (5) resultVectors[i] = IntVector.zero(SPECIES) (6) .blend(FIZZ, threeMask) .blend(BUZZ, fiveMask) .blend(FIZZ_BUZZ, threeMask.and(fiveMask)); } } }     1 A vector species describes the combination of an vector element type (in this case Integer) and a vector shape (in this case 256 bit); i.e. here we\u0026#8217;re going to deal with vectors that hold 8 32 bit int values   2 Vector masks describing the numbers divisible by three (read the bit values from right to left)   3 Vector masks describing the numbers divisible by five   4 Let\u0026#8217;s create the fifteen required result masks and vectors   5 A value in the output array should be set to another value if it\u0026#8217;s divisible by three or five   6 Set the value to -1, -2, or -3, depending on whether its divisible by three, five, or fifteen, respectively; otherwise set it to the corresponding value from the input array    With this infrastructure in place, we can implement the actual method for calculating the FizzBuzz values for an arbitrarily long input array:\n public int[] simdFizzBuzz(int[] values) { int[] result = new int[values.length]; int i = 0; int upperBound = SPECIES.loopBound(values.length); (1) for (; i \u0026lt; upperBound; i += SPECIES.length()) { (2) IntVector chunk = IntVector.fromArray(SPECIES, values, i); (3) int maskIdx = (i/SPECIES.length())%15; (4) IntVector fizzBuzz = chunk.blend(resultValues[maskIdx], resultMasks[maskIdx]); (5) fizzBuzz.intoArray(result, i); (6) } for (; i \u0026lt; values.length; i++) { (7) int value = values[i]; if (value % 3 == 0) { if (value % 5 == 0) { result[i] = FIZZ_BUZZ; } else { result[i] = FIZZ; } } else if (value % 5 == 0) { result[i] = BUZZ; } else { result[i] = value; } } return result; }     1 determine the maximum index in the array that\u0026#8217;s divisible by the species length; e.g. if the input array is 100 elements long, that\u0026#8217;d be 96 in the case of vectors with eight elements each   2 Iterate through the input array in steps of the vector length   3 Load the current chunk of the input array into an IntVector   4 Obtain the index of the right result vector and mask   5 Determine the FizzBuzz numbers for the current chunk (i.e. that\u0026#8217;s the actual SIMD instruction, processing all eight elements of the current chunk at once)   6 Copy the result values at the right index into the result array   7 Process any remainder (e.g. the last four remaining elements in case of an input array with 100 elements) using the traditional scalar approach, as those values couldn\u0026#8217;t fill up another vector instance    To reiterate what\u0026#8217;s happening here: instead of processing the values of the input array one by one, they are processed in chunks of eight elements each by means of the blend() vector operation, which can be mapped to an equivalent SIMD instruction of the CPU. In case the input array doesn\u0026#8217;t have a length that\u0026#8217;s a multiple of the vector length, the remainder is processed in the traditional scalar way. The resulting duplication of the logic seems a bit inelegant, we\u0026#8217;ll discuss in a bit what can be done about that.\n For now, let\u0026#8217;s see whether our efforts pay off; i.e. is this vectorized approach actually faster then the basic scalar implementation? Turns out it is! Here are the numbers I get from JMH on my machine, showing through-put increasing by factor 3:\n Benchmark (arrayLength) Mode Cnt Score Error Units FizzBuzzBenchmark.scalarFizzBuzz 256 thrpt 5 2204774,792 ± 76581,374 ops/s FizzBuzzBenchmark.simdFizzBuzz 256 thrpt 5 6748723,261 ± 34725,507 ops/s   Is there anything that could be further improved? I\u0026#8217;m pretty sure, but as said I\u0026#8217;m not an expert here, so I\u0026#8217;ll leave it to smarter folks to point out more efficient implementations in the comments. One thing I figured is that the division and modulo operation for obtaining the current mask index isn\u0026#8217;t ideal. Keeping a separate loop variable that\u0026#8217;s reset to 0 after reaching 15 proved to be quite a bit faster:\n public int[] simdFizzBuzz(int[] values) { int[] result = new int[values.length]; int i = 0; int j = 0; int upperBound = SPECIES.loopBound(values.length); for (; i \u0026lt; upperBound; i += SPECIES.length()) { IntVector chunk = IntVector.fromArray(SPECIES, values, i); IntVector fizzBuzz = chunk.blend(resultValues[j], resultMasks[j]); fizzBuzz.intoArray(result, i); j++; if (j == 15) { j = 0; } } // processing of remainder... }   Benchmark (arrayLength) Mode Cnt Score Error Units FizzBuzzBenchmark.scalarFizzBuzz 256 thrpt 5 2204774,792 ± 76581,374 ops/s FizzBuzzBenchmark.simdFizzBuzz 256 thrpt 5 6748723,261 ± 34725,507 ops/s FizzBuzzBenchmark.simdFizzBuzzSeparateMaskIndex 256 thrpt 5 8830433,250 ± 69955,161 ops/s   This makes for another nice improvement, yielding 4x the throughput of the original scalar implementation. Now, to make this a true apple-to-apple comparison, a mask-based approach can also be applied to the purely scalar implementation, only that each value needs to be looked up individually:\n private int[] serialMask = new int[] {0, 0, -1, 0, -2, -1, 0, 0, -1, -10, 0, -1, 0, 0, -3}; public int[] serialFizzBuzzMasked(int[] values) { int[] result = new int[values.length]; int j = 0; for (int i = 0; i \u0026lt; values.length; i++) { int res = serialMask[j]; result[i] = res == 0 ? values[i] : res; j++; if (j == 15) { j = 0; } } return result; }   Indeed, this implementation is quite a bit better than the original one, but still the SIMD-based approach is more than twice as fast:\n Benchmark (arrayLength) Mode Cnt Score Error Units FizzBuzzBenchmark.scalarFizzBuzz 256 thrpt 5 2204774,792 ± 76581,374 ops/s FizzBuzzBenchmark.scalarFizzBuzzMasked 256 thrpt 5 4156751,424 ± 23668,949 ops/s FizzBuzzBenchmark.simdFizzBuzz 256 thrpt 5 6748723,261 ± 34725,507 ops/s FizzBuzzBenchmark.simdFizzBuzzSeparateMaskIndex 256 thrpt 5 8830433,250 ± 69955,161 ops/s     Examining the Native Code This all is pretty cool, but can we trust that under the hood things actually happen the way we expect them to happen? In order to verify that, let\u0026#8217;s take a look at the native assembly code that gets produced by the JIT compiler for this implementation. This requires you to run the JVM with the hsdis plug-in; see this post for instructions on how to build and install hsdis. Let\u0026#8217;s create a simple main class which executes the method in question in a loop, so to make sure the method actually gets JIT-compiled:\n public class Main { public static int[] blackhole; public static void main(String[] args) { FizzBuzz fizzBuzz = new FizzBuzz(); var values = IntStream.range(1, 257).toArray(); for(int i = 0; i \u0026lt; 5_000_000; i++) { blackhole = fizzBuzz.simdFizzBuzz(values); } } }   Run the program, enabling the output of the assembly, and piping its output into a log file:\n java -XX:+UnlockDiagnosticVMOptions \\ -XX:+PrintAssembly -XX:+LogCompilation \\ --add-modules=jdk.incubator.vector \\ --class-path target/classes \\ dev.morling.demos.simdfizzbuzz.Main \u0026gt; fizzbuzz.log   Open the fizzbuzz.log file and look for the C2-compiled nmethod block of the simdFizzBuzz method. Somewhere within the method\u0026#8217;s native code, you should find the vpblendvb instruction (output slightly adjusted for better readability):\n ... =========================== C2-compiled nmethod ============================ --------------------------------- Assembly --------------------------------- Compiled method (c2) ... dev.morling.demos.simdfizzbuzz.FizzBuzz:: ↩ simdFizzBuzz (161 bytes) ... 0x000000011895e18d: vpmovsxbd %xmm7,%ymm7 ↩ ;*invokestatic store {reexecute=0 rethrow=0 return_oop=0} ; - jdk.incubator.vector.IntVector::intoArray@42 (line 2962) ; - dev.morling.demos.simdfizzbuzz.FizzBuzz::simdFizzBuzz@76 (line 92) 0x000000011895e192: vpblendvb %ymm7,%ymm5,%ymm8,%ymm0 ↩ ;*invokestatic blend {reexecute=0 rethrow=0 return_oop=0} ; - jdk.incubator.vector.IntVector::blendTemplate@26 (line 1895) ; - jdk.incubator.vector.Int256Vector::blend@11 (line 376) ; - jdk.incubator.vector.Int256Vector::blend@3 (line 41) ; - dev.morling.demos.simdfizzbuzz.FizzBuzz::simdFizzBuzz@67 (line 91) ...   vpblendvb is part of the x86 AVX2 instruction set and \"conditionally copies byte elements from the source operand (second operand) to the destination operand (first operand) depending on mask bits defined in the implicit third register argument\", as such exactly corresponding to the blend() method in the JEP 338 API.\n One detail not quite clear to me is why vpmovsxbd for copying the results into the output array (the intoArray() call) shows up before vpblendvb. If you happen to know the reason for this, I\u0026#8217;d love to hear from you and learn about this.\n   Avoiding Scalar Processing of Tail Elements Let\u0026#8217;s get back to the scalar processing of the potential remainder of the input array. This feels a bit \"un-DRY\", as it requires the algorithm to be implemented twice, once vectorized and once in a scalar way.\n The Vector API recognizes the desire for avoiding this duplication and provides masked versions of all the required operations, so that during the last iteration no access beyond the array length will happen. Using this approach, the SIMD FizzBuzz method looks like this:\n public int[] simdFizzBuzzMasked(int[] values) { int[] result = new int[values.length]; int j = 0; for (int i = 0; i \u0026lt; values.length; i += SPECIES.length()) { var mask = SPECIES.indexInRange(i, values.length); (1) var chunk = IntVector.fromArray(SPECIES, values, i, mask); (2) var fizzBuzz = chunk.blend(resultValues[j], resultMasks.get(j)); fizzBuzz.intoArray(result, i, mask); (2) j++; if (j == 15) { j = 0; } } return result; }     1 Obtain a mask which, during the last iteration, will have bits for those lanes unset, which are larger than the last encountered multiple of the vector length   2 Perform the same operations as above, but using the mask to prevent any access beyond the array length    The implementation looks quite a bit nicer than the version with the explicit scalar processing of the remainder portion. But the impact on throughput is significant, the result is quite a disappointing:\n Benchmark (arrayLength) Mode Cnt Score Error Units FizzBuzzBenchmark.scalarFizzBuzz 256 thrpt 5 2204774,792 ± 76581,374 ops/s FizzBuzzBenchmark.scalarFizzBuzzMasked 256 thrpt 5 4156751,424 ± 23668,949 ops/s FizzBuzzBenchmark.simdFizzBuzz 256 thrpt 5 6748723,261 ± 34725,507 ops/s FizzBuzzBenchmark.simdFizzBuzzSeparateMaskIndex 256 thrpt 5 8830433,250 ± 69955,161 ops/s FizzBuzzBenchmark.simdFizzBuzzMasked 256 thrpt 5 1204128,029 ± 5556,553 ops/s   In its current form, this approach is even slower than the pure scalar implementation. It remains to be seen whether and how performance gets improved here, as the Vector API matures. Ideally, the mask would have to be only applied during the very last iteration. This is something we either could do ourselves\u0026#8201;\u0026#8212;\u0026#8201;re-introducing some special remainder handling, albeit less different from the core implementation than with the pure scalar approach discussed above\u0026#8201;\u0026#8212;\u0026#8201;or perhaps even the compiler itself may be able to apply such transformation.\n One important take-away from this is that a SIMD-based approach does not necessarily have to be faster than a scalar one. So every algorithmic adjustment should be validated with a corresponding benchmark, before drawing any conclusions. Speaking of which, I also ran the benchmark on that shiny new Mac Mini M1 (i.e. an AArch64-based machine) that found its way to my desk recently, and numbers are, mh, interesting:\n Benchmark (arrayLength) Mode Cnt Score Error Units FizzBuzzBenchmark.scalarFizzBuzz 256 thrpt 5 2717990,097 ± 4203,628 ops/s FizzBuzzBenchmark.scalarFizzBuzzMasked 256 thrpt 5 5750402,582 ± 2479,462 ops/s FizzBuzzBenchmark.simdFizzBuzz 256 thrpt 5 1297631,404 ± 15613,288 ops/s FizzBuzzBenchmark.simdFizzBuzzMasked 256 thrpt 5 374313,033 ± 2219,940 ops/s FizzBuzzBenchmark.simdFizzBuzzMasksInArray 256 thrpt 5 1316375,073 ± 1178,704 ops/s FizzBuzzBenchmark.simdFizzBuzzSeparateMaskIndex 256 thrpt 5 998979,324 ± 69997,361 ops/s   The scalar implementation on the M1 out-performs the x86 MacBook Pro by quite a bit, but SIMD numbers are significantly lower.\n I haven\u0026#8217;t checked the assembly code, but solely based on the figures, my guess is that the JEP 338 implementation in the current JDK 16 builds does not yet support AArch64, and the API falls back to scalar execution.\n Here it would be nice to have some method in the API which reveals whether SIMD support is provided by the current platform or not, as e.g. done by .NET with its Vector.IsHardwareAccelerated() method.\n Update, March 9th: After asking about this on the panama-dev mailing list, Ningsheng Jian from Arm explained that the AArch64 NEON instruction set has a maximum hardware vector size of 128 bits; hence the Vector API is transparently falling back to the Java implementation in our case of using 256 bits. By passing the -XX:+PrintIntrinsics flag you can inspect which API calls get intrinsified (i.e. executed via corresponding hardware instructions) and which ones not. When running the main class from above with this option, we get the relevant information (output slightly adjusted for better readability):\n @ 31 jdk.internal.vm.vector.VectorSupport::load (38 bytes) ↩ failed to inline (intrinsic) ... @ 26 jdk.internal.vm.vector.VectorSupport::blend (38 bytes) ↩ failed to inline (intrinsic) ... @ 42 jdk.internal.vm.vector.VectorSupport::store (38 bytes) ↩ failed to inline (intrinsic) ** not supported: arity=0 op=load vlen=8 etype=int ismask=no ** not supported: arity=2 op=blend vlen=8 etype=int ismask=useload ** not supported: arity=1 op=store vlen=8 etype=int ismask=no   Fun fact: during the entire benchmark runtime of 10 min the fan of the Mac Mini was barely to hear, if at all. Definitely a very exciting platform, and I\u0026#8217;m looking forward to doing more Java experiments on it soon.\n   Wrap-Up Am I suggesting you should go and implement your next FizzBuzz using SIMD? Of course not, FizzBuzz just served as an example here for exploring how a well-known \"problem\" can be solved more efficiently via the new Java Vector API (at the cost of increased complexity in the code), also without being a seasoned systems programmer. On the other hand, it may make an impression during your next job interview ;)\n If you want to get started with your own experiments around the Vector API and SIMD, install a current JDK 16 RC (release candidate) build and grab the SIMD FizzBuzz example from this GitHub repo. A nice twist to explore would for instance be using ShortVector instead of IntVector (allowing to put 16 values into 256-bit vector), running the benchmark on machines with the AVX-512 extension (e.g. via the C5 instance type on AWS EC2), or both :)\n Apart from the JEP document itself, there isn\u0026#8217;t too much info out yet about the Vector API; a great starting point are the \"vector\" tagged posts on the blog of Richard Startin. Another inspirational resource is August Nagro\u0026#8217;s project for vectorized UTF-8 validation based on a paper by John Keiser and Daniel Lemire. Kishor Kharbas and Paul Sandoz did a talk about the Vector API at CodeOne a while ago.\n Taking a step back, it\u0026#8217;s hard to overstate the impact which the Vector API potentially will have on the Java platform. Providing SIMD capabilities in a rather easy-to-use, portable way, without having to rely on CPU instruction set specific intrinsics, may result in nothing less than a \"democratization of SIMD\", making these powerful means of parallelizing computations available to a much larger developer audience.\n Also the JDK class library itself may benefit from the Vector API; while JDK authors\u0026#8201;\u0026#8212;\u0026#8201;unlike Java application developers\u0026#8201;\u0026#8212;\u0026#8201;already have the JVM intrinsics mechanism at their disposal, the new API will \"make prototyping easier, and broaden what might be economical to consider\", as pointed out by Claes Redestad.\n But nothing in life is free, and code will have to be restructured or even re-written in order to benefit from this. Some problems lend themselves better than others to SIMD-style processing, and only time will tell in which areas the new API will be adopted. As said above, use cases like image processing and AI can benefit from SIMD a lot, due to the nature of the underlying calculations. Also specific data store operations can be sped up significantly using SIMD instructions; so my personal hope is that the Vector API can contribute to making Java an attractive choice for such applications, which previously were not considered a sweet spot for the Java platform.\n As such, I can\u0026#8217;t think of many recent Java API additions which may prove as influential as the Vector API.\n  ","id":26,"publicationdate":"Mar 8, 2021","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eJava 16 is around the corner, so there\u0026#8217;s no better time than now for learning more about the features which the new version will bring.\nAfter exploring the support for \u003ca href=\"/blog/talking-to-postgres-through-java-16-unix-domain-socket-channels/\"\u003eUnix domain sockets\u003c/a\u003e a while ago,\nI\u0026#8217;ve lately been really curious about the incubating Vector API,\nas defined by \u003ca href=\"https://openjdk.java.net/jeps/338\"\u003eJEP 338\u003c/a\u003e,\ndeveloped under the umbrella of \u003ca href=\"https://openjdk.java.net/projects/panama/\"\u003eProject Panama\u003c/a\u003e,\nwhich aims at \"interconnecting JVM and native code\".\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003e\u003cem\u003eVectors?!?\u003c/em\u003e\nOf course this is not about renewing the ancient Java collection types like \u003ccode\u003ejava.util.Vector\u003c/code\u003e\n(\u0026lt;insert some pun about this here\u0026gt;),\nbut rather about an API which lets Java developers take advantage of the vector calculation capabilities you can find in most CPUs these days.\nNow I\u0026#8217;m by no means an expert on low-level programming leveraging specific CPU instructions,\nbut exactly that\u0026#8217;s why I hope to make the case with this post that the new Vector API makes these capabilities approachable to a wide audience of Java programmers.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"FizzBuzz – SIMD Style!","uri":"https://www.morling.dev/blog/fizzbuzz-simd-style/"},{"content":"Update Feb 5: This post is discussed on Hacker News\n Reading a blog post about what\u0026#8217;s coming up in JDK 16 recently, I learned that one of the new features is support for Unix domain sockets (JEP 380). Before Java 16, you\u0026#8217;d have to resort to 3rd party libraries like jnr-unixsocket in order to use them. If you haven\u0026#8217;t heard about Unix domain sockets before, they are \"data communications [endpoints] for exchanging data between processes executing on the same host operating system\". Don\u0026#8217;t be put off by the name btw.; Unix domain sockets are also supported by macOS and even Windows since version 10.\n Databases such as Postgres or MySQL use them for offering an alternative to TCP/IP-based connections to client applications running on the same machine as the database. In such scenario, Unix domain sockets are both more secure (no remote access to the database is exposed at all; file system permissions can be used for access control), and also more efficient than TCP/IP loopback connections.\n A common use case are proxies for accessing Cloud-based databases, such as as the GCP Cloud SQL Proxy. Running on the same machine as a client application (e.g. in a sidecar container in case of Kubernetes deployments), they provide secure access to a managed database, for instance taking care of the SSL handling.\n My curiousity was piqued and I was wondering what it\u0026#8217;d take to make use of the new Java 16 Unix domain socket for connecting to Postgres. It was your regular evening during the pandemic, without much to do, so I thought \"Let\u0026#8217;s give this a try\". To have a testing bed, I started with installing Postgres 13 on Fedora 33. Fedora might not always have the latest Postgres version packaged just yet, but following the official Postgres instructions it is straight-forward to install newer versions.\n In order to connect with user name and password via a Unix domain socket, one small adjustment to /var/lib/pgsql/13/data/pg_hba.conf is needed: the access method for the local connection type must be switched from the default value peer (which would try to authenticate using the operating system user name of the client process) to md5.\n ... # TYPE DATABASE USER ADDRESS METHOD # \"local\" is for Unix domain socket connections only local all all md5 ...   Make sure to apply the changed configuration by restarting the database (systemctl restart postgresql-13), and things are ready to go.\n The Postgres JDBC Driver The first thing I looked into was the Postgres JDBC driver. Since version 9.4-1208 (released in 2016) it allows you to configure custom socket factories, a feature which explicitly was added considering Unix domain sockets. The driver itself doesn\u0026#8217;t come with a socket factory implementation that\u0026#8217;d actually support Unix domain sockets, but a few external open-source implementations exist. Most notably junixsocket provides such socket factory.\n Custom socket factories must extend javax.net.SocketFactory, and their fully-qualified class name needs to be specified using the socketFactory driver parameter. So it should be easy to create SocketFactory implementation based on the new UnixDomainSocketAddress class, right?\n public class PostgresUnixDomainSocketFactory extends SocketFactory { @Override public Socket createSocket() throws IOException { var socket = new Socket(); socket.connect(UnixDomainSocketAddress.of( \"/var/run/postgresql/.s.PGSQL.5432\")); (1) return socket; } // other create methods ... }     1 Create a Unix domain socket address for the default path of the socket on Fedora and related systems    It compiles just fine; but it turns out not all socket addresses are equal, and java.net.Socket only connects to addresses of type InetSocketAddress (and the PG driver maintainers seem to sense some air of mystery around these \"unusual\" events, too):\n org.postgresql.util.PSQLException: Something unusual has occurred to cause the driver to fail. Please report this exception. at org.postgresql.Driver.connect(Driver.java:285) ... Caused by: java.lang.IllegalArgumentException: Unsupported address type at java.base/java.net.Socket.connect(Socket.java:629) at java.base/java.net.Socket.connect(Socket.java:595) at dev.morling.demos.PostgresUnixDomainSocketFactory.createSocket(PostgresUnixDomainSocketFactory.java:19) ...   Now JEP 380 solely speaks about SocketChannel and stays silent about Socket; but perhaps obtaining a socket from a domain socket channel works?\n public Socket createSocket() throws IOException { var sc = SocketChannel.open(UnixDomainSocketAddress.of( \"/var/run/postgresql/.s.PGSQL.5432\")); return sc.socket(); }   Nope, no luck either:\n java.lang.UnsupportedOperationException: Not supported at java.base/sun.nio.ch.SocketChannelImpl.socket(SocketChannelImpl.java:226) at dev.morling.demos.PostgresUnixDomainSocketFactory.createSocket(PostgresUnixDomainSocketFactory.java:17)   Indeed it looks like JEP 380 is concerning itself only with the non-blocking SocketChannel API, while users of the blocking Socket API do not get to benefit from it. It should be possible to create a custom Socket implementation based on the socket channel support of JEP 380, but that\u0026#8217;s going beyond the scope of my little exploration.\n   The Vert.x Postgres Client If the Postgres JDBC driver doesn\u0026#8217;t easily benefit from the JEP, what about other Java Postgres clients then? There are several non-blocking options, including the Vert.x Postgres client and R2DBC. The former is used to bring Reactive capabilities for Postgres into the Quarkus stack, too, so I turned my attention to it.\n Now the Vert.x Postgres Client already has support for Unix domain sockets, by means of adding the right Netty native transport dependency to your project. So purely from functionality perspective, there\u0026#8217;s not that much to be gained here. But being able to use domain sockets also with the default NIO transport would still be nice, as it means one less dependency to take care of. So I dug a bit into the code of the Postgres client and Vert.x itself and figured out, that two things needed adjustment:\n   The NIO-based Transport class of Vert.x needs to learn about the fact that SocketChannel now also supports Unix domain sockets (currently, an exception is raised when trying to use them without a Netty native transport)\n  Netty\u0026#8217;s NioSocketChannel needs some small changes, as it tries to obtain a Socket from the underlying SocketChannel, which doesn\u0026#8217;t work for domain sockets as we\u0026#8217;ve seen above\n   Step 1 was quickly done by creating a custom sub-class of the default Transport class. Two methods needed changes: channelFactory() for obtaining a factory for the actual Netty transport channel, and convert() for converting a Vert.x SocketAddress into a NIO one:\n public class UnixDomainTransport extends Transport { @Override public ChannelFactory\u0026lt;? extends Channel\u0026gt; channelFactory( boolean domainSocket) { if (!domainSocket) { (1) return super.channelFactory(domainSocket); } else { return () -\u0026gt; { try { var sc = SocketChannel.open(StandardProtocolFamily.UNIX); (2) return new UnixDomainSocketChannel(null, sc); } catch(Exception e) { throw new RuntimeException(e); } }; } } @Override public SocketAddress convert(io.vertx.core.net.SocketAddress address) { if (!address.isDomainSocket()) { (3) return super.convert(address); } else { return UnixDomainSocketAddress.of(address.path()); (4) } } }     1 Delegate creation of non domain socket factories to the regular NIO transport implementation   2 This channel factory returns instances of our own UnixDomainSocketChannel type (see below), passing a socket channel based on the new UNIX protocol family   3 Delegate conversion of non domain socket addresses to the regular NIO transport implementation   4 Create a UnixDomainSocketAddress for the socket\u0026#8217;s file system path    Now let\u0026#8217;s take a look at the UnixDomainSocketChannel class. I was hoping to get away again with creating a sub-class of the NIO-based implementation, io.netty.channel.socket.nio.NioSocketChannel in this case. Unfortunately, though, the NioSocketChannel constructor invokes the taboo SocketChannel#socket() method. Of course that\u0026#8217;d not be a problem when doing this change in Netty itself, but for my little exploration I ended up copying the class and doing the required adjustments in that copy. I ended up doing two small changes:\n   Avoiding the call to SocketChannel#socket() in the constructor:\npublic UnixDomainSocketChannel(Channel parent, SocketChannel socket) { super(parent, socket); config = new NioSocketChannelConfig(this, new Socket()); (1) }     1 Passing a dummy socket instead of socket.socket(), it shouldn\u0026#8217;t be accessed in our case anyways      A few methods call the Socket methods isInputShutdown() and isOutputShutdown(); those should be possible to be by-passed by keeping track of the two shutdown flags ourselves\n  As I was creating the UnixDomainSocketChannel in my own namespace instead of Netty\u0026#8217;s packages, a few references to the non-public method NioChannelOption#getOptions() needed commenting out, which again shouldn\u0026#8217;t be relevant for the domain socket case\n   You can find the complete change in this commit. All in all, not exactly an artisanal piece of software engineering, but the little hack seemed good enough at least for taking a quick glimpse at the new domain socket support. Of course a real implementation could be done much more properly within the Netty project itself.\n So it was time to give this thing a test ride. As we need to configure the custom Transport implementation, retrieval of a PgPool instance is a tad more verbose than usual:\n PgConnectOptions connectOptions = new PgConnectOptions() .setPort(5432) (1) .setHost(\"/var/run/postgresql\") .setDatabase(\"test_db\") .setUser(\"test_user\") .setPassword(\"topsecret!\"); PoolOptions poolOptions = new PoolOptions() .setMaxSize(5); VertxFactory fv = new VertxFactory(); fv.transport(new UnixDomainTransport()); (2) Vertx v = fv.vertx(); PgPool client = PgPool.pool(v, connectOptions, poolOptions); (3)     1 The Vert.x Postgres client constructs the domain socket path from the given port and path (via setHost()); the full path will be /var/run/postgresql/.s.PGSQL.5432, just as above   2 Construct a Vertx instance with the custom transport class   3 Obtain a PgPool instance using the customized Vertx instance    We then can can use the client instance as usual, only that it now will connect to Postgres using the domain socket instead of via TCP/IP. All this solely using the default NIO-based transports, without the need for adding any Netty native dependency, such as its epoll-based transport.\n I haven\u0026#8217;t done any real performance benchmark at this point; in a quick ad-hoc test of executing a trivial SELECT query on a primay key 200,000 times, I observed a latency of ~0.11 ms when using Unix domain sockets\u0026#8201;\u0026#8212;\u0026#8201;with both, netty-transport-native-epoll and JDK 16 Unix domain sockets\u0026#8201;\u0026#8212;\u0026#8201;and ~0.13 ms when connecting via TCP/IP. So definitely a significant improvement which can be a deciding factor for low-latency use cases, though in comparison to other reports, the latency reduction of ~15% appears to be at the lower end of the spectrum.\n Some more sincere performance evaluation should be done, for instance also examining the impact on garbage collection. And it goes without saying that you should only trust your own measurements, on your own hardware, based on your specific workloads, in order to decide whether you would benefit from domain sockets or not.\n   Other Use Cases Database connectivity is just one of the use cases for domain sockets; highly performant local inter-process communication comes in handy for all kinds of use cases. One which I find particularly intriguing is the creation of modular applications based on a multi-process architecture.\n When thinking of classic Java Jakarta EE application servers for instance, you could envision a model where both the application server and each deployment are separate processes, communicating through domain sockets. This would have some interesting advantages, such as stricter isolation (so for instance an OutOfMemoryError in one deployed application won\u0026#8217;t impact others) and re-deployments without any risk of classloader leaks, as the JVM of an deployment would be restarted. On the downside, you\u0026#8217;d be facing a higher overall memory consumption (although that can at least partly be mitigated through class data sharing, which also works across JVM boundaries) and more costly (remote) method invocations between deployments.\n Now the application server model has fallen out of favour for various reasons, but such multi-process design still is very interesting, for instance for building modular applications that should expose a single web endpoint, while being assembled from a set of processes which are developed and deployed by several, independent teams. Another use case would be desktop applications that are made up of a set of processes for isolation purposes, as it\u0026#8217;s e.g. done by most web browsers noawadays with distinct processes for separate tabs. JEP 380 should facilitate this model when creating Java applications, e.g. considering rich clients built with JavaFX.\n Another, really interesting feature of Unix domain sockets is the ability to transfer open file descriptors from one process to another. This allows for non-disruptive upgrades of server applications, without dropping any open TCP connections. This technique is used for instance by Envoy Proxy for applying configuration changes: upon a configuration change, a second Envoy instance with the new configuration is started up, takes over the active sockets from the previous instance and after some \"draining period\" triggers a shutdown of the old instance. This approach enables a truly immutable application design within Envoy itself, with all its advantages, without the need for in-process configuration reloads. I highly recommend to read the two posts linked above, they are super-interesting.\n Unfortunately, JEP 380 doesn\u0026#8217;t seem to support file descriptor transfers. So for this kind of architecture, you\u0026#8217;d still have to refrain to the aforementioned junixsocket library, which explicitly lists file transcriptor transfer support as one of its features. While you couldn\u0026#8217;t take advantage of that using Java\u0026#8217;s NIO API, it should be doable using alternative networking frameworks such as Netty. Probably a topic for another blog post on another one of those pandemic weekends ;)\n And that completes my small exploration of Java 16\u0026#8217;s support for Unix domain sockets. If you want to do your own experiments of using them to connect to Postgres, make sure to install the latest JDK 16 EA build and grab the source code of my experimentation from this GitHub repo.\n It\u0026#8217;d be my hope that frameworks like Netty and Vert.x make use of this JDK feature fairly quickly, as only a small amount of code changes is required, and users get to benefit from the higher performance of domain sockets without having to pull in any additional dependencies. In order to keep compatibility with Java versions prior to 16, multi-release JARs offer one avenue for integrating this feature.\n  ","id":27,"publicationdate":"Jan 31, 2021","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003e\u003cem\u003eUpdate Feb 5: This post is \u003ca href=\"https://news.ycombinator.com/item?id=26012466\"\u003ediscussed on Hacker News\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eReading a blog post about what\u0026#8217;s \u003ca href=\"https://www.loicmathieu.fr/wordpress/en/informatique/java-16-quoi-de-neuf/\"\u003ecoming up in JDK 16\u003c/a\u003e recently,\nI learned that one of the new features is support for Unix domain sockets (\u003ca href=\"https://openjdk.java.net/jeps/380\"\u003eJEP 380\u003c/a\u003e).\nBefore Java 16, you\u0026#8217;d have to resort to 3rd party libraries like \u003ca href=\"https://github.com/jnr/jnr-unixsocket\"\u003ejnr-unixsocket\u003c/a\u003e in order to use them.\nIf you haven\u0026#8217;t heard about \u003ca href=\"https://en.wikipedia.org/wiki/Unix_domain_socket\"\u003eUnix domain sockets\u003c/a\u003e before,\nthey are \"data communications [endpoints] for exchanging data between processes executing on the same host operating system\".\nDon\u0026#8217;t be put off by the name btw.;\nUnix domain sockets are also supported by macOS and even Windows since \u003ca href=\"https://devblogs.microsoft.com/commandline/af_unix-comes-to-windows/\"\u003eversion 10\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Talking to Postgres Through Java 16 Unix-Domain Socket Channels","uri":"https://www.morling.dev/blog/talking-to-postgres-through-java-16-unix-domain-socket-channels/"},{"content":"Discussions around Java\u0026#8217;s jlink tool typically center around savings in terms of (disk) space. Instead of shipping an entire JDK, a custom runtime image created with jlink contains only those JDK modules which an application actually requires, resulting in smaller distributables and container images.\n But the contribution of jlink\u0026#8201;\u0026#8212;\u0026#8201;as a part of the Java module system at large\u0026#8201;\u0026#8212;\u0026#8201;to the development of Java application\u0026#8217;s is bigger than that: with the notion of link time it defines an optional complement to the well known phases compile time and application run-time:\n  Link time is an opportunity to do whole-world optimizations that are otherwise difficult at compile time or costly at run-time. An example would be to optimize a computation when all its inputs become constant (i.e., not unknown). A follow-up optimization would be to remove code that is no longer reachable.\n   Other examples for link time optimizations are the removal of unnecessary classes and resources, the conversion of (XML-based) deployment descriptors into binary representations (which will be more efficiently processable at run-time), obfuscation, or the generation of annotation indexes. It would also be very interesting to create AppCDS archives for all the classes of a runtime image at link time and bake that archive into the image, resulting in faster application start-up, without any further manual configuration needed.\n While these use cases mostly relate to optimization of the runtime image in one way or another, the link time phase also is beneficial for the validation of applications. In the remainder of this post, I\u0026#8217;d like to discuss how link time validation can be employed to ensure the consistency of API signatures within a modularized Java application. This helps to avoid potential NoSuchMethodErrors and related errors which would otherwise be raised by the JVM at application run-time, stemming from the usage of incompatible module versions, different from the ones used at compile time.\n The Example To make things more tangible, let\u0026#8217;s look at an application made up of two modules, customer and order. As always, the full source code is available online, for you to play with. The customer module defines a service interface with the following signature:\n 1 2 3 public interface CustomerService { void incrementLoyaltyPoints(long customerId, long orderValue); }    The CustomerService interface is part of the customer module\u0026#8217;s public API and is invoked from within the order module like so:\n 1 2 3 4 5 6 7 public class OrderService { public static void main(String[] args) { CustomerService customerService = ...; customerService.incrementLoyaltyPoints(123, 4999); } }    Now let\u0026#8217;s assume there\u0026#8217;s a new version of the customer module; the signature of the incrementLoyaltyPoints() method got slightly changed for the sake of a more expressive and type-safe API:\n 1 2 3 4 5 // record CustomerId(long id) {} public interface CustomerService { void incrementLoyaltyPoints(CustomerId customerId, long orderValue); }    We now create a custom runtime image for the application. But we\u0026#8217;re at the end of a tough week, so accidentally we add version 2 of the customer module and the unchanged order module:\n 1 2 3 4 $ $JAVA_HOME/bin/jlink \\ --module-path=path/to/customer-2.0.0.jar:path/to/order-1.0.0.jar \\ --add-modules=com.example.order \\ --output=target/runtime-image    Note that jlink won\u0026#8217;t complain about this and create the runtime image. When executing the application via the image we\u0026#8217;re in for a bad surprise, though (slightly modified for the sake of readability):\n 1 2 3 4 5 $ ./target/runtime-image/bin/java com.example.order.OrderService Exception in thread \"main\" java.lang.NoSuchMethodError: 'void c.e.customer.CustomerService.incrementLoyaltyPoints(long, long)' at com.example.order@1.0.0/c.e.order.OrderService.main(OrderService.java:5)    This might be surprising at first; while jlink and the module system in general put a strong emphasis on reliability and e.g. flag referenced yet missing modules, mismatching API signatures like this are not raised as an issue and will only show up as an error at application run-time.\n Indeed, when I did a quick non-representative poll about this on Twitter, it turned out that more than 40% of participants were not aware of this pitfall:\n    Needless to say that it\u0026#8217;d be much more desirable to spot this error already early on at link time, before shipping the affected application to production, and suffering from all the negative consequences associated to that.\n   The API Signature Check jlink Plug-in While jlink doesn\u0026#8217;t detect this kind of API signature mismatch by itself, it comes with a plug-in API, which allows to hook into and enrich the linking process. By creating a custom jlink plug-in, we can implement the API signature check and fail the image creation process when detecting any invalid method references like the one above.\n Unfortunately though, the plug-in mechanism isn\u0026#8217;t an official, supported API at this point. As a matter of fact, it is not even exported within jlink\u0026#8217;s own module definition. With the right set of javac/java flags and the help of a small Java agent, it is possible though to compile custom plug-ins and have them picked up by jlink. To learn more about the required sorcery, check out this blog post which I wrote a while ago over on the Hibernate team blog.\n Let\u0026#8217;s start with creating the basic structure of the plug-in implementation class:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import jdk.tools.jlink.plugin.Plugin; public class SignatureCheckPlugin implements Plugin { @Override public String getName() { (1) return \"check-signatures\"; } @Override public Category getType() { (2) return Category.VERIFIER; } @Override public String getDescription() { (3) return \"Checks the API references amongst the modules of \" + \"an application for consistency\"; } }      1 Returns the name for the option to enable this plug-in when running the jlink command   2 Returns the category of this plug-in, which impacts the ordering within the plug-in stack (other types include TRANSFORMER, FILTER, etc.)   3 A description which will be shown when listing all plug-ins    There are a few more optional methods which we could implement, e.g. if the plug-in had any parameters for controlling its behaviors, or if we wanted it to be enabled by default. But as that\u0026#8217;s not the case for the plug-in at hand, the only method that\u0026#8217;s missing is transform(), which does the actual heavy-lifting of the plug-in\u0026#8217;s work.\n Now implementing the complete rule set of the JVM applied when loading and linking classes at run-time would be a somewhat daunting task. As I am lazy and this is just meant to be a basic PoC, I\u0026#8217;m going to limit myself to the detection of mismatching signatures of invoked methods, as shown in the customer/order example above. The reason being that this task can be elegantly delegated to an existing tool (I told you, I\u0026#8217;m lazy): Animal Sniffer.\n While typically used as build tool plug-in for verifying that classes built on a newer JDK version can also be executed with older Java versions (and as such mostly obsoleted by the JDK\u0026#8217;s --release option), Animal Sniffer also provides an API for creating and verifying custom signatures. This comes in handy for our jlink plug-in implementation.\n The general design of the transform() mechanism is that of a classic input-process-output pipeline. The method receives a ResourcePool object, which allows to traverse and examine the set of resources going into the image, such as class files, resource bundles, or manifests. A new resource pool is to be returned, which could contain exactly the same resources as the original one (as in our case); but of course it could also contain less or newly generated resources, or modified ones:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 @Override public ResourcePool transform(ResourcePool in, ResourcePoolBuilder out) { try { byte[] signature = createSignature(in); (1) boolean broken = checkSignature(in, signature); (2) if (broken) { (3) throw new PluginException(\"There are API signature \" + \"inconsistencies, please check the logs\"); } } catch(PluginException e) { throw e; } catch(Exception e) { throw new RuntimeException(e); } in.transformAndCopy(e -\u0026gt; e, out); (4) return out.build(); } /** * Creates a signature for all classes in the resource pool. */ private byte[] createSignature(ResourcePool in) throws IOException { ByteArrayOutputStream signatureStream = new ByteArrayOutputStream(); var builder = new StreamSignatureBuilder(signatureStream, new PrintWriterLogger(System.out)); in.entries() (5) .filter(e -\u0026gt; isClassFile(e) \u0026amp;\u0026amp; !isModuleInfo(e)) .forEach(e -\u0026gt; builder.process(e.path(), e.content())); builder.close(); return signatureStream.toByteArray(); } /** * Checks all classes against the given signature. */ private boolean checkSignature(ResourcePool in, byte[] signature) throws IOException { var checker = new StreamSignatureChecker( new ByteArrayInputStream(signature), Collections.\u0026lt;String\u0026gt;emptySet(), new PrintWriterLogger(System.out) ); checker.setSourcePath(Collections.\u0026lt;File\u0026gt;emptyList()); in.entries() (6) .filter(e -\u0026gt; isClassFile(e) \u0026amp;\u0026amp; !isModuleInfo(e) \u0026amp;\u0026amp; !isJdkClass(e)) .forEach(e -\u0026gt; checker.process(e.path(), e.content())); return checker.isSignatureBroken(); } private boolean isJdkClass(ResourcePoolEntry e) { return e.path().startsWith(\"/java.\") || e.path().startsWith(\"/javax.\") || e.path().startsWith(\"/jdk.\"); } private boolean isModuleInfo(ResourcePoolEntry e) { return e.path().endsWith(\"module-info.class\"); } private boolean isClassFile(ResourcePoolEntry e) { return e.path().endsWith(\"class\"); }      1 Create an Animal Sniffer signature for all the APIs in modules added to the runtime image   2 Verify all classes against that signature   3 If there\u0026#8217;s a signature violation, fail the jlink execution by raising a PluginException   4 All classes are passed on as-is   5 Feed each class to Animal Sniffer\u0026#8217;s signature builder for creating the signature; non-class resources and module descriptors are ignored   6 Verify each class against the signature; JDK classes can be skipped here, we assume there\u0026#8217;s no inconsistencies amongst the JDK\u0026#8217;s own modules    The input resource pool is traversed twice: first to create an Animal Sniffer signature of all the APIs, then a second time to validate the image\u0026#8217;s classes against that signature.\n Let me re-iterate that this a very basic, PoC-level implementation of link time API signature validation. A number of incompatibilities would not be detected by this, e.g. adding an abstract method to a superclass or interface, modifying the number and specification of the type parameters of a class, and others. The implementation could also be further optimized by validating only cross-module references. Still, this implementation is good enough to demonstrate the general principle and advantages of link time API consistency validation.\n With the implementation in place (see the README in the PoC\u0026#8217;s GitHub repository for details on building the project), it\u0026#8217;s time to invoke jlink again, this time activating the new plug-in. Now, as mentioned before, the jlink plug-in API isn\u0026#8217;t publicly exposed as of Java 15 (the current Java version at the point of writing), which means we need to jump some hoops in order to enable the plug-in and expose it to the jlink tool itself.\n In a nutshell, a Java agent can be used to bend the module configurations as needed. Details can be found in aforementioned post on the Hibernate blog (the agent\u0026#8217;s source code is here). The required boiler plate can be nicely encapsulated within a shell function:\n 1 2 3 4 5 6 function myjlink { \\ $JAVA_HOME/bin/jlink \\ -J-javaagent:signature-check-jlink-plugin-registration-agent-1.0-SNAPSHOT.jar \\ -J--module-path=signature-check-jlink-plugin-1.0-SNAPSHOT.jar:path/to/animal-sniffer-1.19.jar:path/to/asm-9.0.jar \\ -J--add-modules=dev.morling.jlink.plugins.sigcheck \"$@\" \\ }    All the -J options are VM options passed through to the jlink tool, in order to register the required Java agent and add the plug-in module to jlink\u0026#8217;s module path. Instead of directly calling jlink binary itself, this wrapper function can now be used to invoke jlink with the custom plug-in. Let\u0026#8217;s first take a look at the description in the plug-in list:\n 1 2 3 4 5 6 7 8 9 10 11 $ myjlink --list-plugins ... Plugin Name: check-signatures Plugin Class: dev.morling.jlink.plugins.sigcheck.SignatureCheckPlugin Plugin Module: dev.morling.jlink.plugins.sigcheck Category: VERIFIER Functional state: Functional. Option: --check-signatures Description: Checks the API references amongst the modules of an application for consistency ...    Now let\u0026#8217;s try and create the runtime image with the mismatching customer and order modules again:\n 1 2 3 4 5 6 7 8 9 10 myjlink --module-path=path/to/customer-2.0.0.jar:path/to/order-1.0.0.jar \\ --add-modules=com.example.order \\ --output=target/runtime-image \\ --check-signatures [INFO] Wrote signatures for 6156 classes. [ERROR] /com.example.order/com/example/order/OrderService.class:5: Undefined reference: void com.example.customer.CustomerService .incrementLoyaltyPoints(long, long) Error: Signature violations, check the logs    Et voilà! The mismatching signature of the incrementLoyaltyPoints() method was spotted and the creation of the runtime image failed. Now we could take action, examine our module path and make sure to feed correctly matching versions of the customer and order modules to the image creation process.\n   Summary The link time phase\u0026#8201;\u0026#8212;\u0026#8201;added to the Java platform as part of the module system in version 9, and positioned between the well-known compile time and run-time phases\u0026#8201;\u0026#8212;\u0026#8201;opens up very interesting opportunities to apply whole-world optimizations and validations to Java applications. One example is the checking the API definitions and usages across the different modules of a Java application for consistency. By means of a custom plug-in for the jlink tool, this validation can happen at link time, allowing to detect any mismatches when assembling an application, so that this kind of error can be fixed early on, before it hits an integration test or even production environment.\n This is particularly interesting when using the Java module system for building large, modular monolithic applications. Unless you\u0026#8217;re working with custom module layers\u0026#8201;\u0026#8212;\u0026#8201;e.g. via the Layrry launcher\u0026#8201;\u0026#8212;\u0026#8201;only one version of a given module may be present on the module path. If multiple modules of an application depend on different versions of a transitive dependency, link time API signature validation can help to identify inconsistencies caused by converging to a single version of that dependency.\n The approach can also help saving build time; when only modifying a single module of a larger modularized application, instead of re-compiling everything from scratch, you could just re-build that single module. Then, when re-creating the runtime image using this module and the other existing ones, you would be sure that all module API signature definitions and usages still match.\n The one caveat is the fact that the jlink plug-in API isn\u0026#8217;t a public, supported API of the JDK yet. I hope this is going to change some time soon, though. E.g. the next planned LTS release, Java 17, would be a great opportunity for officially adding the ability to build and use custom jlink plug-ins. This would open the road towards more wide-spread use of link time optimizations and validations, beyond those provided by the JDK and the jlink tool itself.\n Until then, you can explore this area starting from the source code of the signature check plug-in and its accompanying Java agent for enabling its usage with jlink.\n  ","id":28,"publicationdate":"Dec 28, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eDiscussions around Java\u0026#8217;s \u003ca href=\"https://openjdk.java.net/jeps/282\"\u003ejlink\u003c/a\u003e tool typically center around savings in terms of (disk) space.\nInstead of shipping an entire JDK,\na custom runtime image created with jlink contains only those JDK modules which an application actually requires,\nresulting in smaller distributables and \u003ca href=\"blog/smaller-faster-starting-container-images-with-jlink-and-appcds/\"\u003econtainer images\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eBut the contribution of jlink\u0026#8201;\u0026#8212;\u0026#8201;as a part of the Java module system at large\u0026#8201;\u0026#8212;\u0026#8201;to the development of Java application\u0026#8217;s is bigger than that:\nwith the notion of \u003cem\u003elink time\u003c/em\u003e it defines an optional complement to the well known phases \u003cem\u003ecompile time\u003c/em\u003e and application \u003cem\u003erun-time\u003c/em\u003e:\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"quoteblock\"\u003e\n\u003cblockquote\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eLink time is an opportunity to do whole-world optimizations that are otherwise difficult at compile time or costly at run-time. An example would be to optimize a computation when all its inputs become constant (i.e., not unknown). A follow-up optimization would be to remove code that is no longer reachable.\u003c/p\u003e\n\u003c/div\u003e\n\u003c/blockquote\u003e\n\u003c/div\u003e","tags":null,"title":"jlink's Missing Link: API Signature Validation","uri":"https://www.morling.dev/blog/jlinks-missing-link-api-signature-validation/"},{"content":"The other day, a user in the Debezium community reported an interesting issue; They were using Debezium with Java 1.8 and got an odd NoSuchMethodError:\n java.lang.NoSuchMethodError: java.nio.ByteBuffer.position(I)Ljava/nio/ByteBuffer; at io.debezium.connector.postgresql.connection.Lsn.valueOf(Lsn.java:86) at io.debezium.connector.postgresql.connection.PostgresConnection.tryParseLsn(PostgresConnection.java:270) at io.debezium.connector.postgresql.connection.PostgresConnection.parseConfirmedFlushLsn(PostgresConnection.java:235) ...   A NoSuchMethodError typically is an indication for a mismatch of the Java version used to compile some code, and the Java version used for running it: some method existed at compile time, but it\u0026#8217;s not available at runtime.\n Now indeed we use JDK 11 for building the Debezium code base, while targeting Java 1.8 as the minimal required version at runtime. But there is a method position(int) defined on the Buffer class (which ByteBuffer extends) also in Java 1.8. And as a matter of fact, the Debezium code compiles just fine with that version, too. So why would the user run into this error then?\n To understand what\u0026#8217;s going on, let\u0026#8217;s create a very simple class for reproducing the issue:\n 1 2 3 4 5 6 7 8 9 import java.nio.ByteBuffer; public class ByteBufferTest { public static void main(String... args) { ByteBuffer buffer = ByteBuffer.wrap(new byte[] { 1, 2, 3 }); buffer.position(1); (1) System.out.println(buffer.get()); } }      1 Why does this not work with Java 1.8 when compiled with JDK 9 or newer?    Compile this with a current JDK:\n $ javac --source 1.8 --target 1.8 ByteBufferTest.java   And sure enough, the NoSuchMethodError shows up when running this with Java 1.8:\n $ java ByteBufferTest Exception in thread \"main\" java.lang.NoSuchMethodError: java.nio.ByteBuffer.position(I)Ljava/nio/ByteBuffer; at ByteBufferTest.main(ByteBufferTest.java:6)   Whereas, when using 1.8 to compile and run this code, it just works fine. Now, if we take a closer look at the error message again, the missing method is defined as ByteBuffer position(int). I.e. for an invoked method like position(), not only its name, parameter type(s), and the name of the declaring class are part of the byte code for that invocation, but also the method\u0026#8217;s return type. A look at the byte code of the class using javap confirms that:\n $ javap -p -c -s -v -l -constants ByteBufferTest ... public static void main(java.lang.String...); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC, ACC_VARARGS Code: stack=4, locals=2, args_size=1 ... 19: aload_1 20: iconst_1 21: invokevirtual #13 // Method java/nio/ByteBuffer.position:(I)Ljava/nio/ByteBuffer; ...   And this points us to the right direction; In Java 1.8, indeed there is no such method, only the position() method on Buffer, which, of course, returns Buffer and not ByteBuffer. Whereas since Java 9, this method (and several others) is overridden in ByteBuffer\u0026#8201;\u0026#8212;\u0026#8201;leveraging Java\u0026#8217;s support for co-variant return types\u0026#8201;\u0026#8212;\u0026#8201;to return ByteBuffer. The Java compiler will now select that method, ByteBuffer position(int), and record that as the invoked method signature in the byte code of the caller class.\n This is per-se a nice usability improvement, as it allows to invoke further ByteBuffer methods on the return value, instead of just those methods declared by Buffer. But as we\u0026#8217;ve seen, it comes with this little surprise when compiling code on JDK 9 or newer, while trying to keep compatibility with older Java versions. And as it turns out, we were not the first or only ones to encounter this issue. Quite a few open-source projects ran into this, e.g. Eclipse Jetty, Apache Pulsar, Eclipse Vert.x, Apache Thrift, the Yugabyte DB client, and a few others.\n How to Prevent This Situation? So what can you do in order to prevent this issue from happening? One first idea could be to enforce selection of the right method by casting to Buffer:\n 1 ((java.nio.Buffer) buffer).position(1);    But while this produces the desired byte code indeed, it isn\u0026#8217;t exactly the best way for doing so. You\u0026#8217;d have to remember to do so for every invocation of any of the affected ByteBuffer methods, and the seemling unneeded cast might be an easy target for some \"clean-up\" by unsuspecting co-workers on our team.\n Luckily, there\u0026#8217;s a much better way, and this is to rely on the Java compiler\u0026#8217;s --release parameter, which was introduced via JEP 247 (\"Compile for Older Platform Versions\"), added to the platform also in JDK 9. In contrast to the more widely known pair of --source and --target, the --release switch will ensure that only byte code is produced which actually will be useable with the specified Java version. For this purpose, the JDK contains the signature data for all supported Java versions (stored in the $JAVA_HOME/lib/ct.sym file).\n So all that\u0026#8217;s needed really is compiling the code with --release=8:\n $ javac --release=8 ByteBufferTest.java   Examine the bytecode using javap again, and now the expected signature is in place:\n 21: invokevirtual #13 // Method java/nio/ByteBuffer.position:(I)Ljava/nio/Buffer;   When run on Java 1.8, this virtual method call will be resolved to Buffer#position(int) at runtime, whereas on Java 9 and later, it\u0026#8217;d resolve to the bridge method inserted by the compiler into the class file of ByteBuffer due to the co-variant return type, which itself calls the overriding ByteBuffer#position(int) method.\n Now let\u0026#8217;s see what happens if we actually try to make use of the overriding method version in ByteBuffer by re-assigning the result:\n 1 2 3 4 ... ByteBuffer buffer = ByteBuffer.wrap(new byte[] { 1, 2, 3 }); buffer = buffer.position(1); ...    Et voilà, this gets rejected by the compiler when targeting Java 1.8, as the return type of the JDK 1.8 method Buffer#position(int) cannot be assigned to ByteBuffer:\n $ javac --release=8 ByteBufferTest.java ByteBufferTest.java:6: error: incompatible types: Buffer cannot be converted to ByteBuffer buffer = buffer.position(1);   To cut a long story short, we\u0026#8201;\u0026#8212;\u0026#8201;and many other projects\u0026#8201;\u0026#8212;\u0026#8201;should have used the --release switch instead of --source/--target, and the user would not have had that issue. In order to achieve the same in your Maven-based build, just specify the following property in your pom.xml:\n 1 2 3 4 5 ... \u0026lt;properties\u0026gt; \u0026lt;maven.compiler.release\u0026gt;8\u0026lt;/maven.compiler.release\u0026gt; \u0026lt;/properties\u0026gt; ...    Note that theoretically you could achieve the same effect also when using --source and --target; by means of the --boot-class-path option, you could advise the compiler to use a specific set of bootstrap class files instead of those from the JDK used for compilation. But that\u0026#8217;d be quite a bit more cumbersome as it requires you to actually provide the classes of the targeted Java version, whereas --release will make use of the signature data coming with the currently used JDK itself.\n  ","id":29,"publicationdate":"Dec 21, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe other day, a user in the \u003ca href=\"https://debezium.io/\"\u003eDebezium\u003c/a\u003e community reported an interesting issue;\nThey were using Debezium with Java 1.8 and got an odd \u003ccode\u003eNoSuchMethodError\u003c/code\u003e:\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"ByteBuffer and the Dreaded NoSuchMethodError","uri":"https://www.morling.dev/blog/bytebuffer-and-the-dreaded-nosuchmethoderror/"},{"content":"Functional unit and integration tests are a standard tool of any software development organization, helping not only to ensure correctness of newly implemented code, but also to identify regressions\u0026#8201;\u0026#8212;\u0026#8201;bugs in existing functionality introduced by a code change. The situation looks different though when it comes to regressions related to non-functional requirements, in particular performance-related ones: How to detect increased response times in a web application? How to identify decreased throughput?\n These aspects are typically hard to test in an automated and reliable way in the development workflow, as they are dependent on the underlying hardware and the workload of an application. For instance assertions on the duration of specific requests of a web application typically cannot be run in a meaningful way on a developer laptop, which differs from the actual production hardware (ironically, nowadays both is an option, the developer laptop being less or more powerful than the actual production environment). When run in a virtualized or containerized CI environment, such tests are prone to severe measurement distortions due to concurrent load of other applications and jobs.\n This post introduces the JfrUnit open-source project, which offers a fresh angle to this topic by supporting assertions not on metrics like latency/throughput themselves, but on indirect metrics which may impact those. JfrUnit allows you define expected values for metrics such as memory allocation, database I/O, or number of executed SQL statements, for a given workload and asserts the actual metrics values\u0026#8201;\u0026#8212;\u0026#8201;which are obtained from JDK Flight Recorder events\u0026#8201;\u0026#8212;\u0026#8201;against these expected values. Starting off from a defined base line, future failures of such assertions are an indicator for potential performance regressions in an application, as a code change may have introduced higher GC pressure, the retrieval of unneccessary data from the database, or SQL problems commonly induced by ORM tools, like N+1 SELECT statements.\n JfrUnit provides means of identifying and analyzing such anomalies in a reliable, environment independent way in standard JUnit tests, before they manifest as actual performance regressions in production. Test results are independent from wall clock time and thus provide actionable information, also when not testing with production-like hardware and data volumes.\n This post is a bit longer than usual (I didn\u0026#8217;t have the time to write shorter ;), but it\u0026#8217;s broken down into several sections, so you can pause and continue later on with fresh energy:\n   Getting Started With JfrUnit\n  Case Study 1: Spotting Increased Memory Allocation\n  Case Study 2: Identifying Increased I/O With the Database\n  Discussion\n  Summary and Outlook\n   Getting Started With JfrUnit JfrUnit is an extension for JUnit 5 which integrates Flight Recorder into unit tests; it makes it straight forward to initiate a JFR recording for a given set of event types, execute some test routine, and then assert the JFR events which should have been produced.\n Here is a basic example of a JfrUnit test:\n @JfrEventTest (1) public class JfrUnitTest { public JfrEvents jfrEvents = new JfrEvents(); @Test @EnableEvent(\"jdk.GarbageCollection\") (2) @EnableEvent(\"jdk.ThreadSleep\") public void shouldHaveGcAndSleepEvents() throws Exception { System.gc(); Thread.sleep(1000); jfrEvents.awaitEvents(); (3) ExpectedEvent event = event(\"jdk.GarbageCollection\"); (4) assertThat(jfrEvents).contains(event); event = event(\"jdk.GarbageCollection\") (4) .with(\"cause\", \"System.gc()\")); assertThat(jfrEvents).contains(event); event = event(\"jdk.ThreadSleep\"). with(\"time\", Duration.ofSeconds(1))); assertThat(jfrEvents).contains(event); assertThat(jfrEvents.ofType(\"jdk.GarbageCollection\")).hasSize(1); (5) } }     1 @JfrEventTest marks this as a JfrUnit test, activating its extension   2 All JFR event types to be recorded must be enabled via @EnableEvent   3 After running the test logic, awaitEvents() must be invoked as a synchronization barrier, making sure all previously produced events have been received   4 Using the JfrEventsAssert#event() method, an ExpectedEvent instance can be created\u0026#8201;\u0026#8212;\u0026#8201;optionally specifying one or more expected attribute values\u0026#8201;\u0026#8212;\u0026#8201;which then is asserted via JfrEventsAssert#assertThat()   5 JfrEvents#ofType() allows to filter on specific event types, enabling arbitrary assertions against the returned stream of RecordedEvents    By means of a custom assertThat() matcher method for AssertJ, JfrUnit allows to validate that specific JFR events are raised during at test. Events to be matched are described via their event type name, and optionally one more event attribute vaues. As we\u0026#8217;ll see in a bit, JfrUnit also integrates nicely with the Java Stream API, allowing you to filter and aggregate recorded event atribute values and match them against expected values.\n JfrUnit persists a JFR recording file for each test method, which you can examine after a test failure, for instance using JDK Mission Control. To learn more about JfrUnit and its capabilities, take a look at the project\u0026#8217;s README. The project is in an early proof-of-concept stage at the moment, so changes to its APIs and semantics are likely.\n Now that you\u0026#8217;ve taken the JfrUnit quick tour, let\u0026#8217;s put that knowledge into practice. Our example project will be the Todo Manager Quarkus application you may already be familiar with from my earlier post about custom JFR events. We\u0026#8217;re going to discuss two examples for using JfrUnit to identify potential performance regressions.\n   Case Study 1: Spotting Increased Memory Allocation At first, let\u0026#8217;s explore how to identify increased memory allocation rates. Typically, it\u0026#8217;s mostly library and middleware authors who are interested in this. For a library such as Hibernate ORM it can make a huge difference whether a method that is invoked many times on a hot code path allocates a few objects more or less. Less object allocations mean less work for the garbage collector, which in turn means those precious CPU cores of your machine can spend more cycles processing your actual business logic.\n But also for application developers it can be beneficial to keep an eye on\u0026#8201;\u0026#8212;\u0026#8201;and systematically track\u0026#8201;\u0026#8212;\u0026#8201;object allocations, as regressions there lead to increased GC pressure, and in turn eventually to higher latencies and reduced throughput.\n The key for tracking object allocations with JFR are the jdk.ObjectAllocationInNewTLAB and jdk.ObjectAllocationOutsideTLAB events, which are emitted when\n   an object allocation triggered the creation of a new thread-local allocation buffer (TLAB)\n  an object got allocated outside of the thread\u0026#8217;s TLAB\n       Thread-local allocation buffers (TLAB) When creating new object instances on the heap, this primarily happens via thread-local allocation buffers. A TLAB is a pre-allocated memory block that\u0026#8217;s exclusively used by a single thread. Since this space is exclusively owned by the thread, creating new objects within a TLAB can happen without costly synchronization with other threads. Once a thread\u0026#8217;s current TLAB capacity is about to be exceeded by a new object allocation, a new TLAB will be allocated for that thread. In addition, large objects will typically need to be directly allocated outside of the more efficient TLAB space.\n To learn more about TLAB allocation, refer to part #4 of Aleksey Shipilёv\u0026#8217;s \"JVM Anatomy Quark\" blog series.\n     Note these events don\u0026#8217;t allow for tracking of each individual object allocation, as multiple objects will be allocated within a TLAB before a new one is required, and thus the jdk.ObjectAllocationInNewTLAB event will be emitted. But as that event exposes the size of the new TLAB, we can keep track of the overall amount of memory that\u0026#8217;s allocated while the application is running.\n In that sense, jdk.ObjectAllocationInNewTLAB represents a sampling of object allocations, which means we need to collect a reasonable number of events to identify those locations in the program which are the sources of high object allocation and thus frequently trigger new TLAB creations.\n So let\u0026#8217;s start and work on a test for spotting regressions in terms of object allocations of one of the Todo Manager app\u0026#8217;s API methods, GET /todo/{id}. To identify a baseline of the allocation to be expected, we first invoke that method in a loop and print out the actual allocation values. This should happen in intervals, e.g. every 10,000 invocations, so to average out numbers from individual API calls.\n @Test @EnableEvent(\"jdk.ObjectAllocationInNewTLAB\") (1) @EnableEvent(\"jdk.ObjectAllocationOutsideTLAB\") public void retrieveTodoBaseline() throws Exception { Random r = new Random(); HttpClient client = HttpClient.newBuilder() .build(); for (int i = 1; i\u0026lt;= 100_000; i++) { executeRequest(r, client); if (i % 10_000 == 0) { jfrEvents.awaitEvents(); (2) long sum = jfrEvents.filter(this::isObjectAllocationEvent) (3) .filter(this::isRelevantThread) .mapToLong(this::getAllocationSize) .sum(); System.out.printf( Locale.ENGLISH, \"Requests executed: %s, memory allocated: (%,d bytes/request)%n\", i, sum/10_000 ); jfrEvents.reset(); (4) } } private void executeRequest(Random r, HttpClient client) throws Exception { int id = r.nextInt(20) + 1; HttpRequest request = HttpRequest.newBuilder() .uri(new URI(\"http://localhost:8081/todo/\" + id)) .headers(\"Content-Type\", \"application/json\") .GET() .build(); HttpResponse\u0026lt;String\u0026gt; response = client .send(request, HttpResponse.BodyHandlers.ofString()); assertThat(response.statusCode()).isEqualTo(200); } private boolean isObjectAllocationEvent(RecordedEvent re) { (5) String name = re.getEventType().getName(); return name.equals(\"jdk.ObjectAllocationInNewTLAB\") || name.equals(\"jdk.ObjectAllocationOutsideTLAB\"); } private long getAllocationSize(RecordedEvent re) { (6) return re.getEventType().getName() .equals(\"jdk.ObjectAllocationInNewTLAB\") ? re.getLong(\"tlabSize\") : re.getLong(\"allocationSize\"); } private boolean isRelevantThread(RecordedEvent re) { (7) return re.getThread().getJavaName().startsWith(\"vert.x-eventloop\") || re.getThread().getJavaName().startsWith(\"executor-thread\"); } }     1 Enable the jdk.ObjectAllocationInNewTLAB and jdk.ObjectAllocationOutsideTLAB JFR events   2 Every 10,000 events, wait for all the JFR events produced so far   3 Calculate the total allocation size, by summing up the TLAB allocations of all relevant threads   4 Reset the event stream for the next iteration   5 Is this a TLAB event?   6 Get the new TLAB size in case of an in TLAB allocation, otherwise the allocated object size out of TLAB   7 We\u0026#8217;re only interested in the web application\u0026#8217;s own threads, in particular ignoring the main thread which runs the HTTP client of the test    Note that unlike in the initial example showing the usage of JfrUnit, here we\u0026#8217;re not using the simple contains() AssertJ matcher, but rather calculate some custom value\u0026#8201;\u0026#8212;\u0026#8201;the overall object allocation in bytes\u0026#8201;\u0026#8212;\u0026#8201;by means of filtering and aggregating the relevant JFR events.\n Here are the numbers I got from running 100,000 invocations:\n Requests executed: 10000, memory allocated: 34096 bytes/request Requests executed: 20000, memory allocated: 31768 bytes/request Requests executed: 30000, memory allocated: 31473 bytes/request Requests executed: 40000, memory allocated: 31462 bytes/request Requests executed: 50000, memory allocated: 31547 bytes/request Requests executed: 60000, memory allocated: 31545 bytes/request Requests executed: 70000, memory allocated: 31537 bytes/request Requests executed: 80000, memory allocated: 31624 bytes/request Requests executed: 90000, memory allocated: 31703 bytes/request Requests executed: 100000, memory allocated: 31682 bytes/request   As we see, there\u0026#8217;s some warm-up phase during which allocation rates still go down, but after ~20 K requests, the allocation per request is fairly stable, with a volatility of ~1% when averaged out over 10K requests. This means that this initial phase should be excluded during the actual test.\n To emphasize the key part again, this allocation is per request, it is independent from wall clock time and thus is neither dependent from the machine running the test (i.e. the test should behave the same when running on a developer laptop and on a CI machine), nor is it subject to volatility induced by other workloads running concurrently.\n     Tracking Object Allocations in Java 16 The two TLAB allocation events provide all the information required for analysing object allocations in Java applications, but often it\u0026#8217;s not practical to enable them on a continuous basis when running in production. Due to the high amount of events produced, enabling them adds some overhead in terms of latency, also the size of JFR recording files can be hard to predict.\n Both issues are addressed by a JFR improvement that\u0026#8217;s proposed for inclusion into Java 16, \"JFR Event Throttling\". This will provide control over the emission rate of events, e.g. allowing to sample object allocations with a defined rate of 100 events per second, which addresses both the overhead as well as the recording size issue. A new event type, jdk.ObjectAllocationSample will be added, too, which will be enabled in the JFR default configuration.\n For JfrUnit, explicit control over the event sampling rate will be a very interesting capability, as a higher sampling rate may lead to stable results more quickly, in turn resulting in shorter test execution times.\n     Based on that, the actual test could look like so:\n @Test @EnableEvent(\"jdk.ObjectAllocationInNewTLAB\") @EnableEvent(\"jdk.ObjectAllocationOutsideTLAB\") public void retrieveTodo() throws Exception { Random r = new Random(); HttpClient client = HttpClient.newBuilder().build(); for (int i = 1; i\u0026lt;= 20_000; i++) { (1) executeRequest(r, client); } jfrEvents.awaitEvents(); jfrEvents.reset(); for (int i = 1; i\u0026lt;= 10_000; i++) { (2) executeRequest(r, client); } jfrEvents.awaitEvents(); long sum = jfrEvents.filter(this::isObjectAllocationEvent) .filter(this::isRelevantThread) .mapToLong(this::getAllocationSize) .sum(); assertThat(sum / 10_000).isLessThan(33_000); (3) }     1 Warm-up phase   2 The actual test phase   3 Assert the memory allocation per request is within the expected boundary; note we could also add a lower boundary, so to make sure we notice any future improvements (e.g. caused by upgrading to new efficient versions of a library), which otherwise may hide subsequent regressions    Now let\u0026#8217;s assume we\u0026#8217;ve wrapped up the initial round of work on this application, and its tests have been passing on CI for a while. One day, the retrieveTodo() performance test method fails though:\n java.lang.AssertionError: Expecting: \u0026lt;388370L\u0026gt; to be less than: \u0026lt;33000L\u0026gt;   Ugh, it\u0026#8217;s suddenly allocating about ten times more memory per request than before! What has happened? To find the answer, we can take a look at the test\u0026#8217;s JFR recording, which JfrUnit persists under target/jfrunit:\n ls target/jfrunit dev.morling.demos.quarkus.TodoResourcePerformanceTest-createTodo.jfr dev.morling.demos.quarkus.TodoResourcePerformanceTest-retrieveTodo.jfr   Let\u0026#8217;s open the *.jfr file for the failing test in JDK Mission Control (JMC) in order to analyse all the recorded events (note that the recording will always contain some JfrUnit-internal events which are needed for synchronizing the recording stream and the events exposed to the test).\n When taking a look at the TLAB events of the application\u0026#8217;s executor thread, the culprit is identified quickly; a lot of the sampled TLAB allocations contain this stack trace (click on the image to enlarge):\n   Interesting, REST Assured loading a Jackson object mapper, what\u0026#8217;s going on there? Here\u0026#8217;s the full stacktrace:\n   So it seems a REST call to another service is made from within the TodoResource#get(long) method! At this point we know where to look into the source code of the application:\n @GET @Transactional @Produces(MediaType.APPLICATION_JSON) @Path(\"/{id}\") public Response get(@PathParam(\"id\") long id) throws Exception { Todo res = Todo.findById(id); User user = RestAssured.given().port(8082) .when() .get(\"/users/\" + res.userId) .as(User.class); res.userName = user.name; return Response.ok() .entity(res) .build(); }   Gasp, it looks like a developer on the team has been taking the microservices mantra a bit too far, and has changed the code so it invokes another service in order to obtain some additional data associated to the user who created the retrieved todo.\n While that\u0026#8217;s problematic in its own right due to the inherent coupling between the two services (how should the Todo Manager service react if the user service isn\u0026#8217;t available?), they made matters worse by using the REST Assured API as a REST client, in a less than ideal way. The API\u0026#8217;s simplicity and elegance makes it a great solution for testing (and indeed that\u0026#8217;s its primary use case), but this particular usage seems to be not such a good choice for production code.\n At this point you should ask yourself whether the increased allocation per request actually is a problem for your application or not. To determine if that\u0026#8217;s the case, you could run some tests on actual request latency and throughput in a production-like environment. If there\u0026#8217;s no impact based on the workload you have to process, you might very well decide that additional allocations are well spent for your application\u0026#8217;s purposes.\n Increasing the allocation per request by a factor of ten in the described way quite likely does not fall into this category, though. At the very least, we should look into making the call against the User REST API more efficiently, either by setting up REST Assured in a more suitable way, or by looking for an alternative REST client. Of course the external API call just by itself adds to the request latency, which is something we might want to avoid.\n It\u0026#8217;s also worth examining the application\u0026#8217;s garbage collection behavior. In order to so, you can run the performance test method again, either enabling all the GC-related JFR event types, or by enabling a pre-existing JFR configuration (the JDK comes with two built-in JFR configurations, default and profile, but you can also create and export them via JMC):\n @Test @EnableConfiguration(\"profile\") public void retrieveTodo() throws Exception { // ... }   Note that the pre-defined configurations imply minimum durations for certain event types; e.g. the I/O events discussed in the next section will only be recorded if they have a duration of 20 ms or longer. Depending on your testing requirements, you may have to adjust and tweak the configuration to be used.\n Open the recording in JMC, and you\u0026#8217;ll see there\u0026#8217;s a substantial amount of GC activity happening:\n   The difference to the GC behavior before this code change is striking:\n   Pause times are worse, directly impacting the application\u0026#8217;s latency, and the largely increased GC volume means the production environment will be able to serve less concurrent requests when reaching its capacity limits, meaning you\u0026#8217;d have to provision another machine earlier on as your load increases.\n     Memory Leak in the JFR Event Streaming API The astute reader may have noticed that there is a memory leak before and after the code change, as indicated by the ever increased heap size post GC. After some exploration it turned out that this is a bug in the JFR event streaming API which holds on to a large number of RecordedEvent instances internally. Erik Gahlin from the OpenJDK team logged JDK-8257906 for tracking and hopefully fixing this in JDK 16.\n     Now such drastic increase of object allocation and thus potential impact on performance should hopefully be an exception rather than a regular situation. But the example shows how continuous performance unit tests on impacting metrics like memory allocation, using JfrUnit and JDK Flight Recorder and, can help to identify performance issues in an automated and reliable way, preventing such regression to sneak into production. Being able to identify this kind of issue by running tests locally on a developer laptop or a CI server can be a huge time-saver and productivity boost.\n   Case Study 2: Identifying Increased I/O With the Database Once you\u0026#8217;ve started to look at performance regression tests through the lense of JfrUnit, more and more possibilities pop up. Asserting a maximum number of garbage collections? Not a problem. Avoiding an unexpected amount of file system IO? The jdk.FileRead and jdk.FileWrite events are our friend. Examining and asserting the I/O done with the database? Easily doable. Assertions on application-specific JFR event types you\u0026#8217;ve defined yourself? Sure thing!\n You can find a complete list of all JFR event types by JDK version in this nice matrix created by Tom Schindl. The number of JFR event types is growing constantly; as of JDK 15, there are 157 different ones of them.\n Now let\u0026#8217;s take a look at assertions on database I/O, as the amount of data fetched from or written to the database often is a very impactful factor of an enterprise application\u0026#8217;s behavior. A regression here, e.g. fetching more data from the database than anticipated, may indicate that data is unnecessarily loaded. For instance it might be the case that a set of data is loaded only in order to filter it in the application subsequently, instead of doing so via SQL in the database itself, resulting in increased request durations.\n So how could such test look like for our GET /todo/{id} API call? The general approach is the same as before with memory allocations: first define a baseline of the bytes read and written by invoking the API under test for a given number of executions. Once that\u0026#8217;s done, you can implement the actual test, including an assertion on the expected number of bytes read or written:\n @Test @EnableEvent(value=\"jdk.SocketRead\", stackTrace=INCLUDED) (1) @EnableEvent(value=\"jdk.SocketWrite\", stackTrace=INCLUDED) public void retrieveTodo() throws Exception { Random r = new Random(); HttpClient client = HttpClient.newBuilder() .build(); for (int i = 1; i\u0026lt;= ITERATIONS; i++) { executeRequest(r, client); } jfrEvents.awaitEvents(); long count = jfrEvents.filter(this::isDatabaseIoEvent).count(); (2) assertThat(count / ITERATIONS).isEqualTo(4) .describedAs(\"write + read per statement, write + read per commit\"); long bytesReadOrWritten = jfrEvents.filter(this::isDatabaseIoEvent) .mapToLong(this::getBytesReadOrWritten) .sum(); assertThat(bytesReadOrWritten / ITERATIONS).isLessThan(250); (3) } private boolean isDatabaseIoEvent(RecordedEvent re) { (4) return ((re.getEventType().getName().equals(\"jdk.SocketRead\") || re.getEventType().getName().equals(\"jdk.SocketWrite\")) \u0026amp;\u0026amp; re.getInt(\"port\") == databasePort); } private long getBytesReadOrWritten(RecordedEvent re) { (5) return re.getEventType().getName().equals(\"jdk.SocketRead\") ? re.getLong(\"bytesRead\") : re.getLong(\"bytesWritten\"); }     1 Enable the jdk.SocketRead and jdk.SocketWrite event types; by default, those don\u0026#8217;t contain the stacktrace for the events, so that needs to be enabled explicitly   2 There should be four events per invocation of the API method   3 Less than 250 bytes I/O are expected per invocation   4 Only read and write events on the database port are relevant for this test, but e.g. not I/O on the web port of the application   5 Retrieve the value of the event\u0026#8217;s bytesRead or bytesWritten field, depending on the event type    Now let\u0026#8217;s again assume that after some time the test begins to fail. This time it\u0026#8217;s the assertion on the number of executed reads and writes:\n AssertionFailedError: Expecting: \u0026lt;18L\u0026gt; to be equal to: \u0026lt;4L\u0026gt; but was not.   Also the number of bytes read and written has substantially increased:\n java.lang.AssertionError: Expecting: \u0026lt;1117L\u0026gt; to be less than: \u0026lt;250L\u0026gt;   That\u0026#8217;s definitely something to look into. So let\u0026#8217;s open the recording of the failed test in Flight Recorder and take a look at the socket read and write events. Thanks to enabling stacktraces for the two JFR event types we can quite quickly identify the events asssociated to an invocation of the GET /todo/{id} API:\n   At this point, some familiarity with the application in question will come in handy to identify suspicous events. But even without that, we could compare previous recordings of successful test runs with the recording from the failing one in order to see where differences are. In the case at hand, the BlobInputStream and Hibernate\u0026#8217;s BlobTypeDescriptor in the call stack seem pretty unexpected, as our User entity didn\u0026#8217;t have any BLOB attribute before.\n In reality, comparing with the latest version and a look into the git history of that class could confirm that there\u0026#8217;s a new attribute storing an image (perhaps not a best practice to do so ;):\n @Entity public class Todo extends PanacheEntity { public String title; public int priority; public boolean completed; @Lob (1) public byte[] image; }     1 This looks suspicious!    We now would have to decide whether this image attribute actually should be loaded for this particular use case, (if so, we\u0026#8217;d have to adjust the test accordingly), or whether it would for instance make more sense to mark this property as a lazily loaded one and only retrieve it when actually required.\n Solely working with the raw socket read and write events can be a bit cumbersome, though. Wouldn\u0026#8217;t it be nice if we also had the actual SQL statement which caused this I/O? Glad you asked! Neither Hibernate nor the Postgres JDBC driver emit any JFR events at the moment (although well-informed sources are telling me that the Hibernate team wants to look into this). Therefore, in part two of this blog post series, we\u0026#8217;ll discuss how to instrument an existing library to emit events like this, using a Java agent, without modifying the library in question.\n   Discussion JfrUnit in conjunction with JDK Flight Recorder opens up a very interesting approach for identifying potential performance regressions in Java applications. Instead of directly measuring an application\u0026#8217;s performance metrics, most notably latency and throughput, the idea is to measure and assert metrics that impact the performance characteristics. This allows you to implement stable and reliable automated performance regression tests, whose outcome does not depend on the capabilities of the execution environment (e.g. number/size of CPUs), or other influential factors like concurrently running programs.\n Regressions in such impacting metrics, e.g. the amount of allocated memory, or bytes read from a database, are indicators that the application\u0026#8217;s performance may have degraded. This approach offers some interesting advantages over performance tests on actual latency and throughput themselves:\n   Hardware independent: You can identify potential regressions also when running tests on hardware which is different (e.g. less powerful) from the actual production hardware\n  Fast feedback cycle: Being able to run performance regression tests on developer laptops, even in the IDE, allows for fast identification of potential regressions right during development, instead of having to wait for the results of less frequently executed test runs in a traditional performance test lab environment\n  Robustness: Tests are robust and not prone to factors such as the load induced by parallel jobs of a CI server or a virtualized/containerized environment\n  Pro-active identification of performance issues: Asserting a metric like memory allocation can help to identify future performance problems before they actual materialize; while the additional allocation rate may make no difference with the system\u0026#8217;s load as of today, it may negatively impact latency and throughput as the system reaches its limits with increased load; being able to identify the increased allocation rate early on allows for a more efficient handling of the situation while working on the code, compared to when finding out about such regression only later on\n  Reduced need for warm-up: For traditional performance tests of Java-based applications, a thorough warm-up is mandatory, e.g. to ensure proper optimization of the JIT-compiled code. In comparison, metrics like file or database I/O are very stable for a defined workload, so that regressions can be identified also with just a single or a few executions\n   Needless to say, that you should be aware of the limitations of this approach, too:\n   No statement on user-visible performance metrics: Measuring and asserting performance-impacting factors doesn\u0026#8217;t tell you anything in terms of the user-visible performance characteristics themselves. While we can reason about guarantees like \"The system can handle 10K concurrent requests while the 99.9 percentile of requests has a latency of less than 250 ms\", that\u0026#8217;s not the case for metrics like memory allocation or I/O. What does it mean if an application allocates 100 KB of RAM for a particular use case? Is it a lot? Too much? Just fine?\n  Focused on identifying regressions: Somewhat related to the first point, this approach of testing is focused not on specific absolute values, but rather on identifying performance regressions. It\u0026#8217;s hard to tell whether 100 KB database I/O is good or bad for a particular web request, but a change from 100 KB to 200 KB might indicate that something is wrong\n  Focused on identifying potential regressions: A change in performance-impacting metrics does not necessarily imply an actual user-visible performance regression. For instance it might be acceptable for a specific request to allocate more RAM than it did before, if the production system generally isn\u0026#8217;t under high load and the additional GC effort doesn\u0026#8217;t matter in practice\n  Does not work for all performance-impacting metrics: Some performance metrics cannot be meaningfully asserted in plain unit tests; e.g. degraded throughput due to lock contention can typically only be identified with a reasonable number of concurrent requests\n  Only identifies regressions in the application itself: A traditional integrative performance test of an enterprise application will also capture issues in related components, such as the application\u0026#8217;s database. A query run with a sub-optimal execution plan won\u0026#8217;t be noticed with this testing approach\n  Volatile results for timer-based tasks: While metrics like object allocations should be stable e.g. for a specific web request, events which are timing-based, would yield more events on a slower environment than on a faster machine\n     Summary and Outlook JUnit tests based on performance-impacting factors can be a very useful part of the performance testing strategy for an application. They can help to identify potential performance regressions very early in the development lifecycle, when they can be fixed comparatively easy and cheap. Of course they are no silver bullet; you should consider them as complement for classic performance tests running on production-like hardware, not a replacement.\n The approach may feel a bit unfamiliar initially, and it may take some time to learn about the different metrics which can be measured with JFR and asserted via JfrUnit, as well as their implications on an application\u0026#8217;s performance characteristics. But once this hurdle is passed, continuous performance regression tests can be a valuable tool in the box of every software and performance engineer.\n JfrUnit is still in its infancy, and could evolve into a complete toolkit around automated test of JFR-based metrics. Ideas for future development include:\n   A more powerful \"built-in\" API which e.g. provides the functionality for calculating the total TLAB allocations of a given set of threads as a ready-to-use method\n  It could also be very interesting to run assertions against externally collected JFR recording files. This would allow to validate workloads which require more complex set-ups, e.g. running in a dedicated performance testing lab, or even from continuous recordings taken in production\n  The JFR event streaming API could be leveraged for streaming queries on live events streamed from a remote system\n  Another use case we haven\u0026#8217;t explored yet is the validation of resource consumption before and after a defined workload. E.g. after logging in and out a user 100 times, the system should roughly consume\u0026#8201;\u0026#8212;\u0026#8201;ignoring any initial growth after starting up\u0026#8201;\u0026#8212;\u0026#8201;the same amount of memory. A failure of such assertion would indicate a potential memory leak in the application\n  JfrUnit might automatically detect that certain metrics like object allocations are still undergoing some kind of warm-up phase and thus are not stable, and mark such tests as potentially incorrect or flaky\n  Keeping track of historical measurement data, e.g. allowing to identify regressions which got introduced step by step over a longer period of time, with one comparatively small change being the straw finally breaking the camel\u0026#8217;s back\n   Your feedback, feature requests, or even contributions to the project will be highly welcomed!\n Stay tuned for part two of this blog post, where we\u0026#8217;ll explore how to trace the SQL statements executed by an application using the JMC Agent and assert these query events using JfrUnit. This will come in very handy for instance for identifying common performance problems like N+1 SELECT statements.\n Many thanks to Hans-Peter Grahsl, John O\u0026#8217;Hara, Nitsan Wakart, and Sanne Grinovero for their extensive feedback while writing this blog post!\n  ","id":30,"publicationdate":"Dec 16, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eFunctional unit and integration tests are a standard tool of any software development organization,\nhelping not only to ensure correctness of newly implemented code,\nbut also to identify regressions\u0026#8201;\u0026#8212;\u0026#8201;bugs in existing functionality introduced by a code change.\nThe situation looks different though when it comes to regressions related to non-functional requirements, in particular performance-related ones:\nHow to detect increased response times in a web application?\nHow to identify decreased throughput?\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThese aspects are typically hard to test in an automated and reliable way in the development workflow,\nas they are dependent on the underlying hardware and the workload of an application.\nFor instance assertions on the duration of specific requests of a web application typically cannot be run in a meaningful way on a developer laptop,\nwhich differs from the actual production hardware\n(ironically, nowadays both is an option, the developer laptop being less or more powerful than the actual production environment).\nWhen run in a virtualized or containerized CI environment, such tests are prone to severe measurement distortions due to concurrent load of other applications and jobs.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThis post introduces the \u003ca href=\"https://github.com/gunnarmorling/jfrunit\"\u003eJfrUnit\u003c/a\u003e open-source project, which offers a fresh angle to this topic by supporting assertions not on metrics like latency/throughput themselves, but on \u003cem\u003eindirect metrics\u003c/em\u003e which may impact those.\nJfrUnit allows you define expected values for metrics such as memory allocation, database I/O, or number of executed SQL statements, for a given workload and asserts the actual metrics values\u0026#8201;\u0026#8212;\u0026#8201;which are obtained from \u003ca href=\"https://openjdk.java.net/jeps/328\"\u003eJDK Flight Recorder\u003c/a\u003e events\u0026#8201;\u0026#8212;\u0026#8201;against these expected values.\nStarting off from a defined base line, future failures of such assertions are an indicator for potential performance regressions in an application, as a code change may have introduced higher GC pressure,\nthe retrieval of unneccessary data from the database, or SQL problems commonly induced by ORM tools, like N+1 SELECT statements.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Towards Continuous Performance Regression Testing","uri":"https://www.morling.dev/blog/towards-continuous-performance-regression-testing/"},{"content":"A few months ago I wrote about how you could speed up your Java application\u0026#8217;s start-up times using application class data sharing (AppCDS), based on the example of a simple Quarkus application. Since then, quite some progress has been made in this area: Quarkus 1.6 brought built-in support for AppCDS, so that now you just need to provide the -Dquarkus.package.create-appcds=true option when building your project, and you\u0026#8217;ll find an AppCDS file in the target folder.\n Things get more challenging though when combining AppCDS with custom Java runtime images, as produced using the jlink tool added in Java 9. Combining custom runtime images with AppCDS is very attractive, in particular when looking at the deployment of Java applications via Linux containers. Instead of putting the full Java runtime into the container image, you only add those JDK modules which your application actually requires. (Parts of) what you save in image size by doing so, can be used for adding an AppCDS archive to your container image. The result will be a container image which still is smaller than before\u0026#8201;\u0026#8212;\u0026#8201;and thus is faster to push to a container registry, distribute to worker nodes in a Kubernetes cluster, etc.\u0026#8201;\u0026#8212;\u0026#8201;and which starts up significantly faster.\n A challenge though is that AppCDS archives must be created with exactly same Java runtime which later on is used to run the application. In the case of jlink this means the custom runtime image itself must be used to produce the AppCDS archive. In other words, the default archive produced by the Quarkus build unfortunately cannot be used with jlink images. The goal for this post is to explore\n   the steps required to create a custom runtime image for a simple Java CRUD application based on Quarkus,\n  how to build a Linux container image with this custom runtime image and the application itself,\n  how this approach compares to container images with the full Java runtime in terms of size and start-up time.\n   Creating a Modular Runtime Image for a Quarkus Application It\u0026#8217;s a common misbelief that only Java applications which have been fully ported to the Java module system (JPMS) would be able to benefit from jlink. But as explained by Simon Ritter in this blog post, this is not true actually; you don\u0026#8217;t need to fully modularize an application in order to run it via a custom runtime image.\n While indeed the creation of a runtime image is a bit easier when it only is comprised of proper Java modules, it also is possible to create a runtime image by explicitly stating which JDK (or other) modules it should contain. The application can then be run via the traditional classpath, just as you\u0026#8217;d do it with a full Java runtime. Which JDK modules to add though? To answer this question, the jdeps tool comes in handy. Via its --print-module-deps option it can determine for a given set of JARs which (JDK) modules they depend on, and which thus are the ones that need to go into the custom runtime image.\n Having built the example application from the previous blog post via mvn clean verify, let\u0026#8217;s try and invoke jdeps like so:\n jdeps --print-module-deps \\ --class-path target/lib/* \\ target/todo-manager-1.0.0-SNAPSHOT-runner.jar   This results in an error though:\n Error: com.sun.istack.istack-commons-runtime-3.0.10.jar is a multi-release jar file but --multi-release option is not set   Ok, we need to tell which code version to analyse for multi-release JARs; no problem:\n jdeps --print-module-deps \\ --multi-release 15 \\ --class-path target/lib/* \\ target/todo-manager-1.0.0-SNAPSHOT-runner.jar   Hum, some progress, but still an issue:\n Exception in thread \"main\" java.lang.module.FindException: Module java.xml.bind not found, required by java.ws.rs   This one is a bit odd; the file org.jboss.spec.javax.ws.rs.jboss-jaxrs-api_2.1_spec-2.0.1.Final.jar is an explicit module with a module-info.class descriptor, which references the module java.xml.bind, and this one is not found on the module path. It\u0026#8217;s not quite clear to me why this is flagged here, given that the JAX-RS API JAR is part of the class path and not the module path. But it\u0026#8217;s not a big problem, we simply can add the JAXB API (which also is provided on the class path) on the module path, too.\n The same issue arises for some other dependencies which are explicit modules already, so we end up with the following configuration:\n jdeps --print-module-deps \\ --multi-release 15 \\ --module-path target/lib/jakarta.activation.jakarta.activation-api-1.2.1.jar:target/lib/org.reactivestreams.reactive-streams-1.0.3.jar:target/lib/org.jboss.spec.javax.xml.bind.jboss-jaxb-api_2.3_spec-2.0.0.Final.jar \\ --class-path target/lib/* \\ target/todo-manager-1.0.0-SNAPSHOT-runner.jar   And another issue, now about some missing dependencies:\n ... org.postgresql.util.internal.Nullness -\u0026gt; org.checkerframework.dataflow.qual.Pure not found org.wildfly.common.wildfly-common-1.5.4.Final-format-001.jar org.wildfly.common.Substitutions$Target_Branch -\u0026gt; com.oracle.svm.core.annotate.AlwaysInline not found ...   After taking a closer look, these are either compile-time only dependencies (like annotations from the Checker framework), or dependencies of optional features which are not relevant for our case. These can be safely ignored using the --ignore-missing-deps switch, which leaves us with this jdeps invocation:\n jdeps --print-module-deps \\ --ignore-missing-deps \\ --multi-release 15 \\ --module-path target/lib/jakarta.activation.jakarta.activation-api-1.2.1.jar:target/lib/org.reactivestreams.reactive-streams-1.0.3.jar:target/lib/org.jboss.spec.javax.xml.bind.jboss-jaxb-api_2.3_spec-2.0.0.Final.jar \\ --class-path target/lib/* \\ target/todo-manager-1.0.0-SNAPSHOT-runner.jar   The required JDK modules are printed out finally:\n java.base,java.compiler,java.instrument,java.naming,java.rmi,java.security.jgss,java.security.sasl,java.sql,jdk.jconsole,jdk.unsupported   I.e. out of the nearly 60 modules which make up OpenJDK 15, only ten are required by this particular application. Building a custom runtime image containing only these modules should result in quite some space saving.\n     Why is a Particular Module Required? When looking at the module list, you might wonder why certain modules actually are needed. What is this application doing with jdk.jconsole for instance? To gain insight into this, jdeps can help, too. Run it again without the --print-module-deps switch, and you can grep for interesting module references:\n jdeps \u0026lt;...\u0026gt; | grep jconsole org.jboss.narayana.jta.narayana-jta-5.10.6.Final.jar -\u0026gt; jdk.jconsole com.arjuna.ats.arjuna.tools.stats -\u0026gt; com.sun.tools.jconsole jdk.jconsole   In this case, there\u0026#8217;s a single dependency to jconsole, from the Narayana transaction manager. Depending on the details, it might be an opportunity to reach out to the maintainers of such library and discuss, whether this dependency really is needed or whether it could be avoided (e.g. by moving the code in question to a separate module), resulting in a further decreased size of custom runtime images.\n     With the list of required modules, creating the actual runtime image is rather simple:\n $JAVA_HOME/bin/jlink \\ --add-modules java.base,java.compiler,java.instrument,java.naming,java.rmi,java.security.jgss,java.security.sasl,java.sql,jdk.jconsole,jdk.unsupported \\ --compress 2 --no-header-files --no-man-pages \\(1) --output target/runtime-image (2)     1 Compressing the runtime image as well as omitting header files and man pages helps to further reduce the size of the runtime image   2 Output location for creating the runtime image    In order to create a dynamic AppCDS archive for our application classes later on, we now need to add the class data archive for all of the classes of the image itself. Failing to do so results in this error message:\n Error occurred during initialization of VM DynamicDumpSharedSpaces is unsupported when base CDS archive is not loaded   This step isn\u0026#8217;t very well documented, and at this point I was somewhat stuck. But you always can count on the OpenJDK community: after asking about this on Twitter, Claes Redestad pointed me into the right direction:\n ./target/runtime-image/bin/java -Xshare:dump   Thanks, Claes! This creates the base class data archive under target/runtime-image/lib/server/classes.jsa, adding ~12 MB to the runtime image, which now has a size of ~63 MB; not too bad.\n   Adding an AppCDS Archive to a Custom Runtime Image Having created the custom Java runtime image, let\u0026#8217;s now add the AppCDS archive to it. Since the introduction of dynamic AppCDS archives in JDK 13, this is one simple step which only requires to run the application with the -XX:ArchiveClassesAtExit option:\n cd target (1) mkdir runtime-image/cds (2) (3) runtime-image/bin/java \\ -XX:ArchiveClassesAtExit=runtime-image/cds/app-cds.jsa \\ -jar todo-manager-1.0.0-SNAPSHOT-runner.jar cd ..     1 The class path used when running the application later on must be the same as (or rather a prefix of, to be precise) the class path used for building the AppCDS archive; hence changing to the target directory, so to run with -jar *-runner.jar, instead of with -jar target/*-runner.jar   2 Creating a folder for storing the AppCDS archive   3 Using the java binary of the runtime image to launch the application and create the AppCDS archive when exiting    This will create the CDS archive under target/runtime-image/cds/app-cds.jsa. In the next step this can be added to a Linux container image, built e.g. using Docker or podman. Note that while jlink supports cross-platform builds (so for instance you could build a custom runtime image for a Linux container on macOS), the same isn\u0026#8217;t the case for AppCDS. This means an AppCDS archive to be used by a containerized application needs to be built on Linux. When not running on Linux yourself, but on Windows or macOS, you could put the entire build process into a container for this purpose.\n   Creating a Linux Container Image At this point we have built our actual application, a custom Java runtime image with the required JDK modules, and an AppCDS archive for the application\u0026#8217;s classes. The final step is to put everything into a Linux container image, which is quickly done via a small Dockerfile:\n FROM registry.fedoraproject.org/fedora-minimal:33 COPY target/runtime-image /opt/todo-manager/jdk COPY target/lib/* /opt/todo-manager/lib/ COPY target/todo-manager-1.0.0-SNAPSHOT-runner.jar /opt/todo-manager COPY todo-manager.sh /opt/todo-manager ENTRYPOINT [ \"/opt/todo-manager/todo-manager.sh\" ]   This uses the Fedora minimal base image, which is a great foundation for container images. With a size of ~120 MB, it\u0026#8217;s small enough to be distributed efficiently, while still providing the flexibility of a complete Linux distribution, e.g. allowing for the installation of additional tools if needed.\n     Even Smaller Container Images If you wanted to shrink the image size further and felt adventureous, you could look into using Alpine Linux as a base image; the issue there though is that Alpine comes with musl instead of glibc (as used by the JDK) as its implementation of the ISO C and POSIX standard APIs. The OpenJDK Portola project aims at providing a port to Alpine and musl. But as of JDK 15, no GA build of this port exists yet. For JDK 16, an early access build of the Alpine/musl port is available.\n Another option for smaller images is to use jib, which also is supported by Quarkus out of the box. I haven\u0026#8217;t tried out yet though whether/how jib would work with custom runtime images and AppCDS.\n It\u0026#8217;s also worth pointing out that the size of base images doesn\u0026#8217;t matter too much in practice, as container images use a layered file system, which means that typically rather stable base image layers don\u0026#8217;t need to be redistributed too often when pushing or pulling a container image.\n     The container\u0026#8217;s entry point, todo-manager.sh, is a basic shell script, which starts the actual Java application via the Java runtime image:\n #!/bin/bash export PATH=\"/opt/todo-manager/jdk/bin:${PATH}\" cd /opt/todo-manager \u0026amp;\u0026amp; \\ (1) exec java -Xshare:on -XX:SharedArchiveFile=jdk/cds/app-cds.jsa -jar \\ (2) todo-manager-1.0.0-SNAPSHOT-runner.jar     1 Changing into the todo-manager directory, so to make sure the same JAR path is passed as when creating the CDS archive   2 Specifying the archive name; the -Xshare:on isn\u0026#8217;t strictly needed, it\u0026#8217;s used here though to ensure the process will fail if something is wrong with the CDS archive, instead of silently not using it      Let\u0026#8217;s See Some Numbers! Finally, let\u0026#8217;s compare some numbers: container image size, and start-up time for different ways of containerizing the todo manager application. I\u0026#8217;ve tried out four different aproaches:\n   OpenJDK 11 on the RHEL UBI 8.3 image (universal base image), as per the default Dockerfile created for new Quarkus applications\n  A full OpenJDK 15 on Fedora 33 (as there\u0026#8217;s no OpenJDK 15 package for the RHEL base image yet)\n  A custom runtime image for OpenJDK 15 on Fedora 33\n  A custome runtime image with AppCDS on Fedora 33\n   Here are the results, running on a Hetzner Cloud CX4 instance (4 vCPUs, 16 GB RAM), using Fedora 33 as the host OS:\n   As we can see, the container image size is significantly lower when adding a custom Java runtime image instead of the full JDK. In particular when comparing to the OpenJDK package of Fedora 33 which is a fair bit larger than the OpenJDK 11 package of the RHEL UBI 8.3 image, the difference is striking.\n The start-up times are as displayed by Quarkus, averaged over five runs. Numbers have improved by about 10% by going from OpenJDK 11 to 15, which is explained by multiple improvements in this area, most notably the introduction of default CDS archives for the JDK\u0026#8217;s own classes in JDK 12 (JEP 341). Using a custom runtime image by itself doesn\u0026#8217;t have any measurable impact on start-up time. The AppCDS archive improves the start-up time by a whopping 54%. Unless pure image size is the key factor for you (in which case you should look for alternative approaches anyways, see note \"Even Smaller Container Images\" above), I would say that the additional 40 MB for the AppCDS archive are more than worth it. In particular as the resulting container image still is way smaller than when adding the full JDK, be it with the Fedora base image or the RHEL UBI one.\n Based on those numbers, I think it\u0026#8217;s fair to say that custom Java runtime images created via jlink, combined with AppCDS archives are a great foundation for containerized Java applications. Adding a custom runtime image containing only those JDK modules actually needed by an application help to cut down image size signficantly. Parts of that saved space can be invested into adding an AppCDS archive, so you end up with a container image that\u0026#8217;s smaller and starts up faster. I.e. you can have this cake, and eat it, too!\n The one downside is the increased complexity of the build process for producing the runtime image as well as the AppCDS archive. This should be manageable though by means of scripting and automation; also I\u0026#8217;d expect tooling like the Quarkus Maven plug-in and others to further improve on this front. One tricky aspect is that you must not forget to rebuild the custom runtime image, in case you have added dependencies to your application which affect the set of required JDK modules. Automated tests of the application running via the runtime image should help to identify this situation.\n If you\u0026#8217;d like to give it a try yourself, or obtain numbers for the different deployment approaches on your own hardware, you can find all the required code and information in this GitHub repository.\n  ","id":31,"publicationdate":"Dec 13, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eA few months ago I \u003ca href=\"/blog/building-class-data-sharing-archives-with-apache-maven/\"\u003ewrote about\u003c/a\u003e how you could speed up your Java application\u0026#8217;s start-up times using application class data sharing (\u003ca href=\"http://openjdk.java.net/jeps/350\"\u003eAppCDS\u003c/a\u003e),\nbased on the example of a simple \u003ca href=\"https://quarkus.io/\"\u003eQuarkus\u003c/a\u003e application.\nSince then, quite some progress has been made in this area:\nQuarkus 1.6 brought \u003ca href=\"https://quarkus.io/guides/maven-tooling#quarkus-package-pkg-package-config_quarkus.package.create-appcds\"\u003ebuilt-in support for AppCDS\u003c/a\u003e,\nso that now you just need to provide the \u003cem\u003e-Dquarkus.package.create-appcds=true\u003c/em\u003e option when building your project,\nand you\u0026#8217;ll find an AppCDS file in the \u003cem\u003etarget\u003c/em\u003e folder.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThings get more challenging though when combining AppCDS with custom Java runtime images,\nas produced using the \u003ca href=\"https://docs.oracle.com/en/java/javase/15/docs/specs/man/jlink.html\"\u003ejlink\u003c/a\u003e tool added in Java 9.\nCombining custom runtime images with AppCDS is very attractive,\nin particular when looking at the deployment of Java applications via Linux containers.\nInstead of putting the full Java runtime into the container image, you only add those JDK modules which your application actually requires.\n(Parts of) what you save in image size by doing so,\ncan be used for adding an AppCDS archive to your container image.\nThe result will be a container image which still is smaller than before\u0026#8201;\u0026#8212;\u0026#8201;and thus is faster to push to a container registry, distribute to worker nodes in a Kubernetes cluster, etc.\u0026#8201;\u0026#8212;\u0026#8201;and which starts up significantly faster.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Smaller, Faster-starting Container Images With jlink and AppCDS","uri":"https://www.morling.dev/blog/smaller-faster-starting-container-images-with-jlink-and-appcds/"},{"content":"The Testcontainers project is invaluable for spinning up containerized resources during your (JUnit) tests, e.g. databases or Kafka clusters.\n For users of JUnit 5, the project provides the @Testcontainers extension, which controls the lifecycle of containers used by a test. When testing a Quarkus application though, this is at odds with Quarkus' own @QuarkusTest extension; it\u0026#8217;s a recommended best practice to avoid fixed ports for any containers started by Testcontainers. Instead, you should rely on Docker to automatically allocate random free ports. This avoids conflicts between concurrently running tests, e.g. amongst multiple Postgres containers, started up by several parallel job runs in a CI environment, all trying to allocate Postgres' default port 5432. Obtaining the randomly assigned port and passing it into the Quarkus bootstrap process isn\u0026#8217;t possible though when combining the two JUnit extensions.\n One work-around you can find described e.g. on StackOverflow is setting up the database container via a static class initializer block and then propagating the host and port to Quarkus through system properties. While this works, it\u0026#8217;s not ideal in terms of lifecycle control (e.g. how to make sure the container is started up once at the beginning of an entire test suite), and in general, it just feels a bit hack-ish.\n Luckily, there\u0026#8217;s a better alternative, which interestingly isn\u0026#8217;t discussed as much: using Quarkus' notion of test resources. There\u0026#8217;s just two steps involved. First, create an implementation of the QuarkusTestResourceLifecycleManager interface, which controls your resource\u0026#8217;s lifecycle. In case of a Postgres database, this could look like this:\n public class PostgresResource implements QuarkusTestResourceLifecycleManager { static PostgreSQLContainer\u0026lt;?\u0026gt; db = new PostgreSQLContainer\u0026lt;\u0026gt;(\"postgres:13\") (1) .withDatabaseName(\"tododb\") .withUsername(\"todouser\") .withPassword(\"todopw\"); @Override public Map\u0026lt;String, String\u0026gt; start() { (2) db.start(); return Collections.singletonMap( \"quarkus.datasource.url\", db.getJdbcUrl() ); } @Override public void stop() { (3) db.stop(); } }     1 Configure the database container, using the Postgres 13 container image, the given database name, and credentials   2 Start up the database; the returned map of configuration properties amends/overrides the configuration properties of the test; in this case the datasource URL will be overridden with the value obtained from Testcontainers, which contains the randomly allocated public port of the Postgres container   3 Shut down the database after all tests have been executed    All you then need to do is to reference that test resource from your test class using the @QuarkusTestResource annotation:\n @QuarkusTest @QuarkusTestResource(PostgresResource.class) (1) public class TodoResourceTest { @Test public void createTodoShouldYieldId() { given() .when() .contentType(ContentType.JSON) .body(\"\"\" { \"title\" : \"Learn Quarkus\", \"priority\" : 1, } \"\"\") .then() .statusCode(201) .body( matchesJson( \"\"\" { \"id\" : 1, \"title\" : \"Learn Quarkus\", \"priority\" : 1, \"completed\" : false, } \"\"\")); } }     1 Ensures the Postgres database is started up    And that\u0026#8217;s it! Note that all the test resources of the test module are detected and started up, before starting the first test.\n Bonus: Schema Creation One other subtle issue is the creation of the database schema for the test. E.g. for my Todo example application, I\u0026#8217;d like to use a schema named \"todo\" in the Postgres database:\n create schema todo;   Quarkus supports SQL load scripts for executing SQL scripts when Hibernate ORM starts. But this will be executed only after Hibernate ORM has set up all the database objects, such as tables, sequences, indexes etc. (I\u0026#8217;m using the drop-and-create database generation mode during testing). This means that while a load script is great for inserting test data, it\u0026#8217;s executed too late for defining the actual database schema itself.\n Luckily, most database container images themselves support the execution of load scripts right upon database start-up; The Postgres image is no exception, so it\u0026#8217;s just a matter of exposing that script via Testcontainers. All it needs for that is a bit of tweaking of the Quarkus test resource for Postgres:\n static PostgreSQLContainer\u0026lt;?\u0026gt; db = new PostgreSQLContainer\u0026lt;\u0026gt;(\"postgres:13\") .withDatabaseName(\"tododb\") .withUsername(\"todouser\") .withPassword(\"todopw\") .withClasspathResourceMapping(\"init.sql\", (1) \"/docker-entrypoint-initdb.d/init.sql\", BindMode.READ_ONLY);     1 Expose the file src/main/resources/init.sql as /docker-entrypoint-initdb.d/init.sql within the container    With that in place, Postgres will start up and the \"todo\" schema will be created in the database, before Quarkus boots Hibernate ORM, which will populate the schema, and finally, all tests can run.\n You can find the complete source code of this test and the Postgres test resource on GitHub.\n Many thanks to Sergei Egorov for his feedback while writing this blog post!\n  ","id":32,"publicationdate":"Nov 28, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.testcontainers.org/\"\u003eTestcontainers\u003c/a\u003e project is invaluable for spinning up containerized resources during your (JUnit) tests,\ne.g. databases or Kafka clusters.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eFor users of JUnit 5, the project provides the \u003ca href=\"https://www.testcontainers.org/quickstart/junit_5_quickstart/\"\u003e\u003ccode\u003e@Testcontainers\u003c/code\u003e\u003c/a\u003e extension, which controls the lifecycle of containers used by a test.\nWhen testing a \u003ca href=\"https://quarkus.io/\"\u003eQuarkus\u003c/a\u003e application though, this is at odds with Quarkus' own \u003ca href=\"https://quarkus.io/guides/getting-started-testing#recap-of-http-based-testing-in-jvm-mode\"\u003e\u003ccode\u003e@QuarkusTest\u003c/code\u003e\u003c/a\u003e extension;\nit\u0026#8217;s a recommended \u003ca href=\"https://bsideup.github.io/posts/testcontainers_fixed_ports/\"\u003ebest practice\u003c/a\u003e to avoid fixed ports for any containers started by Testcontainers.\nInstead, you should rely on Docker to automatically allocate random free ports.\nThis avoids conflicts between concurrently running tests,\ne.g. amongst multiple Postgres containers,\nstarted up by several parallel job runs in a CI environment, all trying to allocate Postgres' default port 5432.\nObtaining the randomly assigned port and passing it into the Quarkus bootstrap process isn\u0026#8217;t possible though when combining the two JUnit extensions.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Quarkus and Testcontainers","uri":"https://www.morling.dev/blog/quarkus-and-testcontainers/"},{"content":"Layers are sort of the secret sauce of the Java platform module system (JPMS): by providing fine-grained control over how individual JPMS modules and their classes are loaded by the JVM, they enable advanced usages like loading multiple versions of a given module, or dynamically adding and removing modules at application runtime.\n The Layrry API and launcher provides a small plug-in API based on top of layers, which for instance can be used to dynamically add plug-ins contributing new views and widgets to a running JavaFX application. If such plug-in gets removed from the application again, all its classes need to be unloaded by the JVM, avoiding an ever-increasing memory consumption if for instance a plug-in gets updated multiple times.\n In this blog post I\u0026#8217;m going to explore how to ensure classes from removed plug-in layers are unloaded in a timely manner, and how to find the culprit in case some class fails to be unloaded.\n Do We Really Need Plug-ins? Before diving into the details of class unloading, let\u0026#8217;s spend some time to think about the use cases for dynamic plug-ins in Java applications to begin with. I would argue that for typical backend applications this need mostly has diminished. At large, the industry is moving away from application servers and their model around \"deploying\" applications (which you could consider as some kind of \"plug-in\") into a running server process. Instead, there\u0026#8217;s a strong trend towards immutable application packages, based on stacks like Quarkus or Spring Boot, embedding the web server, the application as well as its dependencies, often-times deployed as container images.\n The advantages of this approach centered around immutable images manifold, e.g. in terms of security (no interface for deploying applications is needed) and governance (it\u0026#8217;s always exactly clear which version of the application is running). Updates\u0026#8201;\u0026#8212;\u0026#8201;i.e. the deployment of a new revision of the container image\u0026#8201;\u0026#8212;\u0026#8201;can be put in place e.g. with help of a proxy in front of a cluster of application nodes, which are updated in a rolling manner. That way, there\u0026#8217;s no downtime of the service that\u0026#8217;ll impact the user. Also techniques like canary releases and A/B testing, as well as rolling back to specific earlier versions of an application become a breeze that way.\n The situation is different though when it comes to client applications. When thinking of your favourite editor, IDE or web browser for instance, requiring a restart when installing or updating a plug-in is not desirable. Instead, it should be possible to add plug-ins (or new plug-in versions) to a running application instance and be usable immediately, without interrupting the flow of the user. The same applies for many IoT scenarios, where e.g. an application consuming sensor measurements should be updateable without any downtime.\n   Plug-ins in Layered Java Applications JPMS addresses this requirement via the notion of module layers:\n  A layer is created from a graph of modules in a Configuration and a function that maps each module to a ClassLoader. Creating a layer informs the Java virtual machine about the classes that may be loaded from the modules so that the Java virtual machine knows which module that each class is a member of.\n   Layers are the perfect means of adding new code into a running Java application: they can be added and removed dynamically, and code in an already running layer can invoke functionality from a dynamically added layer in different ways, e.g. via reflection or by using the service loader API. Layrry exposes this functionality via a very basic plug-in API:\n public interface PluginLifecycleListener { void pluginAdded(PluginDescriptor plugin); void pluginRemoved(PluginDescriptor plugin); }   public class PluginDescriptor { public String getName() { ... } public ModuleLayer getModuleLayer() { ... } }   A plug-in in this context is a JPMS layer containing one or more modules (either explicit or automatic) which all are loaded via a single class loader. A Layrry-based application can implement the PluginLifecycleListener service contract in order to be notified whenever a plug-in is added or removed. Plug-ins are loaded from configured directories in the file system which are monitored by Layrry (other means of (un-)installing plug-ins may be added in future versions of Layrry).\n Installing a plug-in is as easy as copying its JAR(s) into a sub-folder of such monitored directory. Layrry will copy the plug-in contents to a temporary directory, create a layer with all the plug-ins JARs, and notify any registered plug-in listeners about the new layer. These will typically use the service loader API then to interact with application-specific services which model its extension points, e.g. to contribute visual UI components in case of a desktop application.\n The reverse process happens when a plug-in gets un-installed: the user removes a plug-in\u0026#8217;s directory, and all listeners will be notified by the Layrry about the removal. They should release all references to any classes from the removed plug-in, rendering it avaible for garbage collection.\n   Class Unloading in Practice There is no API in the Java platform for explicitly unloading a given class. Instead, \"a class or interface may be unloaded if and only if its defining class loader may be reclaimed by the garbage collector\" (JLS, chapter 12.7). This means in a layered Java application any classes in a layer that got removed can be unloaded as soon as the layer\u0026#8217;s class loader is subject to GC. Most importantly, no class in a still running layer must keep a (strong) reference to any class of the removed layer; otherwise this class would hinder collecting the removed layer\u0026#8217;s loader and its classes.\n As an example, let\u0026#8217;s look at the modular-tiles demo, a JavaFX application which uses the Layrry plug-in API for dynamically adding and removing tiles with different widgets like clocks and gauges to its graphical UI. The tiles themselves are implemented using the fabulous TilesFX project by Gerrit Grundwald.\n If you want to follow along, check out the source code of the demo and build it as per the instructions in the README file. Then run the Layrry launcher with the -Xlog:class+unload=info option, so to be notified about any unloaded classes in the system output:\n java -Xlog:class+unload=info \\ -jar path/to/layrry-launcher-1.0-SNAPSHOT-all.jar \\ --layers-config staging/layers.toml \\ --properties staging/versions.properties   Now add and remove some tiles plug-ins a few times:\n cp -r staging/plugins-prepared/* staging/plugins rm -rf staging/plugins/*   The widgets will show up and disappear in the JavaFX UI, but what about class unloading in the logs? In all likelyhood, nothing! This is because without any further configuration, the G1 garbage collector (which is used by the JDK by default since Java 9) will unload classes only during a full garbage collection, which may only run after a long time (if at all), if there\u0026#8217;s no substantial object allocation happening.\n     JEP 158: Unified JVM Logging The -Xlog option has been defined by JEP 158, added to the JDK with Java 9, which provides a \"common logging system for all components of the JVM\". The new unified options should be preferred over the legacy options like -XX:+TraceClassLoading and -XX:+TraceClassUnloading. Usage of -Xlog is described in detail in the java man page; also Nicolai Parlog discusses JEP 158 in great depth in this blog post.\n     So at this point you could trigger a GC explicitly, e.g. via jcmd:\n jcmd \u0026lt;pid\u0026gt; GC.run   But of course that\u0026#8217;s not too desirable when running things in production. Instead, if you\u0026#8217;re on JDK 12 or later, you can use the new G1PeriodicGCInterval option for triggering a periodic GC:\n java -Xlog:class+unload=info \\ -XX:G1PeriodicGCInterval=5000 \\ -jar path/to/layrry-launcher-1.0-SNAPSHOT-all.jar \\ --layers-config staging/layers.toml \\ --properties staging/versions.properties   Introduced via JEP 346 (\"Promptly Return Unused Committed Memory from G1\"), this will periodically initiate a concurrent GC cycle (or optionally even a full GC). Add and remove some plug-ins again, and after some time you should see messages about the unloaded classes in the log:\n ... [138.912s][info][class,unload] unloading class org.kordamp.tiles.sparkline.SparklineTilePlugin 0x0000000800de1840 [138.912s][info][class,unload] unloading class org.kordamp.tiles.gauge.GaugeTilePlugin 0x0000000800de2040 [138.913s][info][class,unload] unloading class org.kordamp.tiles.clock.ClockTilePlugin 0x0000000800de2840 ...   From what I observed, class unloading doesn\u0026#8217;t happen on every concurrent GC cycle; it might take a few cycles after a plug-in has been removed until its classes are unloaded. If you\u0026#8217;re not using G1, but the new low-pause concurrent collectors Shenandoah or ZGC, they\u0026#8217;ll be able to concurrently unload classes without any special configuration needed. Note that class unloading is not a mandatory operation which would have to be provided by every GC implementation. E.g. initial ZGC releases did not support class unloading, which would have rendered them unsuitable for this use case.\n     JEP 371: Hidden Classes As mentioned above, regular classes can only be unloaded if their defining class loader become subject to garbage collection. This can be an issue for frameworks and libraries which generate lots of classes dynamically at runtime, e.g. script language implementations or solutions like Presto, which generates a class for each query.\n The traditional workaround is to generate each class using its own dedicated class loader, which then can be discarded specifically. This solves the GC issue, but it isn\u0026#8217;t ideal in terms of overall memory consumption and speed of class generation. Hence, JDK 15 defines a notion of Hidden Classes (JEP 371), which are not created by class loaders and thus can be unloaded eagerly: \"when all instances of the hidden class are reclaimed and the hidden class is no longer reachable, it may be unloaded even though its notional defining loader is still reachable\".\n You can find some more information on hidden classes in this tweet thread and this code example on GitHub.\n     But who wants to stare at logs in the system output, that\u0026#8217;s so 2010! So let\u0026#8217;s fire up JDK Mission Control and trigger a recording via the JDK Flight Recorder (JFR) to observe what\u0026#8217;s going on in more depth.\n JFR can capture class unloading events, you need to make sure though to enable this event type, which is not the case by default. In order to do so, start a recording, then go to the Template Manager, edit or create a flight recording template and check the Enabled box for the events under Java Virtual Machine \u0026#8594; Class Loading. With the recorder running, add and remove some tiles plug-ins to the running application.\n Once the recording is finished, you should see class unloading events under JVM Internals \u0026#8594; Class Loading:\n   In this case, the classes from a set of plug-ins were unloaded at 16:48:11, which correlates to the periodic GC cycle running at that time and spending a slightly increased time for cleaning up class loader data:\n   As a good Java citizen, Layrry itself also emits JFR events whenever a plug-in layer is added or removed, which helps to track the need for classes to be unloaded:\n     If Things Go Wrong Now let\u0026#8217;s look at the situation where some class failed to unload after its plug-in layer was removed. Common reasons for that include remaining references from classes in a still running layer to classes in the removed layer, threads started by a class in the removed layer which were not stopped, and JVM shutdown hooks registered by code in the removed layer.\n This is known as a class loader leak and is problematic as it means more and more memory will be consumed and cannot be freed as plug-ins are added and removed, which eventually may lead to an OutOfMemoryError. So how could you detect and analyse this situation? An OutOfMemoryError in production would surely be an indicator that there must be a memory or class loader leak somewhere. It\u0026#8217;s also a good idea to regularly examine JFR recording files (e.g. in your testing or staging environment): the absence of any class unloading event despite the removal of plug-ins should trigger an investigation.\n As far as analysing the situation is concerned, examining a heap dump of the application will typically yield insight into the cause rather quickly. Take a heap dump using jcmd as shown above, then load the dump into a tool such as Eclipse MAT. In Eclipse MAT, the \"Duplicate Classes\" action is a great starting point. If one class has been loaded by multiple class loaders, but failed to unload, it\u0026#8217;s a pretty strong indicator that something is wrong:\n   The next step is to analyse the shortest path from the involved class loaders to a GC root:\n   Some object on that path must hold on to a reference to a class or the class loader of the removed plug-in, preventing the loader to be GC-ed. In the case at hand, it\u0026#8217;s the leakingPlugins field in the PluginRegistry class, to which each plug-in is added upon addition of the layer, but then apparently its coffee-deprived author forgot to remove the plug-in from that collection within the pluginRemoved() event handler ;)\n As a quick side note, there\u0026#8217;s a really cool plug-in for Eclipse MAT written by Vladimir Sitnikov, which allows you to query heap dumps using SQL. It maps each class to its own \"table\", so that e.g. classes loaded more than once could be selected using the following SQL query on the java.lang.Class class:\n select c.name, listagg(toString(c.\"@classLoader\")) as 'loaders', count(*) as 'count' from \"java.lang.Class\" c where c.name \u0026lt;\u0026gt; '' group by c.name having count(*) \u0026gt; 1   Resulting in the same list of classes as above:\n   This could come in very handy for more advanced heap dump analyses, which cannot be done using Eclipse MAT\u0026#8217;s built-in query capabilities.\n   Learning More Via module layers, JPMS provides the foundation for dynamic plug-in architectures, as demonstrated by Layrry. Removing layers at runtime requires some care and consideration, so to avoid class loader leaks which eventually may lead to OutOfMemoryErrors. As so often, JDK Mission Control, JFR, and Eclipse MAT prove to be invaluable tools in the box of every Java developer, helping to ensure class unloading in your layered applications is done correctly, and if it is not, helping to understand and fix the underlying issue.\n Here are some more resources about class unloading and analysing class loader leaks:\n   Shenandoah GC in JDK 14, Part 2: Concurrent roots and class unloading: A blog post touching on class unloading in Shenandoah by Roman Kennke\n  ZGC Concurrent Class Unloading: A conference talk by Erik Österlund\n  class loader leaks: A series of blog posts by Mattias Jiderhamn\n  ClassLoader \u0026amp; memory leaks: a Java love story: A post about heap dump analysis by Aloïs Micard\n   Lastly, if you\u0026#8217;d like to explore the dynamic addition and removal of JPMS layers to a running application yourself, the modular-tiles demo app is a great starting point. Its source code can be found on GitHub.\n  ","id":33,"publicationdate":"Oct 14, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eLayers are sort of the secret sauce of the Java platform module system (JPMS):\nby providing fine-grained control over how individual JPMS modules and their classes are loaded by the JVM,\nthey enable advanced usages like loading multiple versions of a given module, or dynamically adding and removing modules at application runtime.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe \u003ca href=\"/blog/introducing-layrry-runner-and-api-for-modularized-java-applications/\"\u003eLayrry\u003c/a\u003e API and launcher provides a small plug-in API based on top of layers,\nwhich for instance can be used to dynamically add plug-ins contributing new views and widgets to a running JavaFX application.\nIf such plug-in gets removed from the application again,\nall its classes need to be unloaded by the JVM, avoiding an ever-increasing memory consumption if for instance a plug-in gets updated multiple times.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIn this blog post I\u0026#8217;m going to explore how to ensure classes from removed plug-in layers are unloaded in a timely manner,\nand how to find the culprit in case some class fails to be unloaded.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Class Unloading in Layered Java Applications","uri":"https://www.morling.dev/blog/class-unloading-in-layered-java-applications/"},{"content":"Lately I\u0026#8217;ve been fascinated by the possibility to analyse the assembly code emitted by the Java JIT (just-in-time) compiler. So far I had only looked only into Java class files using javap; diving into the world of assembly code feels a bit like Alice must have felt when falling down the rabbit whole into wonderland.\n My motivation for this exploration was trying to understand what is faster in Java: a switch statement over strings, or a lookup in a hash map. Solely looking at Java bytecode isn\u0026#8217;t going far enough to answer this question, as the difference lies in the actual assembly statements executed on the CPU. I\u0026#8217;ll keep the details around that for another time; in this post I\u0026#8217;m just going quickly to share what I learned in regards to building a tool needed for this exercise, hsdis.\n hsdis is a disassembler library which can be used with the java runtime as well as tools such as JitWatch to analyse the code produced by the Java JIT compiler. For licensing reasons though it doesn\u0026#8217;t come as a binary with the JDK. Instead, you need it to build yourself from source. Instructions for doing so are spread across a few different places, but I couldn\u0026#8217;t find any 100% current information, in particular as OpenJDK has moved to git and GitHub just recently.\n So here is what you need to do in order to build hsdis for OpenJDK 15; in my case I\u0026#8217;m running on macOS, slightly different steps may apply for other platforms. First, get the OpenJDK source code and check out the version for which you want to build hsdis:\n git clone git@github.com:openjdk/jdk.git git checkout jdk-15+36 # Current stable JDK 15 build   The source location of hsdis has changed with the move from Mercurial to git:\n cd src/utils/hsdis   In order to build hsdis, you\u0026#8217;ll need the GNU Binutils, a collection of several binary tools:\n wget https://ftp.gnu.org/gnu/binutils/binutils-2.35.tar.gz tar xvf binutils-2.35.tar.gz   Then run the actual hsdis build (macOS comes with all the required tools like make):\n make BINUTILS=binutils-2.35 ARCH=amd64   This will take a few minutes; if all goes well, there\u0026#8217;ll be hsdis binary in the build directory, in my case this is build/macosx-amd64/hsdis-amd64.dylib. Copy the library to lib/server of our JDK:\n sudo cp build/macosx-amd64/hsdis-amd64.dylib $JAVA_HOME/lib/server       If you\u0026#8217;re on Linux, you also can provide the hsdis tool via the LD_LIBRARY_PATH environment variable:\n export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:path/to/hsdis/build/linux-amd64   Note this won\u0026#8217;t work on current macOS versions unfortunately due to its System Integrity Protection feature (SIP). Thanks to Brice Dutheil for this tip!\n     Congrats! You now can use the XX:+PrintAssembly flag of the java command to examine the assembly code of your Java program. Let\u0026#8217;s give it a try. Create a Java source file with the following contents:\n public class PrintAssemblyTest { public static void main(String... args) { PrintAssemblyTest hello = new PrintAssemblyTest(); for(int i = 0; i \u0026lt;= 10_000_000; i++) { hello.hello(i); } } private void hello(int i) { if (i % 1_000_000 == 0) { System.out.println(\"Hello, \" + i); } } }   Compile and run it like so:\n javac PrintAssemblyTest.java java -XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly \\ -Xlog:class+load=info -XX:+LogCompilation \\ PrintAssemblyTest   You should then find the assembly code of the hello() method somewhere in the output:\n ============================= C2-compiled nmethod ============================== ----------------------------------- Assembly ----------------------------------- Compiled method (c2) 1409 106 4 PrintAssemblyTest::hello (20 bytes) total in heap [0x000000011e3fce90,0x000000011e3fd148] = 696 relocation [0x000000011e3fcfe8,0x000000011e3fcff8] = 16 main code [0x000000011e3fd000,0x000000011e3fd080] = 128 stub code [0x000000011e3fd080,0x000000011e3fd098] = 24 oops [0x000000011e3fd098,0x000000011e3fd0a0] = 8 metadata [0x000000011e3fd0a0,0x000000011e3fd0a8] = 8 scopes data [0x000000011e3fd0a8,0x000000011e3fd0d0] = 40 scopes pcs [0x000000011e3fd0d0,0x000000011e3fd140] = 112 dependencies [0x000000011e3fd140,0x000000011e3fd148] = 8 -------------------------------------------------------------------------------- [Constant Pool (empty)] -------------------------------------------------------------------------------- [Entry Point] # {method} {0x000000010d74c4b0} 'hello' '(I)V' in 'PrintAssemblyTest' # this: rsi:rsi = 'PrintAssemblyTest' # parm0: rdx = int # [sp+0x30] (sp of caller) 0x000000011e3fd000: mov 0x8(%rsi),%r10d 0x000000011e3fd004: shl $0x3,%r10 0x000000011e3fd008: movabs $0x800000000,%r11 0x000000011e3fd012: add %r11,%r10 0x000000011e3fd015: cmp %r10,%rax 0x000000011e3fd018: jne 0x0000000116977100 ; {runtime_call ic_miss_stub} 0x000000011e3fd01e: xchg %ax,%ax [Verified Entry Point] 0x000000011e3fd020: mov %eax,-0x14000(%rsp) 0x000000011e3fd027: push %rbp 0x000000011e3fd028: sub $0x20,%rsp ;*synchronization entry ; - PrintAssemblyTest::hello@-1 (line 10) 0x000000011e3fd02c: movslq %edx,%r10 0x000000011e3fd02f: mov %edx,%r11d 0x000000011e3fd032: sar $0x1f,%r11d 0x000000011e3fd036: imul $0x431bde83,%r10,%r10 0x000000011e3fd03d: sar $0x32,%r10 0x000000011e3fd041: mov %r10d,%r10d 0x000000011e3fd044: sub %r11d,%r10d 0x000000011e3fd047: imul $0xf4240,%r10d,%r10d ;*irem {reexecute=0 rethrow=0 return_oop=0} ; - PrintAssemblyTest::hello@3 (line 10) 0x000000011e3fd04e: cmp %r10d,%edx 0x000000011e3fd051: je 0x000000011e3fd063 ;*ifne {reexecute=0 rethrow=0 return_oop=0} ; - PrintAssemblyTest::hello@4 (line 10) 0x000000011e3fd053: add $0x20,%rsp 0x000000011e3fd057: pop %rbp 0x000000011e3fd058: mov 0x110(%r15),%r10 0x000000011e3fd05f: test %eax,(%r10) ; {poll_return} 0x000000011e3fd062: retq 0x000000011e3fd063: mov %edx,%ebp 0x000000011e3fd065: sub %r10d,%ebp ;*irem {reexecute=0 rethrow=0 return_oop=0} ; - PrintAssemblyTest::hello@3 (line 10) 0x000000011e3fd068: mov $0xffffff45,%esi 0x000000011e3fd06d: mov %edx,(%rsp) 0x000000011e3fd070: data16 xchg %ax,%ax 0x000000011e3fd073: callq 0x0000000116979080 ; ImmutableOopMap {} ;*ifne {reexecute=1 rethrow=0 return_oop=0} ; - (reexecute) PrintAssemblyTest::hello@4 (line 10) ; {runtime_call UncommonTrapBlob} 0x000000011e3fd078: hlt 0x000000011e3fd079: hlt 0x000000011e3fd07a: hlt 0x000000011e3fd07b: hlt 0x000000011e3fd07c: hlt 0x000000011e3fd07d: hlt 0x000000011e3fd07e: hlt 0x000000011e3fd07f: hlt [Exception Handler] 0x000000011e3fd080: jmpq 0x0000000116a22d80 ; {no_reloc} [Deopt Handler Code] 0x000000011e3fd085: callq 0x000000011e3fd08a 0x000000011e3fd08a: subq $0x5,(%rsp) 0x000000011e3fd08f: jmpq 0x0000000116978ca0 ; {runtime_call DeoptimizationBlob} 0x000000011e3fd094: hlt 0x000000011e3fd095: hlt 0x000000011e3fd096: hlt 0x000000011e3fd097: hlt --------------------------------------------------------------------------------   Interpreting the output is left as an exercise for the astute reader ;-) A great resource for getting started doing so is the post PrintAssembly output explained! by Jean-Philippe Bempel.\n With hsdis in place, you also can use the excellent JitWatch tool for analysing the assembly code, which e.g. not only provides an easy way to navigate from source code to byte code to assembly code, but also comes with helpful tooltips explaining the meaning of the different assembly mnemonics.\n","id":34,"publicationdate":"Oct 5, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eLately I\u0026#8217;ve been fascinated by the possibility to analyse the assembly code emitted by the Java JIT (just-in-time) compiler.\nSo far I had only looked only into Java class files using \u003cem\u003ejavap\u003c/em\u003e;\ndiving into the world of assembly code feels a bit like Alice must have felt when falling down the rabbit whole into wonderland.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Building hsdis for OpenJDK 15","uri":"https://www.morling.dev/blog/building-hsdis-for-openjdk-15/"},{"content":"I\u0026#8217;m excited to share the news about an open-source utility I\u0026#8217;ve been working on lately: JmFrX, a tool for capturing JMX data with JDK Flight Recorder.\n When using JMX (Java Management Extensions), The Java platform\u0026#8217;s standard for monitoring and managing applications, JmFrX allows you to periodically record the attributes from any JMX MBean into JDK Flight Recorder (JFR) files, which you then can analyse using JDK Mission Control (JMC).\n This is useful for a number of reasons:\n   You can track changes to the values of JMX MBean attributes over time without resorting to external monitoring tools\n  You can analyze JMX data from offline JFR recording files in cases where you cannot directly connect to the running application\n  You can export JMX data as live data streams using the JFR event streaming API introduced in Java 14\n   In this blog post I\u0026#8217;m going to explain how to use JmFrX for recording JMX data in your applications, point out some interesting JmFrX implemention details, and lastly will discuss some potential steps for future development of the tool.\n Why JmFrX? JDK Flight Recorder is a \"low-overhead data collection framework for troubleshooting Java applications and the HotSpot JVM\". In combination with the JDK Mission Control client application it allows to gain deep insights into the performance characteristics of Java applications.\n In addition to the built-in metrics and event types, JFR also allows to define and emit custom event types. JFR got open-sourced in JDK 11; since then, developers in the Java eco-system began to support this, enabling users to work with JFR and JMC for analyzing the runtime behavior of 3rd party libraries and frameworks. For instance, JUnit 5.7 produces JFR events related to the execution lifecycle of unit tests.\n At the same time, many library authors are not (yet) in a position where they could easily emit JFR events from their tools, as for instance they might wish to keep compatibility with older Java versions. They might already expose JMX MBeans though which often provide fine-grained information about the execution state of Java applications. This is where JmFrX comes in: by periodically capturing the attribute values from a given set of JMX MBeans, it allows to capture this information in JFR recordings.\n JmFrX isn\u0026#8217;t the first effort that seeks to bridge JMX and JFR; JDK Mission Control project lead Marcus Hirt discusses a similar project in a blog post in 2016. But unlike the implementation described by Marcus in this post, JmFrX is based on the public and supported APIs for defining, configuring and emitting JFR events, as available since OpenJDK 11.\n   How To Use JmFrX In order to use JmFrX, make sure to run OpenJDK 11 or newer. OpenJDK 8 also contains the open-sourced Flight Recorder bits as of release 8u262 (from July this year); so this should work, too, but I haven\u0026#8217;t tested it yet.\n Until a stable release will be provided, you can obtain JmFrX snapshot builds via JitPack. For that, add the JitPack repository to your pom.xml when using Apache Maven (or apply equivalent configuration for your preferred build tool):\n ... \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;jitpack.io\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://jitpack.io\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; ...   Then add the JmFrX dependency:\n ... \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.github.gunnarmorling\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jmfrx\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;master-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; ...   The next step is registering the JmFrX event type with JFR in the start-up routine of your program. This could for instance be done in the main() method, the static initializer of a class loaded early on, an eagerly initialized Spring or CDI bean, etc. A Java agent for this purpose will be provided as part of this project soon.\n When building applications with Quarkus, you could use an application start-up event like so:\n @ApplicationScoped public class EventRegisterer { public void registerEvent(@Observes StartupEvent se) { Jmfrx.getInstance().register(); } public void unregisterEvent(@Observes ShutdownEvent se) { Jmfrx.getInstance().unregister(); } }   Now start your application and create a JFR configuration file which enables the JmFrX event type. To do so, open JDK Mission Control, and choose your running application in the JVM Browser. Then perform these steps:\n   Right-click the target JVM \u0026#8594; Select Start Flight Recording\u0026#8230;\u0026#8203;\n  Click on Template Manager\n  Copy the Continuous setting and click Edit for modifying this copy\n  Expand the JMX and JMX Dump nodes\n  Make sure the JMX Dump event type is Enabled; choose a period for dumping the chosen JMX MBeans (by default 60 s) and specify the MBeans whose data should be captured; that\u0026#8217;s done by means of a regular expression, which matches one or more JMX object names, for instance .*OperatingSystem.*:\n       Close the two last dialogues by clicking OK and OK\n  Important: Make sure that the template you edited is selected under Event settings\n  Click Finish to begin the recording\n   Once the recording is complete, open the recording file in JDK Mission Control and go to the Event Browser. You should see periodic events corresponding to the selected MBeans under the JMX node:\n   When not using JDK Mission Control to initiate recordings, but the jcmd utility on the command line, also follow the same steps as above for creating a configuration as described above. But then, instead of starting the recording, export the configuration file from the template manager and specify its name to jcmd via the settings=/path/to/settings.jfc parameter.\n Now using JmFrX to observe JMX data from for the java.lang MBeans like Runtime and OperatingSystem in JFR isn\u0026#8217;t too exciting yet, as there\u0026#8217;s dedicated JFR event types which contain most of that information. But things get more interesting when capturing data from custom MBean types, as e.g. here for the stream threads metrics from a Kafka Streams application:\n     Customizing Event Formats By default, JmFrX will propagate the raw attribute values from a JMX MBean to the corresponding JFR event. This makes sure that all the information can be retrieved from recordings, but the data format can be a bit unwieldy, e.g. when it comes to data amounts in bytes, or time periods in milli-seconds since epoch.\n To address this, JFR supports a range of metadata annotations such as @DataAmount, @Timespan, or @Percentage, which allow to format event attributes. This information then is used by JMC for instance when displaying events in the browser (see event Properties to the left in the screenshot above).\n JmFrX integrates with this metadata facility via the notion of event profiles, which describe the data format of one MBean type and its attributes. When creating an event for a given JMX MBean, JmFrX will look for a corresponding event profile and apply its settings. Event profiles are defined by implementing the EventProfileContributor SPI. As an example here\u0026#8217;s a subset of the the built-in profile definition for the OperatingSystem MBean:\n public class JavaLangEventProfileContributor implements EventProfileContributor { @Override public void contributeProfiles(EventProfileBuilder builder) { builder.addEventProfile(\"java.lang:type=OperatingSystem\") (1) .addAttributeProfile(\"TotalSwapSpaceSize\", long.class, new AnnotationElement(DataAmount.class, DataAmount.BYTES), (2) v -\u0026gt; v) .addAttributeProfile(\"FreeSwapSpaceSize\", long.class, new AnnotationElement(DataAmount.class, DataAmount.BYTES), v -\u0026gt; v) (3) .addAttributeProfile(\"CpuLoad\", double.class, new AnnotationElement(Percentage.class), v -\u0026gt; v) .addAttributeProfile(\"ProcessCpuLoad\", double.class, new AnnotationElement(Percentage.class), v -\u0026gt; v) .addAttributeProfile(\"SystemCpuLoad\", double.class, new AnnotationElement(Percentage.class), v -\u0026gt; v) .addAttributeProfile(\"ProcessCpuTime\", long.class, new AnnotationElement(Timespan.class, Timespan.NANOSECONDS), v -\u0026gt; v ); } }     1 Profiles are linked via the MBean name   2 The atribute type is specified via an AnnotationElement for one of the JFR type metadata annotations   3 If needed, the actual value can be modified too, e.g. to convert it into another data type, or to shift its value into an expected range (for instance 0 to 1 for percentage values)    Once you\u0026#8217;ve defined the event profiles for your MBean type(s), don\u0026#8217;t forget to register the contributor type either as a service implementation in your module-info.java descriptor (when building a modular Java application):\n module com.example { requires jdk.jfr; requires dev.morling.jmfrx; provides dev.morling.jmfrx.spi.EventProfileContributor with com.example.MyEventProfileContributor; }   When building an application using the traditional classpath, register the names of all profile contributors in the META-INF/services/dev.morling.jmfrx.spi.EventProfileContributor file.\n There\u0026#8217;s a small (yet hopefully growing) set of event profiles built into JmFrX. But as event profile contributors are discovered using the Java service loader mechanism, you can also easily plug in event profiles for other MBean types, e.g. for the JMX MBeans of Apache Kafka or Kafka Connect, or application servers like WildFly.\n Also your pull requests for contributing event profiles for common JMX applications to JmFrX itself will be very welcomed!\n   How It Works If you solely want to use JmFrX, you can pretty much stop reading this post at this point. But if you\u0026#8217;re curious about how it is working internally, stay with me for a bit longer: JmFrX uses two lesser known JFR features which also might be interesting for your own application-specific event types, periodic JFR events and dynamic event types.\n Unlike most JFR event types which are emitted when some specific JVM or application functionality is executed, periodic events are produced in a regular interval. The default interval (which can be overridden by the user) is specified using the @Period annotation on the event type definition:\n @Name(JmxDumpEvent.NAME) @Label(\"JMX Dump\") @Category(\"JMX\") @Description(\"Periodically dumps specific JMX MBeans\") @StackTrace(false) @Period(\"60 s\") public class JmxDumpEvent extends Event { public static final String NAME = \"dev.morling.jmfrx.JmxDumpEvent\"; // event implementation ... }   Upon application start-up, JmFrX registers this event type with the JFR environment:\n ... private Runnable hook; public void register() { hook = () -\u0026gt; { (1) JmxDumpEvent dumpEvent = new JmxDumpEvent(); if (!dumpEvent.isEnabled()) { return; } dumpEvent.begin(); // retrieve data from matching MBean(s) and create event(s) ... dumpEvent.commit(); }; FlightRecorder.addPeriodicEvent(JmxDumpEvent.class, hook); (2) } public void unregister() { FlightRecorder.removePeriodicEvent(hook); (3) } ...     1 The event hook implementation   2 Register the periodic event   3 Unregister the periodic event    The regular expression for specifying the MBean name(s) is passed to the event type as a SettingControl. You can learn more about event settings in my post on custom JFR event types.\n When the periodic event hook runs, it must create one event for each captured MBean. As JmFrX cannot know which MBean(s) you\u0026#8217;re interested in, it\u0026#8217;s not an option to pre-define these event types and their structure.\n This is where dynamic JFR event types come in: Using the EventFactory class, event types can be defined at runtime. Under the covers, JFR will create a corresponding Event sub-class dynamically using the ASM API. Here\u0026#8217;s the relevant JmFrX code which defines the event type for a given MBean:\n ... public static EventDescriptor getDescriptorFor(String mBeanName) { MBeanServer mbeanServer = ManagementFactory.getPlatformMBeanServer(); try { ObjectName objectName = new ObjectName(mBeanName); MBeanInfo mBeanInfo = mbeanServer.getMBeanInfo(objectName); List\u0026lt;AnnotationElement\u0026gt; eventAnnotations = Arrays.asList( (1) new AnnotationElement(Category.class, getCategory(objectName)), new AnnotationElement(StackTrace.class, false), new AnnotationElement(Name.class, getName(objectName)), new AnnotationElement(Label.class, getLabel(objectName)), new AnnotationElement(Description.class, mBeanInfo.getDescription()) ); List\u0026lt;AttributeDescriptor\u0026gt; fields = getFields(objectName, mBeanInfo); List\u0026lt;ValueDescriptor\u0026gt; valueDescriptors = fields.stream() (2) .map(AttributeDescriptor::getValueDescriptor) .collect(Collectors.toList()); return new EventDescriptor(EventFactory.create(eventAnnotations, valueDescriptors), fields); } catch (Exception e) { throw new RuntimeException(e); } } ...     1 Define event metadata like name, label, category etc. via the JFR metadata annotations   2 For each MBean attribute, an attribute is added to the event type; its definition is based on the information in the corresponding event profile, if present    The actual implemention is slightly more complex, as it deals with integrating metadata from JmFrX event profiles and more. You can find the complete code in the EventProfile class.\n   Takeaways JmFrX is a small utility which allows you to capture JMX data with JDK Flight Recorder. It\u0026#8217;s open-source (Apache License, version 2), you can find the source code on GitHub. With the wide usage of JMX for application monitoring in the Java world, JmFrX can help to bring that information into JFR recordings, making it available for offline investigations and analyses.\n Potential next steps for JmFrX include more meaningful handling of tabular and composite JMX data, adding a Java agent for registering the event type, providing some more built-in event profiles and publishing a stable release on Maven Central. Eventually, the JmFrX project might move over to the rh-jmc-team GitHub organization, which is is managed by Red Hat\u0026#8217;s OpenJDK team and contains many other very useful projects around JDK Flight Recorder and Mission Control.\n Your feedback on and contributions to JmFrX will be very welcomed!\n  ","id":35,"publicationdate":"Aug 18, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eI\u0026#8217;m excited to share the news about an open-source utility I\u0026#8217;ve been working on lately:\n\u003ca href=\"https://github.com/gunnarmorling/jmfrx\"\u003eJmFrX\u003c/a\u003e,\na tool for capturing JMX data with JDK Flight Recorder.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eWhen using JMX (\u003ca href=\"https://en.wikipedia.org/wiki/Java_Management_Extensions\"\u003eJava Management Extensions\u003c/a\u003e), The Java platform\u0026#8217;s standard for monitoring and managing applications,\nJmFrX allows you to periodically record the attributes from any JMX MBean into \u003ca href=\"https://openjdk.java.net/jeps/328\"\u003eJDK Flight Recorder\u003c/a\u003e (JFR) files,\nwhich you then can analyse using \u003ca href=\"https://openjdk.java.net/projects/jmc/\"\u003eJDK Mission Control\u003c/a\u003e (JMC).\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Introducing JmFrX: A Bridge From JMX to JDK Flight Recorder","uri":"https://www.morling.dev/blog/introducing-jmfrx-a-bridge-from-jmx-to-jdk-flight-recorder/"},{"content":"I have built a custom search functionality for this blog, based on Java and the Apache Lucene full-text search library, compiled into a native binary using the Quarkus framework and GraalVM. It is deployed as a Serverless application running on AWS Lambda, providing search results without any significant cold start delay. If you thought Java wouldn\u0026#8217;t be the right language for this job, keep reading; in this post I\u0026#8217;m going to give an overview over the implementation of this feature and my learnings along the way.\n Having a search functionality for my blog has been on my mind for quite some time; I\u0026#8217;d like to give users the opportunity to find specific contents on this blog right here on this site, without having to use an external search engine. That\u0026#8217;s not only nice in terms of user experience, but also having insight into the kind of information readers look for on this blog should help me to identify interesting things to write about in the future.\n Now this blog is a static site\u0026#8201;\u0026#8212;\u0026#8201;generated using Hugo, hosted on GitHub Pages\u0026#8201;\u0026#8212;\u0026#8201;which makes this an interesting challenge. I didn\u0026#8217;t want to rely on an external search service (see \"Why No External Search Service\" below for the reasoning), and also a purely client-side solution as described in this excellent blog post didn\u0026#8217;t seem ideal. While technically fascinating, I didn\u0026#8217;t like the fact that it requires shipping the entire search index to the client for executing search queries. Also things like result highlighting, customized result scoring, word stemming, fuzzy search and more seemed a bit more than I\u0026#8217;d be willing to implement on the client.\n All these issues have largely been solved on the server-side by libraries such as Apache Lucene for quite some time. Using a library like Lucene means implementing a custom server-side process, though. How to deploy such service? Operating a VM 24/7 with my search backend for what\u0026#8217;s likely going to be not more than a few dozen queries per month seemed a bit like overkill.\n So after some consideration I decided to implement my own search functionality, based on the highly popular Apache Lucene library, deployed as a Serverless application, which is started on-demand if a user runs a query on my website. In the remainder of this post I\u0026#8217;m going to describe the solution I came up with and how it works.\n If you like, you can try it out right now, this post is about this little search input control at the top right of this page!\n     Why No External Search Service? When tweeting about my serverless search experiment, one of the questions was \"What\u0026#8217;s wrong with Algolia?\". To be very clear, there\u0026#8217;s nothing wrong with it at all. External search services like Algolia, Google Custom Search, or an Elasticsearch provider such as Bonsai promise an easy-to-use, turn-key search functionality which can be a great choice for your specific use case.\n However, I felt that none of these options would provide me the degree of control and customizability I was after. I also ruled out any \"free\" options, as they\u0026#8217;d either mean having ads or paying for the service with the data of myself or that of my readers. And to be honest, I also just fancied the prospect of solving the problem by myself, instead of relying on an off-the-shelf solution.\n     Why Serverless? First of all, let\u0026#8217;s discuss why I opted for a Serverless solution. It boils down to three reasons:\n   Security: While it\u0026#8217;d only cost a few EUR per month to set up a VM with a cloud provider like Digital Ocean or Hetzner, having to manage a full operating system installation would require too much of my attention; I don\u0026#8217;t want someone to mine bitcoins or doing other nasty things on a box I run, just because I failed to apply some security patch\n  Cost: Serverless does not only promise to scale-out (and let\u0026#8217;s be honest, there likely won\u0026#8217;t be millions of search queries on my blog every month), but also scale-to-zero. As Serverless is pay-per-use and there are free tiers in place e.g. for AWS Lambda, this service ideally should cost me just a few cents per month\n  Learning Opportunity: Last but not least, this also should be a nice occasion for me to dive into the world of Serverless, by means of designing, developing and running a solution for a real-world problem, exploring how Java as my preferred programming language can be used for this task\n     Solution Overview The overall idea is quite simple: there\u0026#8217;s a simple HTTP service which takes a query string, runs the query against a Lucene index with my blog\u0026#8217;s contents and returns the search results to the caller. This service gets invoked via JavaScript from my static blog pages, where results are shown to the user.\n The Lucene search index is read-only and gets rebuilt whenever I update the blog. It\u0026#8217;s baked into the search service deployment package, which that way becomes fully immutable. This reduces complexities and the attack surface at runtime. Surely that\u0026#8217;s not an approach that\u0026#8217;s viable for more dynamic use cases, but for a blog that\u0026#8217;s updated every few weeks, it\u0026#8217;s perfect. Here\u0026#8217;s a visualization of the overall flow:\n   The search service is deployed as a Serverless function on AWS Lambda. One important design goal for me is to avoid lock-in to any specific cloud provider: the solution should be portable and also be usable with container-based Serverless approaches like Knative.\n Relying on a Serverless architecture means its start-up time must be a matter of milli-seconds rather than seconds, so to not have a user wait for a noticeable amount of time in case of a cold start. While substantial improvements have been made in recent Java versions to improve start-up times, it\u0026#8217;s still not ideal for this kind of use case. Therefore, the application is compiled into a native binary via Quarkus and GraalVM, which results in a start-up time of ~30 ms on my laptop, and ~180 ms when deployed to AWS Lambda. With that we\u0026#8217;re in a range where a cold start won\u0026#8217;t impact the user experience in any significant way.\n The Lambda function is exposed to callers via the AWS API Gateway, which takes incoming HTTP requests, maps them to calls of the function and converts its response into an HTTP response which is sent back to the caller.\n Now let\u0026#8217;s dive down a bit more into the specific parts of the solution. Overall, there are four steps involved:\n   Data extraction: The blog contents to be indexed must be extracted and converted into an easy-to-process data format\n  Search backend implementation: A small HTTP service is needed which exposes the search functionality of Apache Lucene, which in particular requires some steps to enable Lucene being used in a native GraalVM binary\n  Integration with the website: The search service must be integrated into the static site on GitHub Pages\n  Deployment: Finally, the search service needs to be deployed to AWS API Gateway and Lambda\n     Data Extraction The first step was to obtain the contents of my blog in an easily processable format. Instead of requiring something like a real search engine\u0026#8217;s crawler, I essentially only needed to have a single file in a structured format which then can be passed on to the Lucene indexer.\n This task proved rather easy with Hugo; by means of a custom output format it\u0026#8217;s straight-forward to produce a JSON file which contains the text of all my blog pages. In my config.toml I declared the new output format and activate it for the homepage (largely inspired by this write-up):\n [outputFormats.SearchIndex] mediaType = \"application/json\" baseName = \"searchindex\" isPlainText = true notAlternative = true [outputs] home = [\"HTML\",\"RSS\", \"SearchIndex\"]   The template in layouts/_default/list.searchindex.json isn\u0026#8217;t too complex either:\n {{- $.Scratch.Add \"searchindex\" slice -}} {{- range $index, $element := .Site.Pages -}} {{- $.Scratch.Add \"searchindex\" (dict \"id\" $index \"title\" $element.Title \"uri\" $element.Permalink \"tags\" $element.Params.tags \"section\" $element.Section \"content\" $element.Plain \"summary\" $element.Summary \"publicationdate\" ($element.Date.Format \"Jan 2, 2006\")) -}} {{- end -}} {{- $.Scratch.Get \"searchindex\" | jsonify -}}   The result is this JSON file:\n [...{\"content\":\"The JDK Flight Recorder (JFR) is an invaluable tool...\",\"id\":12,\"publicationdate\":\"Jan 29, 2020\",\"section\":\"blog\",\"summary\":\"\\u003cdiv class=\\\"paragraph\\\"\\u003e\\n\\u003cp\\u003eThe \\u003ca href=\\\"https://openjdk.java.net/jeps/328\\\"\\u003eJDK Flight Recorder\\u003c/a\\u003e (JFR) is an invaluable tool...\",\"tags\":[\"java\",\"monitoring\",\"microprofile\",\"jakartaee\",\"quarkus\"],\"title\":\"Monitoring REST APIs with Custom JDK Flight Recorder Events\",\"uri\":\"https://www.morling.dev/blog/rest-api-monitoring-with-custom-jdk-flight-recorder-events/\"},...]   This file gets automatically updated whenever I republish the blog.\n   Search Backend Implementation My stack of choice for this kind of application is Quarkus. As a contributor, I am of course biased, but Quarkus is ideal for the task at hand: built and optimized from the ground up for implementing fast-starting and memory-efficient cloud-native and Serverless applications, it makes building HTTP services, e.g. based on JAX-RS, running on GraalVM a trivial effort.\n Now typically a Java library such as Lucene will not run in a GraalVM native binary out-of-the-box. Things like reflection or JNI usage require specific configuration, while other Java features like method handles are only supported partly or not at all.\n Apache Lucene in a GraalVM Native Binary Quarkus enables a wide range of popular Java libraries to be used with GraalVM, but at this point there\u0026#8217;s no extension yet which would take care of Lucene. So I set out to implement a small Quarkus extension for Lucene. Depending on the implementation details of the library in question, this can be a more or less complex and time-consuming endeavor. The workflow is like so:\n   compile down an application using the library into a native image\n  run into some sort of exception, e.g. due to types accessed via Java reflection (which causes the GraalVM compiler to miss them during call flow analysis so that they are missing from the generated binary image)\n  fix the issue e.g. by registering the types in question for reflection\n  rinse and repeat\n   The good thing there is that the list of Quarkus extensions is constantly growing, so that you hopefully don\u0026#8217;t have to go through this by yourself. Or if you do, consider publishing your extension via the Quarkus platform, saving others from the same work.\n For my particular usage of Lucene, I ran luckily into two issues only. The first is the usage of method handles in the AttributeFactory class for dynamically instantiating sub-classes of the AttributeImpl type, which isn\u0026#8217;t supported in that form by GraalVM. One way for dealing with this is to define substitutions, custom methods or classes which will override a specific original implementation. As an example, here\u0026#8217;s one of the substitution classes I had to create:\n @TargetClass(className = \"org.apache.lucene.util.AttributeFactory$DefaultAttributeFactory\") public final class DefaultAttributeFactorySubstitution { public DefaultAttributeFactorySubstitution() {} @Substitute public AttributeImpl createAttributeInstance(Class\u0026lt;? extends Attribute\u0026gt; attClass) { if (attClass == BoostAttribute.class) { return new BoostAttributeImpl(); } else if (attClass == CharTermAttribute.class) { return new CharTermAttributeImpl(); } else if (...) { ... } throw new UnsupportedOperationException(\"Unknown attribute class: \" + attClass); } }   During native image creation, the GraalVM compiler will discover all substitute classes and apply their code instead of the original ones.\n The other problem I ran into was the usage of method handles in the MMapDirectory class, which will be used by Lucene by default on Linux when obtaining a file-system backed index directory. I didn\u0026#8217;t explore how to circumvent that, instead I opted for using the SimpleFSDirectory implementation which proved to work fine in my native GraalVM binary.\n While this was enough in order to get Lucene going in a native image, you might run into different issues when using other libraries with GraalVM native binaries. Quarkus comes with a rich set of so-called build items which extension authors can use in order to enable external dependencies on GraalVM, e.g. for registering classes for reflective access or JNI, adding additional resources to the image, and much more. I recommend you take a look at the extension author guide in order to learn more.\n Besides enabling Lucene on GraalVM, that Quarkus extension also does two more things:\n   Parse the previously extracted JSON file, build a Lucene index from that and store that index in the file system; that\u0026#8217;s fairly standard Lucene procedure without anything noteworthy; I only had to make sure that the index fields are stored in their original form in the search index, so that they can be accessed at runtime when displaying fragments with the query hits\n  Register a CDI bean, which allows to obtain the index at runtime via @Inject dependency injection from within the HTTP endpoint class\n   A downside of creating binaries via GraalVM is the increased build time: creating a native binary for macOS via a locally installed GraalVM SDK takes about two minutes on my laptop. For creating a Linux binary to be used with AWS Lambda, I need to run the build in a Linux container, which takes about five minutes. But typically this task is only done once when actually deploying the application, whereas locally I\u0026#8217;d work either with the Quarkus Dev Mode (which does a live reload of the application as its code changes) or test on the JVM. In any case it\u0026#8217;s a price worth paying: only with start-up times in the range of milli-seconds on-demand Serverless cold starts with the user waiting for a response become an option.\n  The Search HTTP Service The actual HTTP service implementation for running queries is rather unspectacular; It\u0026#8217;s based on JAX-RS and exposes as simple endpoint which can be invoked with a given query like so:\n http \"https://my-search-service/search?q=java\" HTTP/1.1 200 OK Connection: keep-alive Content-Length: 4930 Content-Type: application/json Date: Tue, 21 Jul 2020 17:05:00 GMT { \"message\": \"ok\", \"results\": [ { \"fragment\": \"...plug-ins. In this post I\u0026amp;#8217;m going to explore how the \u0026lt;b\u0026gt;Java\u0026lt;/b\u0026gt; Platform Module System's notion of module layers can be leveraged for implementing plug-in architectures on the JVM. We\u0026amp;#8217;ll also discuss how Layrry, a launcher and runtime for layered \u0026lt;b\u0026gt;Java\u0026lt;/b\u0026gt; applications, can help with this task. A key requirement...\", \"publicationdate\": \"Apr 21, 2020\", \"title\": \"Plug-in Architectures With Layrry and the \u0026lt;b\u0026gt;Java\u0026lt;/b\u0026gt; Module System\", \"uri\": \"https://www.morling.dev/blog/plugin-architectures-with-layrry-and-the-java-module-system/\" }, { \"fragment\": \"...the current behavior indeed is not intended (see JDK-8236597) and in a future \u0026lt;b\u0026gt;Java\u0026lt;/b\u0026gt; version the shorter version of the code shown above should work. Wrap-Up In this blog post we\u0026amp;#8217;ve explored how invariants on \u0026lt;b\u0026gt;Java\u0026lt;/b\u0026gt; 14 record types can be enforced using the Bean Validation API. With just a bit...\", \"publicationdate\": \"Jan 20, 2020\", \"title\": \"Enforcing \u0026lt;b\u0026gt;Java\u0026lt;/b\u0026gt; Record Invariants With Bean Validation\", \"uri\": \"https://www.morling.dev/blog/enforcing-java-record-invariants-with-bean-validation/\" }, ... ] }   Internally it\u0026#8217;s using Lucene\u0026#8217;s MultiFieldQueryParser for parsing the query and running it against the \"title\" and \"content\" fields of the index. It is set to combine multiple terms using the logical AND operator by default (who ever would want the default of OR?), it supports phrase queries given in quotes, and a number of other query operators.\n Query hits are highlighted using the FastVectorHighlighter highlighter and SimpleHTMLFormatter as a fallback (not all kinds of queries can be processed by FastVectorHighlighter). The highlighter wraps the matched search terms in the returned fragment in \u0026lt;b\u0026gt; tags, which are styled appropriately in my website\u0026#8217;s CSS. I was prepared to do some adjustments to result scoring, but this wasn\u0026#8217;t necessary so far. Title matches are implicitly ranked higher than content matches due to the shorter length of the title field values.\n Implementing the service using a standard HTTP interface instead of relying on specific AWS Lambda contracts is great in terms of local testing as well as portability: I can work on the service using the Quarkus Dev Mode and invoke it locally, without having to deploy it into some kind of Lambda test environment. It also means that should the need arise, I can take this service and run it elsewhere, without requiring any code changes. As I\u0026#8217;ll discuss in a bit, Quarkus takes care of making this HTTP service runnable within the Lambda environment by means of a single dependency configuration.\n    Wiring Things Up Now it was time to hook up the search service into my blog. I wouldn\u0026#8217;t want to have the user navigate to the URL of the AWS API Gateway in their browser; this means that the form with the search text input field cannot actually be submitted. Instead, the default form handling must be disabled, and the search string be sent via JavaScript to the API Gateway URL.\n This means the search feature won\u0026#8217;t work for users who have JavaScript disabled in their browser. I deemed this an acceptable limitation; in order to avoid unnecessary confusion and frustration, the search text input field is hidden in that case via CSS:\n \u0026lt;noscript\u0026gt; \u0026lt;style type=\"text/css\"\u0026gt; .search-input { display:none; } \u0026lt;/style\u0026gt; \u0026lt;/noscript\u0026gt;   The implementation of the backend call is fairly standard JavaScript business using the XMLHttpRequest API, so I\u0026#8217;ll spare you the details here. You can find the complete implementation in my GitHub repo.\n There\u0026#8217;s one interesting detail to share though in terms of improving the user experience after a cold start. As mentioned above, the Quarkus application itself starts up on Lambda in about ~180 ms. Together with the initialization of the Lambda execution environment I typically see ~370 ms for a cold start. Add to that the network round-trip times, and a user will feel a slight delay. Nothing dramatical, but it doesn\u0026#8217;t have that snappy instant feeling you get when executing the search with a warm environment.\n Thinking about the typical user interaction though, the situation can be nicely improved: if a visitor puts the focus onto the search text input field, it\u0026#8217;s highly likely that they will submit a query shortly thereafter. We can take advantage of that and have the website send a small \"ping\" request right at the point when the input field obtains the focus. This gives us enough headstart to have the Lambda function being started before the actual query comes in. Here\u0026#8217;s the request flow of a typical interaction (the \"Other\" requests are CORS preflight requests):\n   Note how the search call is issued only a few hundred ms after the ping. Now you could beat this e.g. when navigating to the text field using your keyboard and if you were typing really fast. But most users will use their mouse or touchpad to put the cursor into the input, and then change to the keyboard to enter the query, which is time enough for this little trick to work.\n The analysis of the logs confirms that essentially all executed queries hit a warmed up Lambda function, making cold starts a non-issue. To avoid any unneeded warm-up calls, they are only done when entering the input field for the first time after loading the page, or when staying on the page for long enough, so that the Lambda might have shut down again due to lack of activity.\n Of course you\u0026#8217;ll be charged for the additional ping requests, but for the volume I expect, this makes no relevant difference whatsoever.\n   Deployment to AWS Lambda The last part of my journey towards a Serverless search function was deployment to AWS Lambda. I was exploring Heroku and Google Cloud Run as alternatives, too. Both allow you to deploy regular container images, which then are automatically scaled on demand. This results in great portability, as things hardly can get any more standard than plain Linux containers.\n With Heroku, cold start times proved problematic, though: I observed 5 - 6 seconds, which completely ruling it out. This wasn\u0026#8217;t a problem with Cloud Run, and it\u0026#8217;d surely work very well overall. In the end I went for AWS Lambda, as its entire package of service runtime, API Gateway and web application firewall seemed more complete and mature to me.\n With AWS Lambda, I observed cold start times of less than 0.4 sec for my actual Lambda function, plus the actual request round trip. Together with the warm-up trick described above, this means that a user practically never will get a cold start when executing the search.\n You shouldn\u0026#8217;t under-estimate the time needed though to get familiar with Lambda itself, the API Gateway which is needed for routing HTTP requests to your function and the interplay of the two.\n To get started, I configured some playground Lambda and API in the web console, but eventually I needed something along the lines of infrastructure-as-code, means of reproducible and automated steps for configuring and setting up all the required components. My usual go-to solution in this area is Terraform, but here I settled for the AWS Serverless Application Model (SAM), which is tailored specifically to setting up Serverless apps via Lambda and API Gateway and thus promised to be a bit easier to use.\n Building Quarkus Applications for AWS Lambda Quarkus supports multiple approaches for building Lambda-based applications:\n   You can directly implement Lambda\u0026#8217;s APIs like RequestHandler, which I wanted to avoid though for the sake of portability between different environments and cloud providers\n  You can use the Quarkus Funqy API for building portable functions which e.g. can be deployed to AWS, Azure Functions and Google Cloud Functions; the API is really straight-forward and it\u0026#8217;s a very attractive option, but right now there\u0026#8217;s no way to use Funqy for implementing an HTTP GET API with request parameters, which ruled out this option for my purposes\n  You can implement your Lambda function using the existing and well-known HTTP APIs of Vert.x, RESTEasy (JAX-RS) and Undertow; in this case Quarkus will take care of mapping the incoming function call to the matching HTTP endpoint of the application\n   Used together with the proxy feature of the AWS API Gateway, the third option is exactly what I was looking for. I can implement the search endpoint using the JAX-RS API I\u0026#8217;m familiar with, and the API Gateway proxy integration together with Quarkus' glue code will take care of everything else for running this. This is also great in terms of portability: I only need to add the io.quarkus:quarkus-amazon-lambda-http dependency to my project, and the Quarkus build will emit a function.zip file which can be deployed to AWS Lambda. I\u0026#8217;ve put this into a separate Maven build profile, so I can easily switch between creating the Lambda function deployment package and a regular container image with my REST endpoint which I can deploy to Knative and environments like OpenShift Serverless, without requiring any code changes whatsoever.\n The Quarkus Lambda extension also produces templates for the AWS SAM tool for deploying my stack. They are a good starting point which just needs a little bit of massaging; For the purposes of cost control (see further below), I added an API usage plan and API key. I also enabled CORS so that the API can be called from my static website. This made it necessary to disable the configuration of binary media types which the generated template contains by default. Lastly, I used a specific pre-configured execution role instead of the default AWSLambdaBasicExecutionRole.\n With the SAM descriptor in place, re-building and publishing the search service becomes a procedure of three steps:\n mvn clean package -Pnative,lambda -DskipTests=true \\ -Dquarkus.native.container-build=true sam package --template-file sam.native.yaml \\ --output-template-file packaged.yaml \\ --s3-bucket \u0026lt;my S3 bucket\u0026gt; sam deploy --template-file packaged.yaml \\ --capabilities CAPABILITY_IAM \\ --stack-name \u0026lt;my stack name\u0026gt;   The lambda profile takes care of adding the Quarkus Lambda HTTP extension, while the native profile makes sure that a native binary is built instead of a JAR to be run on the JVM. As I need to build a Linux binary for the Lambda function while running on macOS locally, I\u0026#8217;m using the -Dquarkus.native.container-build=true option, which will make the Quarkus build running in a container itself, producing a Linux binary no matter which platform this build itself is executed on.\n The function.zip file produced by the Quarkus build has a size of ~15 MB, i.e. it\u0026#8217;s uploaded and deployed to Lambda in a few seconds. Currently it also contains the Lucene search index, meaning I need to run the time-consuming GraalVM build whenever I want to update the index. As an optimization I might at some point extract the index into a separate Lambda layer, which then could be deployed by itself, if there were no code changes to the search service otherwise.\n  Identity and Access Management A big pain point for me was identity and access management (IAM) for the AWS API Gateway and Lambda. While the AWS IAM is really powerful and flexible, there\u0026#8217;s unfortunately no documentation, which would describe the minimum set of required permissions in order to deploy a stack like my search using SAM.\n Things work nicely if you use a highly-privileged account, but I\u0026#8217;m a strong believer into running things with only the least privileges needed for the job. For instance I don\u0026#8217;t want my Lambda deployer to set up the execution role, but rather have it using one I pre-defined. The same goes for other resources like the S3 bucket used for uploading the deployment package.\n Identifying the set of privileges actually needed is a rather soul-crushing experience of trial and error (please let me know in the comments below if there\u0026#8217;s a better way to do this), which gets complicated by the fact that different resources in the AWS stack expose insufficient privileges in inconsistent ways, or sometimes in no really meaningful way at all when configured via SAM. I spent hours identifying a lacking S3 privilege when trying to deploy a Lambda layer from the Serverless Application Repository.\n Hoping to spare others from this tedious work, here\u0026#8217;s the policy for my deployment role I came up with:\n {\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":[\"s3:PutObject\",\"s3:GetObject\"],\"Resource\":[\"arn:aws:s3:::\u0026lt;deployment-bucket\u0026gt;\",\"arn:aws:s3:::\u0026lt;deployment-bucket\u0026gt;/*\"]},{\"Effect\":\"Allow\",\"Action\":[\"lambda:CreateFunction\",\"lambda:GetFunction\",\"lambda:GetFunctionConfiguration\",\"lambda:AddPermission\",\"lambda:UpdateFunctionCode\",\"lambda:ListTags\",\"lambda:TagResource\",\"lambda:UntagResource\"],\"Resource\":[\"arn:aws:lambda:eu-central-1:\u0026lt;account-id\u0026gt;:function:search-morling-dev-SearchMorlingDev-*\"]},{\"Effect\":\"Allow\",\"Action\":[\"iam:PassRole\"],\"Resource\":[\"arn:aws:iam::\u0026lt;account-id\u0026gt;:role/\u0026lt;execution-role\u0026gt;\"]},{\"Effect\":\"Allow\",\"Action\":[\"cloudformation:DescribeStacks\",\"cloudformation:DescribeStackEvents\",\"cloudformation:CreateChangeSet\",\"cloudformation:ExecuteChangeSet\",\"cloudformation:DescribeChangeSet\",\"cloudformation:GetTemplateSummary\"],\"Resource\":[\"arn:aws:cloudformation:eu-central-1:\u0026lt;account-id\u0026gt;:stack/search-morling-dev/*\",\"arn:aws:cloudformation:eu-central-1:aws:transform/Serverless-2016-10-31\"]},{\"Effect\":\"Allow\",\"Action\":[\"apigateway:POST\",\"apigateway:PATCH\",\"apigateway:GET\"],\"Resource\":[\"arn:aws:apigateway:eu-central-1::/restapis\",\"arn:aws:apigateway:eu-central-1::/restapis/*\"]},{\"Effect\":\"Allow\",\"Action\":[\"apigateway:POST\",\"apigateway:GET\"],\"Resource\":[\"arn:aws:apigateway:eu-central-1::/usageplans\",\"arn:aws:apigateway:eu-central-1::/usageplans/*\",\"arn:aws:apigateway:eu-central-1::/apikeys\",\"arn:aws:apigateway:eu-central-1::/apikeys/search-morling-dev-apikey\"]}]}   Perhaps this could be trimmed down some more, but I felt it\u0026#8217;s good enough for my purposes.\n  Performance At this point I haven\u0026#8217;t conducted any systematic performance testing yet. There\u0026#8217;s definitely a significant difference in terms of latency between running things locally on my (not exactly new) laptop and on AWS Lambda. Where the app starts up in ~30 ms locally, it\u0026#8217;s ~180 ms when deployed to Lambda. Note this is only the number reported by Quarkus itself, the entire cold start duration of the application on Lambda, i.e. including the time required for fetching the code to the execution environment and starting the container, is ~370 ms (with 256 MB RAM assigned). Due to the little trick described above, though, a visitor is very unlikely to ever experience this delay when executing a query.\n Similarly, there\u0026#8217;s a substantial difference in terms of request execution duration. Still, when running a quick test of the deployed service via Siege, the vast majority of Lambda executions clocked in well below 100 ms (depending on the number of query hits which need result highlighting), putting them into the lowest bracket of billed Lambda execution time. As I learned, Lambda allocates CPU resources proportionally to assigned RAM, meaning assigning twice as much RAM should speed up execution, also if my application actually does not need that much memory. Indeed, with 512 MB RAM assigned, Lambda execution is down to ~30 - 40 ms after some warm-up, which is more than good enough for my purposes.\n Raw Lambda execution of course is only one part of the overall request duration, on top of that some time is spent in the API Gateway and on the wire to the user; The service is deployed in the AWS eu-central-1 region (Frankfurt, Germany), yielding roundtrip times for me, living a few hundred km away, between 50 - 70 ms (again with 512 MB RAM). With longer distances, network latencies outweigh the Lambda execution time: My good friend Eric Murphy from Seattle in the US reported a roundtrip time of ~240 ms when searching for \"Java\", which I think is still quite good, given the long distance.\n  Cost Control The biggest issue for me as a hobbyist when using pay-per-use services like AWS Lambda and API Gateway is cost control. Unlike typical enterprise scenarios where you might be willing to accept higher cost for your service in case of growing demand, in my case I\u0026#8217;d rather set up a fixed spending limit and shut down my search service for the rest of the month, once that has been reached. I absolutely cannot have an attacker doing millions and millions of calls against my API which could cost me a substantial amount of money.\n Unfortunately, there\u0026#8217;s no easy way on AWS for setting up a maximum spending after which all service consumption would be stopped. Merely setting up a budget alert won\u0026#8217;t cut it either, as this won\u0026#8217;t help me while sitting on a plane for 12h (whenever that will be possible again\u0026#8230;\u0026#8203;) or being on vacation for three weeks. And needless to say, I don\u0026#8217;t have an ops team monitoring my blog infrastructure 24/7 either.\n So what to do to keep costs under control? An API usage plan is the first part of the answer. It allows you to set up a quota (maximum number of calls in a given time frame) which is pretty much what I need. Any calls beyond the quota are rejected by the API Gateway and not charged.\n There\u0026#8217;s one caveat though: a usage plan is tied to an API key, which the caller needs to pass using the X-API-Key HTTP request header. The idea being that different usage plans can be put in place for different clients of an API. Any calls without the API key are not charged either. Unfortunately though this doesn\u0026#8217;t play well with CORS preflight requests as needed in my particular use case. Such requests will be sent by the browser before the actual GET calls to validate that the server actually allows for that cross-origin request. CORS preflight requests cannot have any custom request headers, though, meaning they cannot be part of a usage plan. The AWS docs are unclear whether those preflight requests are charged or not, and in a way it seems unfair if they were charged given there\u0026#8217;s no way to prevent this situation. But at this point it is fair to assume they are charged and we need a way to prevent having to pay for a gazillion preflight calls by a malicious actor.\n In good software developer\u0026#8217;s tradition I turned to Stack Overflow for finding help, and indeed I received a nice idea: A budget alert can be linked with an SNS topic, to which a message will be sent once the alert triggers. Then another Lambda function can be used to set the allowed rate of API invocations to 0, effectively disabling the API, preventing any further cost to pile up. A bit more complex than I was hoping for, but it does the trick. Thanks a lot to Harish for providing this nice answer on Stack Overflow and his blog! I implemented this solution and sleep much better now.\n Note that you should set the alert to a lower value than what you\u0026#8217;re actually willing to spend, as billing happens asynchronously and requests might come in some more time until the alert triggers: as per Corey Quinn, there\u0026#8217;s an \"8-48 hour lag between 'you incur the charge' and 'it shows up in the billing system where an alert can see it and thus fire'\". It\u0026#8217;s therefore also a good idea to reduce the allowed request rate. E.g. in my case I\u0026#8217;m not expecting really that there\u0026#8217;d be more than let\u0026#8217;s say 25 concurrent requests (unless this post hits the Hackernews front page of course), so setting the allowed rate to that value helps to at least slow down the spending until the alert triggers.\n With these measures in place, there should (hopefully!) be no bad surprises at the end of the month. Assuming a (very generously estimated) number of 10K search queries per month, each returning a payload of 5 KB, I\u0026#8217;d be looking at an invoice over EUR 0.04 for the API Gateway, while the Lambda executions would be fully covered by the AWS free tier. That seems manageable :)\n    Wrap-Up and Outlook Having rolled out the search feature for this blog a few days ago, I\u0026#8217;m really happy with the outcome. It was a significant amount of work to put everything together, but I think a custom search is a great addition to this site which hopefully proves helpful to my readers. Serverless is a perfect architecture and deployment option for this use case, being very cost-efficient for the expected low volume of requests, and providing a largely hands-off operations experience for myself.\n With AOT compilation down to native binaries and enabling frameworks like Quarkus, Java definitely is in the game for building Serverless apps. Its huge eco-system of libraries such as Apache Lucene, sophisticated tooling and solid performance make it a very attractive implementation choice. Basing the application on Quarkus makes it a matter of configuration to switch between creating a deployment package for Lambda and a regular container image, avoiding any kind of lock-in into a specific platform.\n Enabling libraries for being used in native binaries can be a daunting task, but over time I\u0026#8217;d expect either library authors themselves to do the required adjustment to smoothen that experience, and of course the growing number of Quarkus extensions also helps to use more and more Java libraries in native apps. I\u0026#8217;m also looking forward to Project Leyden, which aims at making AOT compilation a part of the Java core platform.\n The deployment to AWS Lambda and API Gateway was definitely more involved than I had anticipated; things like IAM and budget control are more complex than I think they could and should be. That there is no way to set up a hard spend capping is a severe shortcoming; hobbyists like myself should be able to explore this platform without having to fear any surprise AWS bills. It\u0026#8217;s particular bothersome that API usage plans are no 100% safe way to enforce API quotas, as they cannot be applied to unauthorized CORS pref-flight requests and custom scripting is needed in order to close this loophole.\n But then this experiment also was an interesting learning experience for me; working on libraries and integration solutions most of the time during my day job, I sincerely enjoyed the experience of designing a service from the ground-up and rolling it out into \"production\", if I may dare to use that term here.\n While the search functionality is rolled out on my blog, ready for you to use, there\u0026#8217;s a few things I\u0026#8217;d like to improve and expand going forward:\n   CI pipeline: Automatically re-building and deploying the search service after changes to the contents of my blog; this should hopefully be quite easy using GitHub Actions\n  Performance improvements: While the performance of the query service definitely is good enough, I\u0026#8217;d like to see whether and how it could be tuned here and there. Tooling might be challenging there; where I\u0026#8217;d use JDK Flight Recorder and Mission Control with a JVM based application, I\u0026#8217;m much less familiar with equivalent tooling for native binaries. One option I\u0026#8217;d like to explore in particular is taking advantage of Quarkus bytecode recording capability: bytecode instructions for creating the in-memory data structure of the Lucene index could be recorded at build time and then just be executed at application start-up; this might be the fastest option for loading the index in my special use case of a read-only index\n  Serverless comments: Currently I\u0026#8217;m using Disqus for the commenting feature of the blog. It\u0026#8217;s not ideal in terms of privacy and page loading speed, which is why I\u0026#8217;m looking for alternatives. One idea could be a custom Serverless commenting functionality, which would be very interesting to explore, in particular as it shifts the focus from a purely immutable application to a stateful service that\u0026#8217;ll require some means of modifiable, persistent storage\n   In the meantime, you can find the source code of the Serverless search feature on GitHub. Feel free to take the code and deploy it to your own website!\n Many thanks to Hans-Peter Grahsl and Eric Murphy for their feedback while writing this post!\n  ","id":36,"publicationdate":"Jul 29, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003e\u003cem\u003eI have built a custom search functionality for this blog,\nbased on Java and the Apache Lucene full-text search library,\ncompiled into a native binary using the Quarkus framework and GraalVM.\nIt is deployed as a Serverless application running on AWS Lambda,\nproviding search results without any significant cold start delay.\nIf you thought Java wouldn\u0026#8217;t be the right language for this job, keep reading;\nin this post I\u0026#8217;m going to give an overview over the implementation of this feature and my learnings along the way.\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"How I Built a Serverless Search for My Blog","uri":"https://www.morling.dev/blog/how-i-built-a-serverless-search-for-my-blog/"},{"content":"Ahead-of-time compilation (AOT) is the big topic in the Java ecosystem lately: by compiling Java code to native binaries, developers and users benefit from vastly improved start-up times and reduced memory usage. The GraalVM project made huge progress towards AOT-compiled Java applications, and Project Leyden promises to standardize AOT in a future version of the Java platform.\n This makes it easy to miss out on significant performance improvements which have been made on the JVM in recent Java versions, in particular when it comes to faster start-up times. Besides a range of improvements related to class loading, linking and bytecode verification, substantial work has been done around class data sharing (CDS). Faster start-ups are beneficial in many ways: shorter turnaround times during development, quicker time-to-first-response for users in coldstart scenarios, cost savings when billed by CPU time in the cloud.\n With CDS, class metadata is persisted in an archive file, which during subsequent application starts is mapped into memory. This is faster than loading the actual class files, resulting in reduced start-up times. When starting multiple JVM processes on the same host, read-only archives of class metadata can also be shared between the VMs, so that less memory is consumed overall.\n Originally a partially commercial feature of the Oracle JDK, CDS was completely open-sourced in JDK 10 and got incrementally improved since then in a series of Java improvement proposals:\n   JEP 310, Application Class-Data Sharing (AppCDS), in JDK 10: \"To improve startup and footprint, extend the existing [CDS] feature to allow application classes to be placed in the shared archive\"\n  JEP 341, Default CDS Archives, in JDK 12: \"Enhance the JDK build process to generate a class data-sharing (CDS) archive, using the default class list, on 64-bit platforms\"\n  JEP 350, Dynamic CDS Archives, in JDK 13: \"Extend application class-data sharing to allow the dynamic archiving of classes at the end of Java application execution. The archived classes will include all loaded application classes and library classes that are not present in the default, base-layer CDS archive\"\n   In the remainder of this blog post we\u0026#8217;ll discuss how to automatically create AppCDS archives as part of your (Maven) project build, based on the improvements made with JEP 350. I.e. Java 13 or later is a prerequisite for this. To learn more about using CDS with the current LTS release JDK 11 and about CDS in general, refer to the excellent blog post on everything CDS by Nicolai Parlog.\n Manually Creating CDS Archives At first let\u0026#8217;s see what\u0026#8217;s needed to manually create and use an AppCDS archive (note I\u0026#8217;m going to use \"AppCDS\" and \"CDS\" somewhat interchangeably for the sake of brevity). Subsequently, we\u0026#8217;ll discuss how the task can be automated in a Maven project build.\n To have an example to work with which goes beyond a plain \"Hello World\", I\u0026#8217;ve created a small web application for managing personal to-dos, using the Quarkus stack. If you\u0026#8217;d like to follow along, clone the repo and build the project:\n git clone git@github.com:gunnarmorling/quarkus-cds.git cd quarkus-cds mvn clean verify -DskipTests=true   The application uses a Postgres database for persisting the to-dos; fire it up via Docker:\n cd compose docker run -d -p 5432:5432 --name pgdemodb \\ -v $(pwd)/init.sql:/docker-entrypoint-initdb.d/init.sql \\ -e POSTGRES_USER=todouser \\ -e POSTGRES_PASSWORD=todopw \\ -e POSTGRES_DB=tododb postgres:11   The next step is to run the application and create the CDS archive file. Do so by passing the -XX:ArchiveClassesAtExit option:\n java -XX:ArchiveClassesAtExit=target/app-cds.jsa \\ (1) -jar target/todo-manager-1.0.0-SNAPSHOT-runner.jar     1 Triggers creation of a CDS archive at the given location upon application shutdown    Only loaded classes will be added to the archive. As classloading on the JVM happens lazily, you must invoke some functionality in your application in order to cause all the relevant classes to be loaded. For that to happen, open the application\u0026#8217;s API endpoint in a browser or invoke it via curl, httpie or similar:\n http localhost:8080/api   Stop the application by hitting Ctrl+C. This will create the CDS archive under target/app-cds.jsa. In our case it should have a size of about 41 MB. Also observe the log messages about classes which were skipped from archiving:\n ... [190.220s][warning][cds] Skipping java/lang/invoke/LambdaForm$MH+0x0000000800bd0c40: Hidden or Unsafe anonymous class [190.220s][warning][cds] Skipping java/lang/invoke/LambdaForm$DMH+0x0000000800fdc840: Hidden or Unsafe anonymous class [190.220s][warning][cds] Pre JDK 6 class not supported by CDS: 46.0 antlr/TokenStreamIOException ...   Mostly this is about hidden or anonymous classes which cannot be archived; there\u0026#8217;s not so much you can do about that (apart from using less Lambda expressions perhaps\u0026#8230;\u0026#8203;).\n The hint on old classfile versions is more actionable: only classes using classfile format 50 (= JDK 1.6) or newer are supported by CDS. In the case at hand, the classes from Antlr 2.7.7 are using classfile format 46 (which was introduced in Java 1.2) and thus cannot be added to the CDS archive. Note this also applies to any subclasses, even if they themselves use a newer classfile format version.\n It\u0026#8217;s thus a good idea to check whether you can upgrade to newer versions of your dependencies, as this may result in more classes becoming available for CDS, resulting in better start-up times in turn.\n   Using the CDS Archive Now let\u0026#8217;s run the application again, this time using the previously created CDS archive:\n java -XX:SharedArchiveFile=target/app-cds.jsa \\ (1) -Xlog:class+load:file=target/classload.log \\ (2) -Xshare:on \\ (3) -jar target/todo-manager-1.0.0-SNAPSHOT-runner.jar     1 The path to the CDS archive   2 classloading logging allows to verify whether the CDS archive gets applied as expected   3 While class data sharing is enabled by default on JDK 12 and newer, explicitely enforcing it will ensure an error is raised if something is wrong, e.g. a mismatch of Java versions between building and using the archive    When examining the classload.log file, you should see how most class metadata is obtained from the CDS archive (\"source: shared object file\"), while some classes such as the ancient Antlr classes are loaded just as usual from the corresponding JAR:\n [0.016s][info][class,load] java.lang.Object source: shared objects file [0.016s][info][class,load] java.io.Serializable source: shared objects file [0.016s][info][class,load] java.lang.Comparable source: shared objects file [0.016s][info][class,load] java.lang.CharSequence source: shared objects file ... [2.555s][info][class,load] antlr.Parser source: file:/.../antlr.antlr-2.7.7.jar ...   Note it is vital that the exact same Java version is used as when creating the archive, otherwise an error will be raised. Unfortunately, this also means that AppCDS archives cannot be built cross-platform. This would be very useful, e.g. when building a Java application on macOS or Windows, which should be packaged in a Linux container. If you are aware of a way for doing so, please let me know in the comments below.\n     CDS and the Java Module System Beginning with Java 11, not only classes from the classpath can be added to CDS archives, but also classes from the module path of a modularized Java application. One important detail to consider there is that the --upgrade-module-path and --patch-module options will cause CDS to be disabled or disallowed (with -Xshare:on) is specified. This is to avoid a mismatch of class metadata in the CDS archive and classes brought in by a newer module version.\n       Creating CDS Archives in Your Maven Build Manually creating a CDS archive is not very efficient nor reliable, so let\u0026#8217;s see how the task can be automated as part of your project build. The following shows the required configuration when using Apache Maven, but of course the same approach could be implemented with Gradle or any other build system.\n The basic idea is the follow the same steps as before, but executed as part of the Maven build:\n  start up the application with the -XX:ArchiveClassesAtExit option\n  invoke some application functionality to initiate the loading of all relevant classes\n  stop the application\n       It might appear as a compelling idea to produce the CDS archive as part of regular test execution, e.g. via JUnit. This will not work though, as the classpath at the time of using the CDS archive must be not miss any entries from the classpath at the time of creating it. As during test execution all the test-scoped dependencies will be part of the classpath, any CDS archive created that way couldn\u0026#8217;t be used when running the application later on without those test dependencies.\n     Steps 1. and 3. can be automated with help of the Process-Exec Maven plug-in, binding it to the pre-integration-test and post-integration-test build phases, respectively. While I was thinking of using the more widely known Exec plug-in initially, this turned out to not be viable as there\u0026#8217;s no way for stopping any forked process in a later build phase.\n Here\u0026#8217;s the relevant configuration:\n ... \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;com.bazaarvoice.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;process-exec-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.9\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; (1) \u0026lt;id\u0026gt;app-cds-creation\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;pre-integration-test\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;start\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;name\u0026gt;todo-manager\u0026lt;/name\u0026gt; \u0026lt;healthcheckUrl\u0026gt;http://localhost:8080/\u0026lt;/healthcheckUrl\u0026gt; (2) \u0026lt;arguments\u0026gt; \u0026lt;argument\u0026gt;java\u0026lt;/argument\u0026gt; (3) \u0026lt;argument\u0026gt;-XX:ArchiveClassesAtExit=app-cds.jsa\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;-jar\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt; ${project.build.directory}/${project.artifactId}-${project.version}-runner.jar \u0026lt;/argument\u0026gt; \u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;execution\u0026gt; (4) \u0026lt;id\u0026gt;stop-all\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;post-integration-test\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;stop-all\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; ...     1 Start up the application in the pre-integration-test build phase   2 The health-check URL is used to await application start-up before proceeding with the next build phase   3 Assemble the java invocation   4 Stop the application in the post-integration-test build phase    What remains to be done is the automation of step 2, the invocation of the required application logic so to trigger the loading of all relevant classes. This can be done with help of the Maven Surefire plug-in. A simple \"integration test\" via REST Assured does the trick:\n public class ExampleResourceAppCds { @Test public void getAll() { given() .when() .get(\"/api\") .then() .statusCode(200); } }   We just need to configure a specific execution of the plug-in, which only picks up any test classes whose names end with *AppCds.java, so to keep them apart from actual integration tests:\n ... \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-failsafe-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0-M4\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;integration-test\u0026lt;/goal\u0026gt; \u0026lt;goal\u0026gt;verify\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;includes\u0026gt; \u0026lt;include\u0026gt;**/*AppCds.java\u0026lt;/include\u0026gt; \u0026lt;/includes\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; ...   And that\u0026#8217;s all we need; when now building the project via mvn clean verify, a CDS archive will be created at target/app-cds.jsa. You can find the complete example project and steps for building/running it on GitHub.\n   What Do You Gain? Creating a CDS archive is nice, but is it also worth the effort? In order to answer this question, I\u0026#8217;ve done some measurements of the \"time-to-first-response\" metric, following the Quarkus guide on measuring performance. I.e. instead of awaiting some rather meaningless \"start-up complete\" status, which could arbitrarily be tweaked by means of lazy initialization, this measures the time until the application is actually ready to handle the first incoming request after start-up.\n I\u0026#8217;ve done measurements on OpenJDK 1.8.0_252 (AdoptOpenJDK build), OpenJDK 14.0.1 (upstream build, without and with AppCDS), and OpenJDK 15-ea-b26 (upstream build, with AppCDS). Please see the README file of the example repo for the exact steps.\n Here are the numbers, averaged over ten runs each:\n   Update, June 12th: I had originally classload logging enabled for the OpenJDK 14 AppCDS runs, which added an unneccessary overhead (thanks a lot to Claes Redestad for pointing this out!). The numbers and chart have been updated accordingly. I\u0026#8217;ve also added numbers for OpenJDK 15-ea.\n Time-to-first-response values are 2s 267ms, 2s 162ms, 1s 669ms 1s 483ms, and 1s 279ms. I.e. on my machine (2014 MacBook Pro), with this specific workload, there\u0026#8217;s an improvement of ~100ms just by upgrading to the current JDK, and of another ~500ms ~700ms by using AppCDS.\n With OpenJDK 15 things will further improve. The latest EA build at the time of writing (b26) shortens time-to-first-response by another ~200ms. The upcoming EA build 27 should bring another improvement, as Lambda proxy classes will be added to AppCDS archives then.\n That all is definitely a nice improvement, in particular as we get it essentially for free, without any changes to the actual application itself. You should contrast this with the additional size of the application distribution, though. E.g. when obtaining the application as a container image from a remote container registry, downloading the additional ~40 MB might take longer than the time saved during application start-up. Typically, this will only affect the first start-up of on a particular node, though, after which the image will be cached locally.\n As always when it comes to any kinds of performance numbers, please take these numbers with a grain of salt, do your own measurements, using your own applications and in your own environment.\n     Addressing Different Workload Profiles If your application supports different \"work modes\", e.g. \"online\" and \"batch\", which work with a largely differing set of classes, you also might consider to create different CDS archives for the specific workloads. This might give you a good balance between additional size and realized improvements of start-up times, when for instance dealing with at large monolithic application instead of more fine-grained microservices.\n       Wrap-Up AppCDS provides Java developers with a useful tool for reducing start-up times of their applications, without requiring any code changes. For the example discussed, we could observe an improvement of the time-to-first-response metric by about 30% when running with OpenJDK 14. Other users reported even bigger improvements.\n We didn\u0026#8217;t discuss any potential memory improvements due to CDS when sharing class metadata between multiple JVMs on one host. In containerized server applications, with each JVM being packaged in its own container image, this won\u0026#8217;t play a role. It could make a difference on desktop systems, though. For instance multiple instances of the Java language server, as leveraged by VSCode and other editors, could benefit from that.\n That all being said, when raw start-up time is your primary concern, e.g. in a serverless or Function-based setting, you should look at AOT compilation with GraalVM (or Project Leyden in the future). This will bring down start-up times to a completely different level; for example the todo manager application would return a first response within a few 10s of milliseconds when executed as a native image via GraalVM.\n But AOT is not always an option, nor does it always make sense: the JVM may offer a better latency than native binaries, external dependencies migh not be ready for usage in AOT-compiled native images yet, or you simply might want to be able to benefit from all the JVM goodness, like familiar debugging tools, the JDK Flight Recorder, or JMX. In that case, CDS can give you a nice start-up time improvement, solely by means of adding a few steps to your build process.\n Besides class data sharing in OpenJDK, there are some other related techniques for improving start-up times which are worth exploring:\n   Eclipse OpenJ9 has its own implementation of class data sharing\n  Alibaba\u0026#8217;s Dragonwell distribution of the OpenJDK comes with JWarmUp, a tool for speeding up initial JIT compilations\n   To learn more about AppCDS, a long yet insightful post is this one by Vladimir Plizga. Volker Simonis did another interesting write-up. Also take a look at the CDS documentation in the reference docs of the java command.\n Lastly, the Quarkus team is working on out-of-the-box support for CDS archives. This could fully automate the creation of an archive for all required classes without any further configuration, making it even easier to benefit from the start-up time improvements promised by CDS.\n  ","id":37,"publicationdate":"Jun 11, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eAhead-of-time compilation (AOT) is \u003cem\u003ethe\u003c/em\u003e big topic in the Java ecosystem lately:\nby compiling Java code to native binaries, developers and users benefit from vastly improved start-up times and reduced memory usage.\nThe \u003ca href=\"https://www.graalvm.org/\"\u003eGraalVM\u003c/a\u003e project made huge progress towards AOT-compiled Java applications,\nand \u003ca href=\"https://mail.openjdk.java.net/pipermail/discuss/2020-April/005429.html\"\u003eProject Leyden\u003c/a\u003e promises to standardize AOT in a future version of the Java platform.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThis makes it easy to miss out on significant performance improvements which have been made on the JVM in recent Java versions,\nin particular when it comes to \u003ca href=\"https://cl4es.github.io/2019/11/20/OpenJDK-Startup-Update.html\"\u003efaster start-up times\u003c/a\u003e.\nBesides a range of improvements related to class loading, linking and bytecode verification,\nsubstantial work has been done around \u003ca href=\"https://docs.oracle.com/en/java/javase/14/vm/class-data-sharing.html\"\u003eclass data sharing\u003c/a\u003e (CDS).\nFaster start-ups are beneficial in many ways:\nshorter turnaround times during development,\nquicker time-to-first-response for users in coldstart scenarios,\ncost savings when billed by CPU time in the cloud.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eWith CDS, class metadata is persisted in an archive file,\nwhich during subsequent application starts is mapped into memory.\nThis is faster than loading the actual class files, resulting in reduced start-up times.\nWhen starting multiple JVM processes on the same host, read-only archives of class metadata can also be shared between the VMs, so that less memory is consumed overall.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Building Class Data Sharing Archives with Apache Maven","uri":"https://www.morling.dev/blog/building-class-data-sharing-archives-with-apache-maven/"},{"content":"Do you remember Angus \"Mac\" MacGyver? The always creative protagonist of the popular 80ies/90ies TV show, who could solve about any problem with nothing more than a Swiss Army knife, duct tape, shoe strings and a paper clip?\n The single message transformations (SMTs) of Kafka Connect are almost as versatile as MacGyver\u0026#8217;s Swiss Army knife:\n   How to change the timezone or format of date/time message fields?\n  How to change the topic a specific message gets sent to?\n  How to filter out specific records?\n   SMTs can be the answer to these and many other questions that come up in the context of Kafka Connect. Applied to source or sink connectors, SMTs allow to modify Kafka records before they are sent to Kafka, or after they are consumed from a topic, respectively.\n In this post I\u0026#8217;d like to focus on some interesting (hopefully anyways) usages of SMTs. Those use cases are mostly based on my experiences from using Kafka Connect with Debezium, an open-source platform for change data capture (CDC). I also got some great pointers on interesting SMT usages when asking the community about this on Twitter some time ago:\n    I definitely recommend to check out the thread; thanks a lot to all who replied! In order to learn more about SMTs in general, how to configure them etc., refer to the resources given towards the end of this post.\n For each category of use cases, I\u0026#8217;ve also asked our sympathetic TV hero for his opinion on the usefulness of SMTs for the task at hand. You can find his rating at the end of each section, ranging from 📎 (poor fit) to 📎📎📎📎📎 (perfect fit).\n Format Conversions Probably the most common application of SMTs is format conversion, i.e. adjustments to type, format and representation of data. This may apply to entire messages, or to specific message attributes. Let\u0026#8217;s first look at a few examples for converting individual message attribute formats:\n   Timestamps: Different systems tend to have different assumptions of how timestamps should be typed and formatted. Debezium for instance represents most temporal column types as milli-seconds since epoch. Change event consumers on the other hand might expect such date and time values using Kafka Connect\u0026#8217;s Date type, or as an ISO-8601 formatted string, potentially using a specific timezone\n  Value masking: Sensitive data might have be to masked or truncated, or specific fields should even be removed altogether; the org.apache.kafka.connect.transforms.MaskField and ReplaceField SMTs shipping with Kafka Connect out of the box come in handy for that\n  Numeric types: Similar to timestamps, requirements around the representation of (decimal) numbers may differ between systems; e.g. Kafka Connect\u0026#8217;s Decimal type allows to convey arbitrary-precision decimals, but its binary representation of numbers might not be supported by all sink connectors and consumers\n  Name adjustments: Depending on the chosen serialization formats, specific field names might be unsupported; when working with Apache Avro for instance, field names must not start with a number\n   In all these cases, either existing, ready-made SMTs or bespoke implementations can be used to apply the required attribute type and/or format conversions.\n When using Kafka Connect for integrating legacy services and databases with newly built microservices, such format conversions can play an important role for creating an anti-corruption layer: by using better field names, choosing more suitable data types or by removing unneeded fields, SMTs can help to shield a new service\u0026#8217;s model from the oddities and quirks of the legacy world.\n But SMTs cannot only modify the representation of single fields, also the format and structure of entire messages can be adjusted. E.g. Kafka Connect\u0026#8217;s ExtractField transformation allows to extract a single field from a message and propagate that one. A related SMT is Debezium\u0026#8217;s SMT for change event flattening. It can be used to convert the complex Debezium change event structure with old and new row state, metadata and more, into a flat row representation, which can be consumed by many existing sink connectors.\n SMTs also allow to fine-tune schema namespaces; that can be of interest when working with a schema registry for managing schemas and their versions, and specific schema namespaces should be enforced for the messages on given topics. Two more, very useful examples of SMTs in this category are kafka-connect-transform-xml and kafka-connect-json-schema by Jeremy Custenborder, which will take XML or text and produce a typed Kafka Connect Struct, based on a given XML schema or JSON schema, respectively.\n Lastly, as a special kind of format conversion, SMTs can be used to modify or set the key of Kafka records. This may be desirable if a source connector doesn\u0026#8217;t produce any meaningful key, but one can be extracted from the record value. Also changing the message key can be useful, when considering subsequent stream processing. Choosing matching keys right at the source side e.g. allows for joining multiple topics via Kafka Streams, without the need for re-keying records.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎📎\u0026nbsp;\u0026nbsp; SMTs are the perfect tool for format conversions of Kafka Connect records\n   Ensuring Backwards Compatibility Changes to the schema of Kafka records can potentially be disruptive for consumers. If for instance a record field gets renamed, a consumer must be adapted accordingly, reading the value using the new field name. In case a field gets dropped altogether, consumers must not expect this field any longer.\n Message transformations can help with such transition from one schema version to the next, thus reducing the coupling of the lifecycles of message producers and consumers. In case of a renamed field, an SMT could add the field another time, using the original name. That\u0026#8217;ll allow consumers to continue reading the field using the old name and to be upgraded to use the new name at their own pace. After some time, once all consumers have been adjusted, the SMT can be removed again, only exposing the new field name going forward. Similarly, a field that got removed from a message schema could be re-added, e.g. using some sort of constant placeholder value. In other cases it might be possible to derive the field value from other, still existing fields. Again consumers could then be updated at their own pace to not expect and access that field any longer.\n It should be said though that there are limits for this usage: e.g. when changing the type of a field, things quickly become tricky. One option could be a multi-step approach where at first a separate field with the new type is added, before renaming it again as described above.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎\u0026nbsp;\u0026nbsp; SMTs can primarily help to address basic compatibility concerns around schema evolution\n   Filtering and Routing When applied on the source side, SMTs allow to filter out specific records produced by the connector. They also can be used for controlling the Kafka topic a record gets sent to. That\u0026#8217;s in particular interesting when filtering and routing is based on the actual record contents. In an IoT scenario for instance where Kafka Connect is used to ingest data from some kind of sensors, an SMT might be used to filter out all sensor measurements below a certain threshold, or route measurement events above a threshold to a special topic.\n Debezium provides a range of SMTs for record filtering and routing:\n   The logical topic routing SMT allows to send change events originating from multiple tables to the same Kafka topic, which can be useful when working with partition tables in Postgres, or with data that is sharded into multiple tables\n  The Filter and ContentBasedRouter SMTs let you use script expressions in languages such as Groovy or JavaScript for filtering and routing change events based on their contents; such script-based approach can be an interesting middleground between ease-of-use (no Java code must be compiled and deployed to Kafka Connect) and expressiveness; e.g. here is how the routing SMT could be used with GraalVM\u0026#8217;s JavaScript engine for routing change events from a table with purchase orders to different topics in Kafka, based on the order type:\n... transforms=route transforms.route.type=io.debezium.transforms.ContentBasedRouter transforms.route.topic.regex=.*purchaseorders transforms.route.language=jsr223.graal.js transforms.route.topic.expression= value.after.ordertype == 'B2B' ? 'b2b_orders' : 'b2c_orders' ...     The outbox event router comes in handy when implementing the transactional outbox pattern for data propagation between microservices: it can be used to send events originating from a single outbox table to a specific Kafka topic per aggregate (when thinking of domain driven design) or event type\n   There are also two SMTs for routing purposes in Kafka Connect itself: RegexRouter which allows to re-route records two different topics based on regular expressions, and TimestampRouter for determining topic names based on the record\u0026#8217;s timestamp.\n While routing SMTs usually are applied to source connectors (defining the Kafka topic a record gets sent to), it can also make sense to use them with sink connectors. That\u0026#8217;s the case when a sink connector derives the name of downstream table names, index names or similar from the topic name.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎📎\u0026nbsp;\u0026nbsp; Message filtering and topic routing\u0026#8201;\u0026#8212;\u0026#8201;no problem for SMTs\n   Tombstone Handling Tombstone records are Kafka records with a null value. They carry special semantics when working with compacted topics: during log compaction, all records with the same key as a tombstone record will be removed from the topic.\n Tombstones will be retained on a topic for a configurable time before compaction happens (controlled via delete.retention.ms topic setting), which means that also Kafka Connect sink connectors need to handle them. Unfortunately though, not all connectors are prepared for records with a null value, typically resulting in NullPointerExceptions and similar. A filtering SMT such as the one above can be used to drop tombstone records in such case.\n But also the exact opposite\u0026#8201;\u0026#8212;\u0026#8201;producing tombstone records\u0026#8201;\u0026#8212;\u0026#8201;can be useful: some sink connectors use tombstone records as the indicator to delete corresponding rows from a downstream datastore. Now when using a CDC connector like Debezium to capture changes from a database where \"soft deletes\" are used (i.e. records are not physically deleted, but a logically deleted flag is set to true when deleting a record), those change events will be exported as update events (which they technically are). A bespoke SMT can be used to translate these update events into tombstone records, triggering the deletion of corresponding records in downstream datastores.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎\u0026nbsp;\u0026nbsp; SMTs work well to discard tombstones or convert soft delete events into tombstones. What\u0026#8217;s not possible though is to keep the original event and produce an additional tombstone record at the same time\n   Externalizing Large Payloads Even some advanced enterprise application patterns can be implemented with the help of SMTs, one example being the claim check pattern. This pattern comes in handy in situations like this:\n  A message may contain a set of data items that may be needed later in the message flow, but that are not necessary for all intermediate processing steps. We may not want to carry all this information through each processing step because it may cause performance degradation and makes debugging harder because we carry so much extra data.\n \u0026#8201;\u0026#8212;\u0026#8201;Gregor Hohpe, Bobby Woolf; Enterprise Application Patterns\n   A specific example could again be a CDC connector that captures changes from a database table Users, with a BLOB column that contains the user\u0026#8217;s profile picture (surely not a best practice, still not that uncommon in reality\u0026#8230;\u0026#8203;).\n     Apache Kafka and Large Messages Apache Kafka isn\u0026#8217;t meant for large messages. The maximum message size is 1 MB by default, and while this can be increased, benchmarks are showing best throughput for much smaller messages. Strategies like chunking and externalizing large payloads can thus be vital in order to ensure a satisfying performance.\n     When propagating change data events from that table to Apache Kafka, adding the picture data to each event poses a significant overhead. In particular, if the picture BLOB hasn\u0026#8217;t changed between two events at all.\n Using an SMT, the BLOB data could be externalized to some other storage. On the source side, the SMT could extract the image data from the original record and e.g. write it to a network file system or an Amazon S3 bucket. The corresponding field in the record would be updated so it just contains the unique address of the externalised payload, such as the S3 bucket name and file path:\n   As an optimization, it could be avoided to re-upload unchanged file contents another time by comparing earlier and current hash of the externalized file.\n A corresponding SMT instance applied to sink connectors would retrieve the identifier of the externalized files from the incoming record, obtain the contents from the external storage and put it back into the record before passing it on to the connector.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎\u0026nbsp;\u0026nbsp; SMTs can help to externalize payloads, avoiding large Kafka records. Relying on another service increases overall complexity, though\n   Limitations As we\u0026#8217;ve seen, single message transformations can help to address quite a few requirements that commonly come up for users of Kafka Connect. But there are limitations, too; Like MacGyver, who sometimes has to reach for some other tool than his beloved Swiss Army knife, you shouldn\u0026#8217;t think of SMTs as the perfect solution all the time.\n The biggest shortcoming is already hinted at in their name: SMTs only can be used to process single records, one at a time. E.g. you cannot split up a record into multiple ones using an SMT, as they only can return (at most) one record. Also any kind of stateful processing, like aggregating data from multiple records, or correlating records from several topics is off limits for SMTs. For such use cases, you should be looking at stream processing technologies like Kafka Streams and Apache Flink; also integration technologies like Apache Camel can be of great use here.\n One thing to be aware of when working with SMTs is configuration complexity; when using generic, highly configurable SMTs, you might end up with lengthy configuration that\u0026#8217;s hard to grasp and debug. You might be better off implementing a bespoke SMT which is focussing on one particular task, leveraging the full capabilities of the Java programming language.\n     SMT Testing Whether you use ready-made SMTs by means of configuration, or you implement custom SMTs in Java, testing your work is essential.\n While unit tests are a viable option for basic testing of bespoke SMT implementations, integration tests running against Kafka Connect connectors are recommended for testing SMT configurations. That way you\u0026#8217;ll be sure that the SMT can process actual messages and it has been configured the way you intended to.\n Testcontainers and the Debezium support for Testcontainers are a great foundation for setting up all the required components such as Apache Kafka, Kafka Connect, connectors and the SMTs to test.\n     A specific feature I wished for every now and then is the ability to apply SMTs only to a specific sub-set of the topics created or consumed by a connector. In particular if connectors create different kinds of topics (like an actual data topic and another one with with metadata), it can be desirable to apply SMTs only to the topics of one group but not the other. This requirement is captured in KIP-585 (\"Filter and Conditional SMTs\"), please join the discussion on that one if you got requirements or feedback related to that.\n   Learning More There are several great presentations and blog posts out there which describe in depth what SMTs are, how you can implement your own one, how they are configured etc.\n Here are a few resources I found particularly helpful:\n   KIP-66: The original KIP (Kafka Improvement Proposal) that introduced SMTs\n  Singe Message Transforms are not the Transformations You\u0026#8217;re Looking For: A great overview on SMTs, their capabilities as well as limitations, by Ewen Cheslack-Postava\n  A hands-on experience with Kafka Connect SMTs: In-depth blog post on SMT use cases, things to be aware of and more, by Gian D\u0026#8217;Uia\n   Now, considering this wide range of use cases for SMTs, would MacGyver like and use them for implementing various tasks around Kafka Connect? I would certainly think so. But as always, the right tool for the job must be chosen: sometimes an SMT may be a great fit, another time a more flexible (and complex) stream processing solution might be preferable.\n Just as MacGyver, you got to make a call when to use your Swiss Army knife, duct tape or a paper clip.\n Many thanks to Hans-Peter Grahsl for his feedback while writing this blog post!\n  ","id":38,"publicationdate":"May 14, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eDo you remember Angus \"Mac\" MacGyver?\nThe always creative protagonist of the popular 80ies/90ies TV show, who could solve about any problem with nothing more than a Swiss Army knife, duct tape, shoe strings and a paper clip?\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe single message transformations (SMTs) of Kafka Connect are almost as versatile as MacGyver\u0026#8217;s Swiss Army knife:\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"ulist\"\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eHow to change the timezone or format of date/time message fields?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow to change the topic a specific message gets sent to?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow to filter out specific records?\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eSMTs can be the answer to these and many other questions that come up in the context of Kafka Connect.\nApplied to source or sink connectors,\nSMTs allow to modify Kafka records before they are sent to Kafka, or after they are consumed from a topic, respectively.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Single Message Transformations - The Swiss Army Knife of Kafka Connect","uri":"https://www.morling.dev/blog/single-message-transforms-swiss-army-knife-of-kafka-connect/"},{"content":"For libraries and frameworks it\u0026#8217;s a common requirement to make specific aspects customizeable via service provider interfaces (SPIs): contracts to be implemented by the application developer, which then are invoked by framework code, adding new or replacing existing functionality.\n Often times, the method implementations of such an SPI need to return value(s) to the framework. An alternative to return values are \"emitter parameters\": passed by the framework to the SPI method, they offer an API for receiving value(s) via method calls. Certainly not revolutionary or even a new idea, I find myself using emitter parameters more and more in libraries and frameworks I work on. Hence I\u0026#8217;d like to discuss some advantages I perceive about the emitter parameter pattern.\n An Example As an example, let\u0026#8217;s consider a blogging platform which provides an SPI for extracting categories and tags from given blog posts. Application developers can plug in custom implementations of that SPI, e.g. based on the latest and greatest algorithms in information retrieval and machine learning. Here\u0026#8217;s how a basic SPI contract for this use case could look like, using regular method return values:\n 1 2 3 4 5 public interface BlogPostDataExtractor { Set\u0026lt;String\u0026gt; extractCategories(String contents); Set\u0026lt;String\u0026gt; extractTags(String contents); }    This probably would get the job done, but there are a few problems: any implementation will have to do two passes on the given blog post contents, once in each method\u0026#8201;\u0026#8212;\u0026#8201;not ideal. Also let\u0026#8217;s assume that most blog posts only belong to exactly one category. Implementations still would have to allocate a set for the single returned category.\n While there\u0026#8217;s not much we can do about the second issue with a return value based design, the former problem could be addressed by combining the two methods:\n 1 2 3 4 public interface BlogPostDataExtractor { CategoriesAndTags extractCategoriesAndTags(String contents); }    Now an implementation can retrieve both categories and tags at once. But it\u0026#8217;s worth thinking about how an SPI implementation would instantiate the return type.\n Exposing a concrete class to be instantiated by implementors poses a challenge for future evolution of the SPI: following the best practice and making the return object type immutable, all its properties must be passed to its constructor. Now if an additional attribute should be extracted from blog posts, such as a teaser, the existing constructor cannot be modified, so to not break existing user code. Instead, we\u0026#8217;d have to introduce new constructors whenever adding further attributes. Dealing with all these constructors could become quite inconvenient, in particular if a specific SPI implementation is only interested in producing some of the attributes.\n All in all, for SPIs it\u0026#8217;s often a good idea to only expose interfaces, but no concrete classes. So we could make the return type an interface and leave it to SPI implementors to create an implementation class, but that\u0026#8217;d be rather tedious.\n   The Emitter Parameter Pattern Or, we could provide some sort of builder object which can be used to construct CategoriesAndTags objects. But then why even return an object at all, instead of simply mutating the state of a builder that is provided through a method parameter? And that\u0026#8217;s essentially what the emitter parameter pattern is about: passing in an object which can be used to emit the values which should be \"returned\" by the method.\n     I\u0026#8217;m not aware of any specific name for this pattern, so I came up with \"emitter parameter pattern\" (the notion of callback parameters is related, yet different). And hey, perhaps I\u0026#8217;ll become famous for coining a design pattern name ;) Please let me know in the comments below if you know this pattern under a different name.\n     Here\u0026#8217;s how the extractor SPI could look like when designed with an emitter parameter:\n 1 2 3 4 5 6 7 8 9 10 public interface BlogPostDataExtractor { void extractData(String contents, BlogPostDataReceiver data); (1) interface BlogPostDataReceiver { (2) void addCategory(String category); void addTag(String tag); } }      1 SPI method with input parameter and emitter parameter   2 Emitter parameter type    An implementation would emit the retrieved information by invoking the methods on the data parameter:\n 1 2 3 4 5 6 7 8 9 10 public class MyBlogPostDataExtractor implements BlogPostDataExtractor { public void extractData(String contents, BlogPostDataReceiver data) { String category = ...; Stream\u0026lt;String\u0026gt; tags = ...; data.addCatgory(category); tags.forEach(data::addTag); } }    This approach nicely avoids all the issues with the return value based design:\n   Single and multiple value case handled uniformly: an implementation can call addCategory() just once, or multiple times; either way, it doesn\u0026#8217;t have to deal with the creation of a set, list, or other container for the produced value(s)\n  Flexible evolution of the SPI contract: new methods such as addTeaser(), or addTags(String\u0026#8230;\u0026#8203; tags) can be added to the emitter parameter type, avoiding the creation of more and more return type constructors; as the passed BlogPostDataReceiver instance is controlled by the framework itself, we also could add methods which provide more context required for the task at hand\n  No need for exposing concrete types on the SPI surface: as no return value needs to be instantiated by SPI implementations, the solution works solely with interfaces on the SPI surface; this provides more control to the framework, e.g. the emitter object could be re-used etc.\n  Flexible implementation choices: by not requiring SPI implementations to allocate any return objects, the platform gains a lot of flexibility for how it\u0026#8217;s processing the emitted values: while it could collect the values in a set or list, it also has the option to not allocate any intermediary collections, but process and pass on values one-by-one in a streaming-based way, without any of this impacting SPI implementors\n   Now, are there some downsides to this approach, too? I can see two: if a method only ever should yield a single value, the emitter API might be misleading. We could raise an exception though if an emitter method is called more than once. Also an implementation might hold on to the emitter object and invoke its methods after the call flow has returned from the SPI method, which typically isn\u0026#8217;t desirable. Again that\u0026#8217;s something that can be prevented by invalidating the emitter object after the SPI method returned, raising an exception in case of further method invocations.\n Overall, I think the emitter parameter pattern is a valuable tool in the box of library and framework authors; it provides flexibility for implementation choices and future evolution when designing SPIs. Real-world examples include the ValueExtractor SPI in Bean Validation 2.0 (where it was chosen to provide a uniform value of extracting single and multiple values from container objects) and the ChangeRecordEmitter contract in Debezium\u0026#8217;s SPI.\n Many thanks to Hans-Peter Grahsl and Nils Hartmann for reviewing an early version of this blog post.\n  ","id":39,"publicationdate":"May 4, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eFor libraries and frameworks it\u0026#8217;s a common requirement to make specific aspects customizeable via \u003ca href=\"https://en.wikipedia.org/wiki/Service_provider_interface\"\u003eservice provider interfaces\u003c/a\u003e (SPIs):\ncontracts to be implemented by the application developer, which then are invoked by framework code,\nadding new or replacing existing functionality.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eOften times, the method implementations of such an SPI need to return value(s) to the framework.\nAn alternative to return values are \"emitter parameters\":\npassed by the framework to the SPI method, they offer an \u003cem\u003eAPI\u003c/em\u003e for receiving value(s) via method calls.\nCertainly not revolutionary or even a new idea,\nI find myself using emitter parameters more and more in libraries and frameworks I work on.\nHence I\u0026#8217;d like to discuss some advantages I perceive about the emitter parameter pattern.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"The Emitter Parameter Pattern for Flexible SPI Contracts","uri":"https://www.morling.dev/blog/emitter-parameter-pattern-for-flexible-spis/"},{"content":"Making applications extensible with some form of plug-ins is a very common pattern in software design: based on well-defined APIs provided by the application core, plug-ins can customize an application\u0026#8217;s behavior and provide new functionality. Examples include desktop applications like IDEs or web browsers, build tools such as Apache Maven or Gradle, as well as server-side applications such as Apache Kafka Connect, a runtime for Kafka connectors plug-ins.\n In this post I\u0026#8217;m going to explore how the Java Platform Module System's notion of module layers can be leveraged for implementing plug-in architectures on the JVM. We\u0026#8217;ll also discuss how Layrry, a launcher and runtime for layered Java applications, can help with this task.\n A key requirement for any plug-in architecture is strong isolation between different plug-ins: their state, classes and dependencies should be encapsulated and independent of each other. E.g. package declarations in two plug-ins should not collide, also they should be able to use different versions of another 3rd party dependency. This is why the default module path of Java (specified using the --module-path option) is not enough for this purpose: it doesn\u0026#8217;t support more than one version of a given module.\n The module system\u0026#8217;s answer are module layers: by organizing an application and its plug-ins into multiple layers, the required isolation between plug-ins can be achieved.\n     With the module system, each Java application always contains at least one layer, the boot layer. It contains the platform modules and the modules provided on the module path.\n     An Example: The Greeter CLI App To make things more tangible, let\u0026#8217;s consider a specific example; The \"Greeter\" app is a little CLI utility, that can produce greetings in different languages.\n In order to not limit the number of supported languages, it provides a plug-in API, which allows to add additional greeting implementations, without the need to rebuild the core application. Here is the Greeter contract, which is to be implemented by each language plug-in:\n 1 2 3 4 5 package com.example.greeter.api; public interface Greeter { String greet(String name); }    Greeters are instantiated via accompanying implementations of GreeterFactory:\n 1 2 3 4 5 public interface GreeterFactory { String getLanguage(); (1) String getFlag(); Greeter getGreeter(); (2) }      1 The getLanguage() and getFlag() methods are used to show a description of all available greeters in the CLI application   2 The getGreeter() method returns a new instance of the corresponding Greeter type    Here\u0026#8217;s the overall architecture of the Greeter application, with three different language implementations:\n   The application is made up of five different layers:\n   greeter-platform: contains the Greeter and GreeterFactory contracts\n  greeter-en, greeter-de and greeter-fr: greeter implementations for different languages; note how each one is depending on a different version of some greeter-date module. As they are isolated in different layers, they can co-exist within the application\n  greeter-app: the \"shell\" of the application which loads all the greeter implementations and makes them accessible as a simple CLI application\n   Now let\u0026#8217;s see how this application structure can be assembled using Layrry.\n   Application Plug-ins With Layrry In a previous blog post we\u0026#8217;ve explored how applications can be cut into layers, described in Layrry\u0026#8217;s layers.yml configuration file. A simple static layer definition would defeat the purpose of a plug-in architecture, though: not all possible plug-ins are known when assembling the application.\n Layrry addresses this requirement by allowing to source different layers from directories on the file system:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 layers: platform: (1) modules: - \"com.example.greeter:greeter-api:1.0.0\" plugins: (2) parents: - \"api\" directory: path/to/plugins app: (3) parents: - \"plugins\" modules: - \"com.example.greeter:greeter-app:1.0.0\" main: module: com.example.greeter.app class: com.example.greeter.app.App      1 The platform layer with the API module   2 The plug-in layer(s)   3 The application layer with the \"application shell\"    Whereas the platform and app layers are statically defined, using the Maven GAV coordinates of the modules to include, the plugins part of the configuration describes an open-ended set of layers. Each sub-directory of the given directory represents its own layer. All modules within this sub-directory will be added to the layer, and the API layer will be the parent of each of the plug-in layers. The app layer has all the plug-in layers as its ancestors, allowing it to retrieve plug-in implementations from these layers.\n More greeter plug-ins can be added to the application by simply creating a sub-directory with the required module(s).\n   Finding Plug-in Implementations With the Java Service Loader Structuring the application into different layers isn\u0026#8217;t all we need for building a plug-in architecture; we also need a way for detecting and loading the actual plug-in implementations. The service loader mechanism of the Java platform comes in handy for that. If you have never worked with the service loader API, it\u0026#8217;s definitely recommended to study its extensive JavaDoc description:\n  A service is a well-known interface or class for which zero, one, or many service providers exist. A service provider (or just provider) is a class that implements or subclasses the well-known interface or class. A ServiceLoader is an object that locates and loads service providers deployed in the run time environment at a time of an application\u0026#8217;s choosing.   Having been a supported feature of Java since version 6, the service loader API has been been reworked and refined to work within modular environments when the Java Module System was introduced in JDK 9.\n In order to retrieve service implementations via the service loader, a consuming module must declare the use of the service in its module descriptor. For our purposes, the GreeterFactory contract is a perfect examplification of the service idea. Here\u0026#8217;s the descriptor of the Greeter application\u0026#8217;s app module, declaring its usage of this service:\n 1 2 3 4 5 module com.example.greeter.app { exports com.example.greeter.app; requires com.example.greeter.api; uses com.example.greeter.api.GreeterFactory; }    The module descriptor of each greeter plug-in must declare the service implementation(s) which it provides. E.g. here is the module descriptor of the English greeter implementation:\n 1 2 3 4 5 6 module com.example.greeter.en { requires com.example.greeter.api; requires com.example.greeter.dateutil; provides com.example.greeter.api.GreeterFactory with com.example.greeter.en.EnglishGreeterFactory; }    From within the app module, the service implementations can be retrieved via the java.util.ServiceLoader class.\n When using the service loader in layered applications, there\u0026#8217;s one potential pitfall though, which mostly will affect existing applications which are migrated: in order to access service implementations located in a different layer (specifically, in an ancestor layer of the loading layer), the method load(ModuleLayer, Class\u0026lt;?\u0026gt;) must be used. When using other overloaded variants of load(), e.g. the commonly used load(Class\u0026lt;?\u0026gt;), those implementations won\u0026#8217;t be found.\n Hence the code for loading the greeter implementations from within the app layer could look like this:\n 1 2 3 4 5 6 7 8 9 10 private static List\u0026lt;GreeterFactory\u0026gt; getGreeterFactories() { ModuleLayer appLayer = App.class.getModule().getLayer(); return ServiceLoader.load(appLayer, GreeterFactory.class) .stream() .map(p -\u0026gt; p.get()) .sorted((gf1, gf2) -\u0026gt; gf1.getLanguage().compareTo( gf2.getLanguage())) .collect(Collectors.toList()); }    Having loaded the list of greeter factories, it doesn\u0026#8217;t take too much code to display a list with all available implementations, expect a choice by the user and invoke the greeter for the chosen language. This code which isn\u0026#8217;t too interesting is omitted here for the sake of brevity and can be found in the accompanying example source code repo.\n     JDK 9 brought some more nice improvements for the service loader API. E.g. the type of service implementations can be examined without actually instantiating them. This allows for interesting alternatives for providing service meta-data and choosing an implementation based on some criteria. For instance, greeter metadata like the language name and flag could be given using an annotation:\n 1 2 3 4 @GreeterDefinition(lang=\"English\", flag=\"🇬🇧\") public class EnglishGreeterFactory implements GreeterFactory { Greeter getGreeter(); }    Then the method ServiceLoader.Provider#type() can be used to obtain the annotation and return a greeter factory for a given language:\n 1 2 3 4 5 6 7 8 9 10 11 private Optional\u0026lt;GreeterFactory\u0026gt; getGreeterFactoryForLanguage( String language) { ModuleLayer layer = App.class.getModule().getLayer(); return ServiceLoader.load(layer, GreeterFactory.class) .stream() .filter(gf -\u0026gt; gf.type().getAnnotation( GreeterDefinition.class).lang().equals(language)) .map(gf -\u0026gt; gf.get()) .findFirst(); }          Seeing it in Action Lastly, let\u0026#8217;s take a look at the complete Greeter application in action. Here it is, initially with two, and then with three greeter implementations:\n   The layers configuration file is adjusted to load greeter plug-ins from the plugins directory; initially, two greeters for English and French exist. Then the German greeter implementation gets picked up by the application after adding it to the plug-in directory, without requiring any changes to the application tiself.\n The complete source code, including the logic for displaying all the available greeters and prompting for input, is available in the Layrry repository on GitHub.\n And there you have it, a basic plug-in architecture using Layrry and the Java Module System. Going forward, this might evolve in a few ways. E.g. it might be desirable to detect additional plug-ins without having to restart the application, e.g. when thinking of desktop application use cases. While loading additional plug-ins in new layers should be comparatively easy, unloading already loaded layers, e.g. when updating a plug-in to a newer version, could potentially be quite tricky. In particular, there\u0026#8217;s no way to actively unload layers, so we\u0026#8217;d have to rely on the garbage collector to clean up unused layers, making sure no references to any of their classes are kept in other, active layers.\n One also could think of an event bus, allowing different plug-ins to communicate in a safe, yet loosely coupled way. What requirements would you have for plug-in centered applications running on the Java Module System? Let\u0026#8217;s exchange in the comments below!\n  ","id":40,"publicationdate":"Apr 21, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eMaking applications extensible with some form of plug-ins is a very common pattern in software design:\nbased on well-defined APIs provided by the application core, plug-ins can customize an application\u0026#8217;s behavior and provide new functionality.\nExamples include desktop applications like IDEs or web browsers, build tools such as Apache Maven or Gradle, as well as server-side applications such as Apache Kafka Connect,\na runtime for Kafka connectors plug-ins.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIn this post I\u0026#8217;m going to explore how the \u003ca href=\"https://www.jcp.org/en/jsr/detail?id=376\"\u003eJava Platform Module System\u003c/a\u003e's notion of module layers can be leveraged for implementing plug-in architectures on the JVM.\nWe\u0026#8217;ll also discuss how \u003ca href=\"https://github.com/moditect/layrry\"\u003eLayrry\u003c/a\u003e, a launcher and runtime for layered Java applications, can help with this task.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Plug-in Architectures With Layrry and the Java Module System","uri":"https://www.morling.dev/blog/plugin-architectures-with-layrry-and-the-java-module-system/"},{"content":"One of the biggest changes in recent Java versions has been the introduction of the module system in Java 9. It allows to organize Java applications and their dependencies in strongly encapsulated modules, utilizing explicit and well-defined module APIs and relationships.\n In this post I\u0026#8217;m going to introduce the Layrry open-source project, a launcher and Java API for executing modularized Java applications. Layrry helps Java developers to assemble modularized applications from dependencies using their Maven coordinates and execute them using module layers. Layers go beyond the capabilities of the \"flat\" module path specified via the --module-path parameter of the java command, e.g. allowing to use multiple versions of one module within one and the same application.\n Why Layrry? The Java Module System doesn\u0026#8217;t define any means of mapping between modules (e.g. com.acme.crm) and JARs providing such module (e.g. acme-crm-1.0.0.Final.jar), or retrieving modules from remote repositories using unique identifiers (e.g. com.acme:acme-crm:1.0.0.Final). Instead, it\u0026#8217;s the responsibility of the user to obtain all required JARs of a modularized application and provide them via the --module-path parameter.\n Furthermore, the module system doesn\u0026#8217;t define any means of module versioning; i.e. it\u0026#8217;s the responsibility of the user to obtain all modules in the right version. Using the --module-path option, it\u0026#8217;s not possible, though, to assemble an application that uses multiple versions of one and the same module. This may be desirable for transitive dependencies of an application, which might be required in different versions by two separate direct dependencies.\n This is where Layrry comes in (pronounced \"Larry\"): it provides a declarative approach as well as an API for assembling modularized applications. The (modular) JARs to be included are described using Maven GAV (group id, artifact id, version) coordinates, solving the issue of retrieving all required JARs from a remote repository, in the right version.\n With Layrry, applications are organized in module layers, which allows to use different versions of one and the same module in different layers of an application (as long as they are not exposed in a conflicting way on module API boundaries).\n   An Example As an example, let\u0026#8217;s consider an application made up of the following modules:\n   The application\u0026#8217;s main module, com.example:app, depends on two others, com.example:foo and com.example:bar. They in turn depend on the Log4j API and another module, com.example:greeter. The latter is used in two different versions, though.\n Let\u0026#8217;s take a closer look at the Greeter class in these modules. Here is the version in com.example:greeter@1.0.0, as used by com.example:foo:\n 1 2 3 4 5 6 public class Greeter { public String greet(String name, String from) { return \"Hello, \" + name + \" from \" + from + \" (Greeter 1.0.0)\"; } }    And this is how it looks in com.example:greeter@2.0.0, as used by com.example:bar:\n 1 2 3 4 5 6 7 8 9 10 11 public class Greeter { public String hello(String name, String from) { return \"Hello, \" + name + \" from \" + from + \" (Greeter 2.0.0)\"; } public String goodBye(String name, String from) { return \"Good bye, \" + name + \" from \" + from + \" (Greeter 2.0.0)\"; } }    The Greeter API has evolved in a backwards-incompatible way, i.e. it\u0026#8217;s not possible for the foo and bar modules to use the same version.\n With a \"flat\" module path (or classpath), there\u0026#8217;s no way for dealing with this situation. You\u0026#8217;d inevitably end up with a NoSuchMethodError, as either foo or bar would be linked at runtime against a version of the class different from the version it has been compiled against.\n The lack of support for using multiple module versions when working with the --module-path option might be surprising at first, but it\u0026#8217;s an explicit non-requirement of the module system to support multiple module versions or even deal with selecting matching module versions at all.\n This means that the module descriptors of both foo and bar require the greeter module without any version information:\n 1 2 3 4 5 module com.example.foo { exports com.example.foo; requires org.apache.logging.log4j; requires com.example.greeter; }    1 2 3 4 5 module com.example.bar { exports com.example.bar; requires org.apache.logging.log4j; requires com.example.greeter; }      Module Layers to the Rescue While only one version of a given module is supported when running applications via java --module-path=\u0026#8230;\u0026#8203;, there\u0026#8217;s a lesser known feature of the module system which provides a way out: module layers.\n A module layer \"is created from a graph of modules in a Configuration and a function that maps each module to a ClassLoader.\" Using the module layer API, multiple versions of a module can be loaded in different layers, thus using different classloaders.\n Note the layers API doesn\u0026#8217;t concern itself with obtaining JARs or modules from remote locations such as the Maven Central repository; instead, any modules must be provided as Path objects. Here is how a layer with the foo and greeter:1.0.0 modules could be assembled:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ModuleLayer boot = ModuleLayer.boot(); ClassLoader scl = ClassLoader.getSystemClassLoader(); Path foo = Paths.get(\"path/to/foo-1.0.0.jar\"); (1) Path greeter10 = Paths.get(\"path/to/greeter-1.0.0.jar\"); (2) ModuleFinder fooFinder = ModuleFinder.of(foo, greeter10); Configuration fooConfig = boot.configuration() (3) .resolve( fooFinder, ModuleFinder.of(), Set.of(\"com.example.foo\", \"com.example.greeter\") ); ModuleLayer fooLayer = boot.defineModulesWithOneLoader( fooConfig, scl); (4)      1 obtain foo-1.0.0.jar   2 obtain greeter-1.0.0.jar   3 Create a configuration derived from the \"boot\" module of the JVM, providing a ModuleFinder for the two JARs obtained before, and resolving the two modules   4 Create a module layer using the configuration, loading all contained modules with a single classloader    Similarly, you could create a layer for bar and greeter:2.0.0, as well as layers for log4j and the main application module. The layers API is very flexible, e.g. you could load each module in its own classloader and more. But all this flexibility can make using the API direcly a daunting task.\n Also using an API might not be what you want in the first place: wouldn\u0026#8217;t it be nice if there was a CLI tool, akin to using java --module-path=\u0026#8230;\u0026#8203;, but with the additional powers of module layers?\n   The Layrry Launcher This is where Layrry comes in: it is a CLI tool which takes a configuration of a layered application (defined in a YAML file) and executes it. The layer descriptor for the example above looks like so:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 layers: log: (1) modules: (2) - \"org.apache.logging.log4j:log4j-api:jar:2.13.1\" - \"org.apache.logging.log4j:log4j-core:jar:2.13.1\" - \"com.example:logconfig:1.0.0\" foo: parents: (3) - \"log\" modules: - \"com.example:greeter:1.0.0\" - \"com.example:foo:1.0.0\" bar: parents: - \"log\" modules: - \"com.example:greeter:2.0.0\" - \"com.example:bar:1.0.0\" app: parents: - \"foo\" - \"bar\" modules: - \"com.example:app:1.0.0\" main: (4) module: com.example.app class: com.example.app.App      1 Each layer has a unique name   2 The modules element lists all the modules contained in the layer, using Maven coordinates (group id, artifact id, version), unambigously referencing a (modular) JAR in a specific version   3 A layer can have one or more parent layers, whose modules it can access; if no parent is given, the JVM\u0026#8217;s \"boot\" layer is the implicit parent of a layer   4 The given main module and class is the one that will be executed by Layrry    The configuration above describes four layers, log, foo, bar and app, with the modules they contain and the parent/child relationships between these layers. Note how the versions 1.0.0 and 2.0.0 of the greeter module are used in foo and bar. The file also specifies the main class to execute when running this application.\n Using Layrry, a modular application is executed like this:\n 1 2 3 4 5 6 7 java -jar layrry-1.0-SNAPSHOT-jar-with-dependencies.jar \\ --layers-config layers.yml \\ Alice 20:58:01.451 [main] INFO com.example.foo.Foo - Hello, Alice from Foo (Greeter 1.0.0) 20:58:01.472 [main] INFO com.example.bar.Bar - Hello, Alice from Bar (Greeter 2.0.0) 20:58:01.473 [main] INFO com.example.bar.Bar - Good bye, Alice from Bar (Greeter 2.0.0)    The log messages show how the two versions of greeter are used by foo and bar, respectively. Layrry will download all referenced JARs using the Maven resolver API, i.e. you don\u0026#8217;t have to deal with manually obtaining all the JARs and providing them to the java runtime.\n   Using the Layrry API In addition to the YAML-based launcher, Layrry provides also a Java API for assembling and running layered applications. This can be used in cases where the structure of layers is only known at runtime, or for implementing plug-in architectures.\n In order to use Layrry programmatically, add the following dependency to your pom.xml:\n 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.moditect.layrry\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;layrry\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;    Then, the Layrry Java API can be used like this (showing the same example as above):\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Layers layers = Layers.layer(\"log\") .withModule(\"org.apache.logging.log4j:log4j-api:jar:2.13.1\") .withModule(\"org.apache.logging.log4j:log4j-core:jar:2.13.1\") .withModule(\"com.example:logconfig:1.0.0\") .layer(\"foo\") .withParent(\"log\") .withModule(\"com.example:greeter:1.0.0\") .withModule(\"com.example:foo:1.0.0\") .layer(\"bar\") .withParent(\"log\") .withModule(\"com.example:greeter:2.0.0\") .withModule(\"com.example:bar:1.0.0\") .layer(\"app\") .withParent(\"foo\") .withParent(\"bar\") .withModule(\"com.example:app:1.0.0\") .build(); layers.run(\"com.example.app/com.example.app.App\", \"Alice\");      Next Steps The Layrry project is still in its infancy. Nevertheless it can be a useful tool for application developers wishing to leverage the Java Module System. Obtaining modular JARs via Maven coordinates and providing an easy-to-use mechanism for organizing modules in layers enables usages which cannot be addressed using the plain java --module-path \u0026#8230;\u0026#8203; approach.\n Layrry is open-source (under the Apache License version 2.0). The source code is hosted on GitHub, and your contributions are very welcomed.\n Please let me know about your ideas and requirements in the comments below or by opening up issues on GitHub. Planned enhancements include support for creating modular runtime images (jlink) based on the modules referenced in a layers.yml file, and visualization of module layers and their modules via GraphViz.\n  ","id":41,"publicationdate":"Mar 29, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eOne of the biggest changes in recent Java versions has been the introduction of the \u003ca href=\"http://openjdk.java.net/projects/jigsaw/spec/\"\u003emodule system\u003c/a\u003e in Java 9.\nIt allows to organize Java applications and their dependencies in strongly encapsulated modules, utilizing explicit and well-defined module APIs and relationships.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIn this post I\u0026#8217;m going to introduce the \u003ca href=\"https://github.com/moditect/layrry\"\u003eLayrry\u003c/a\u003e open-source project, a launcher and Java API for executing modularized Java applications.\nLayrry helps Java developers to assemble modularized applications from dependencies using their Maven coordinates and execute them using module layers.\nLayers go beyond the capabilities of the \"flat\" module path specified via the \u003cem\u003e--module-path\u003c/em\u003e parameter of the \u003cem\u003ejava\u003c/em\u003e command,\ne.g. allowing to use multiple versions of one module within one and the same application.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Introducing Layrry: A Launcher and API for Modularized Java Applications","uri":"https://www.morling.dev/blog/introducing-layrry-runner-and-api-for-modularized-java-applications/"},{"content":"Within Debezium, the project I\u0026#8217;m working on at Red Hat, we recently encountered an \"interesting\" situation where we had to resolve a rather difficult merge conflict. As others where interested in how we addressed the issue, and also for our own future reference, I\u0026#8217;m going to give a quick run down of the problem we encountered and how we solved it.\n The Problem Ideally, we\u0026#8217;d only ever work on a single branch and would never have to deal with porting changes between the master and other branches. Oftentimes we cannot get around this, though: specific versions of a software may have to be maintained for some time, requiring to backport bugfixes from the current development branch to the branch corresponding to the maintained version.\n In our specific case we had to deal with backporting changes to our project documentation. To complicate things, this documentation (written in AsciiDoc) has been largely re-organized between master and the targeted older branch, 1.0. What used to be one large AsciiDoc file for each of the Debezium connectors, got split up into multiple smaller files on master now. This split was meant to be applied to 1.0 too, but due to some miscommunication in the team (these things happen, right) this wasn\u0026#8217;t done, whereas an asorted set of documentation changes had been backported already to the larger, monolithic AsciiDoc files.\n So the situation we faced was this:\n   large, monolithic AsciiDoc files on the 1.0 branch\n  smaller, modularized AsciiDoc files on master\n  Documentation updates applied on master, of which only a subset is relevant for 1.0 (new features shouldn\u0026#8217;t be added to the Debezium 1.0 documentation)\n  Some of the documentation updates relevant for the 1.0 branch already had been backported from master, while others had not\n   All in all, a rather convoluted situation; the full diff of the documentation sub-directory between the two branches was about 13K lines.\n So what should we do? Cherry-picking individual commits from master was not really an option, as there were a few hundred commits on master since 1.0 had been forked off. Also many commits would contain documentation and code changes. The latter had already been backported successfully before.\n Realizing that resolving that merge conflict was next to impossible, the next idea was to essentially start from scratch and re-apply all relevant documentation changes to the 1.0 branch. Our initial idea was to create a patch with the difference of the documentation directory between the two branches. But editing that patch file with 13K lines turned out to be not manageable, either.\n   The Solution This is when we were reminded of the possibilities of git filter-branch: using this command it should be possible to isolate all the documentation changes done on master since Debezium 1.0 and apply the required sub-set of these changes to the 1.0 branch.\n To start with a clean slate, we created a new temporary branch based on 1.0:\n git checkout -b docs_backport 1.0   We then reset the contents of the documentation directory to its state as of the 1.0.0.Final release, as that\u0026#8217;s where the 1.0 and master branches diverged.\n rm -rf documentation git add documentation git checkout v1.0.0.Final documentation git commit -m \"Resetting documentation dir to v1.0.0.Final\" # This should yield no differences git diff v1.0.0.Final..docs_backport documentation   The next step was to filter all commits on master so to only keep any changes to the documentation directory. This was done on a new branch, docs_filtered. The --subdirectory-filter option comes in handy for that:\n git checkout -b docs_filtered master git filter-branch -f --prune-empty \\ --subdirectory-filter documentation \\ v1.0.0.Final..docs_filtered   This leaves us with a branch docs_filtered which only contains the commits since the v1.0.0.Final tag that modified the documentation directory.\n The --subdirectory-filter option also moves the contents of the given directory to the root of the repo, though. That\u0026#8217;s not exactly what we need. But another option, --tree-filter, lets us restore the original directory layout. It allows to run a set of commands against each of the filtered commits. We can use this to move the contents of documentation back to that directory:\n git filter-branch -f \\ --tree-filter 'mkdir -p documentation; \\ mv antora.yml documentation 1\u0026gt;/dev/null 2\u0026gt;/dev/null; \\ mv modules documentation 1\u0026gt;/dev/null 2\u0026gt;/dev/null;' \\ v1.0.0.Final..docs_filtered   Examining the history now, we can see that the commits on the docs_filtered apply the changes to the documentation directory, as expected.\n One problem still remains, though: by means of the --subdirectory-filter option, the very first commit removes all contents besides the documentation directory. This can be fixed by doing an interactive rebase of the current branch, beginning at the v1.0.0.Final tag:\n git rebase -i v1.0.0.Final   We need to edit the very first commit; all changes besides those to the documentation directory need to be reverted from that commit. There might be a better way of doing so, I simply ran git checkout for all the other resources:\n git checkout v1.0.0.Final debezium-connector-mongodb git checkout v1.0.0.Final debezium-connector-mysql ...   At this point the filtered branch still is based off of the v1.0.0.Final tag, whereas it should be based off of the docs_backport branch. git rebase --onto to the rescue:\n git rebase --onto docs_backport v1.0.0.Final docs_filtered   This rebases all the commits from the docs_filtered branch onto the docs_backport branch. Now we have a state where where all the documention changes have been cleanly applied to the 1.0 code base, i.e. the following should yield no differences:\n git diff docs_filtered..master documentation   The last and missing step is to do another rebase of all the documentation commits, discarding those that apply to any features that didn\u0026#8217;t get backported to 1.0.\n Thankfully, my partner-in-crime Jiri Pechanec stepped in here: as he had done the original feature backport, it didn\u0026#8217;t take him too long to go through the list of documentation commits and identify those which were relevant for the 1.0 code base. After one more interactive rebase for applying those we finally were in a state, where all the required documentation changes had been backported.\n Looking at the 1.0 history, you\u0026#8217;d still see some partial documentation changes up to the point, where we decided to start all over and revert these. Theoretically we could do another git filter run to exclude those, but we decided against that, as we already had done releases off of the 1.0 branch and didn\u0026#8217;t want to alter the commit history of a released branch after the fact.\n  ","id":42,"publicationdate":"Mar 16, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eWithin \u003ca href=\"https://debezium.io/\"\u003eDebezium\u003c/a\u003e, the project I\u0026#8217;m working on at Red Hat, we recently encountered an \"interesting\" situation where we had to resolve a rather difficult merge conflict.\nAs others where interested in how we addressed the issue, and also for our own future reference,\nI\u0026#8217;m going to give a quick run down of the problem we encountered and how we solved it.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Reworking Git Branches with git filter-branch","uri":"https://www.morling.dev/blog/reworking-git-branches-with-git-filter-branch/"},{"content":"","id":43,"publicationdate":"Jan 29, 2020","section":"tags","summary":"","tags":null,"title":"jakartaee","uri":"https://www.morling.dev/tags/jakartaee/"},{"content":"","id":44,"publicationdate":"Jan 29, 2020","section":"tags","summary":"","tags":null,"title":"java","uri":"https://www.morling.dev/tags/java/"},{"content":"","id":45,"publicationdate":"Jan 29, 2020","section":"tags","summary":"","tags":null,"title":"microprofile","uri":"https://www.morling.dev/tags/microprofile/"},{"content":"","id":46,"publicationdate":"Jan 29, 2020","section":"tags","summary":"","tags":null,"title":"monitoring","uri":"https://www.morling.dev/tags/monitoring/"},{"content":"The JDK Flight Recorder (JFR) is an invaluable tool for gaining deep insights into the performance characteristics of Java applications. Open-sourced in JDK 11, JFR provides a low-overhead framework for collecting events from Java applications, the JVM and the operating system.\n In this blog post we\u0026#8217;re going to explore how custom, application-specific JFR events can be used to monitor a REST API, allowing to track request counts, identify long-running requests and more. We\u0026#8217;ll also discuss how the JFR Event Streaming API new in Java 14 can be used to export live events, making them available for monitoring and alerting via tools such as Prometheus and Grafana.\n JFR and its companion tool JDK Mission Control (JMC) for analyzing JFR recordings have come a long way; originally developed at BEA and part of the JRockit VM, they were later on commercial features of the Oracle JDK. As of Java 11, JFR got open-sourced and is part of OpenJDK distributions. JMC is also open-source, but it\u0026#8217;s an independent tool under the OpenJDK umbrella, which must be downloaded separately.\n Using the combination of JFR and JMC, you can get all kinds of information about your Java application, such as events on garbage collection, compilation, classloading, memory allocation, file and socket IO, method profiling data, and much more. To learn more about Flight Recorder and Mission Control in general, have a look at the Code One 2019 presentation Introduction to JDK Mission Control \u0026amp; JDK Flight Recorder by Marcus Hirt and Klara Ward. You can find some more links to related useful resources towards the end of this post.\n Custom Flight Recorder Events One thing that\u0026#8217;s really great about JFR and JMC is that you\u0026#8217;re not limited to the events and data baked into the JVM and platform libraries: JFR also provides an API for implementing custom events. That way you can use the low-overhead event recording infrastructure (its goal is to add at most 1% performance overhead) for your own event types. This allows you to record and analyze higher-level events, using the language of your application-specific domain.\n Taking my day job project Debezium as an example (an open-source platform for change data capture for a variety of databases), we could for instance produce events such as \"Snapshot started\", \"Snapshotting of table 'Customers' completed\", \"Captured change event for transaction log offset 123\" etc. Users could send us recordings with these events and we could dive into them, in order to identify bugs or performance issues.\n In the following let\u0026#8217;s consider a less complex and hence better approachable example, though. We\u0026#8217;ll implement an event for measuring the duration of REST API calls. The Todo service from my recent blog post on Quarkus Qute will serve as our guinea pig. It is based on the Quarkus stack and provides a simple REST API based on JAX-RS. As always, you can find the complete source code for this blog post on GitHub.\n Event types are implemented by extending the jdk.jfr.Event class; It already provides us with some common attributes such as a timestamp and a duration. In sub-classes you can add application-specific payload attributes, as well as some metadata such as a name and category which will be used for organizing and displaying events when looking at them in JMC.\n Which attributes to add depends on your specific requirements; you should aim for the right balance between capturing all the relevant information that will be useful for analysis purposes later on, while not going overboard and adding too much, as that could cause record files to become too large, in particular for events that are emitted with a high frequency. Also retrieval of the attributes should be an efficient operation, so to avoid any unneccessary overhead.\n Here\u0026#8217;s a basic event class for monitoring our REST API calls:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 @Name(JaxRsInvocationEvent.NAME) (1) @Label(\"JAX-RS Invocation\") @Category(\"JAX-RS\") @Description(\"Invocation of a JAX-RS resource method\") @StackTrace(false) (2) public class JaxRsInvocationEvent extends Event { static final String NAME = \"dev.morling.jfr.JaxRsInvocation\"; @Label(\"Resource Method\") (3) public String method; @Label(\"Media Type\") public String mediaType; @Label(\"Java Method\") public String javaMethod; @Label(\"Path\") public String path; @Label(\"Query Parameters\") public String queryParameters; @Label(\"Headers\") public String headers; @Label(\"Length\") @DataAmount (4) public int length; @Label(\"Response Headers\") public String responseHeaders; @Label(\"Response Length\") public int responseLength; @Label(\"Response Status\") public int status; }      1 The @Name, @Category, @Description and @Label annotations define some meta-data, e.g. used for controlling the appearance of these events in the JMC UI   2 JAX-RS invocation events shouldn\u0026#8217;t contain a stacktrace by default, as that\u0026#8217;d only increase the size of Flight Recordings without adding much value   3 One payload attribute is defined for each relevant property such as HTTP method, media type, the invoked path etc.   4 @DataAmount tags this attribute as a data amount (by default in bytes) and will be displayed accordingly in JMC; there are many other similar annotations in the jdk.jfr package, such as @MemoryAddress, @Timestamp and more    Having defined the event class itself, we must find a way for emitting event instances at the right point in time. In the simplest case, e.g. suitable for events related to your application logic, this might happen right in the application code itself. For more \"technical\" events it\u0026#8217;s a good idea though to keep the creation of Flight Recorder events separate from your business logic, e.g. by using mechanisms such as servlet filters, interceptors and similar, which allow to inject cross-cutting logic into the call flow of your application.\n You also might employ byte code instrumentation at build or runtime for this purpose. The JMC Agent project aims at providing a configurable Java agent that allows to dynamically inject code for emitting JFR events into running programs. Via the EventFactory class, the JFR API also provides a way for defining event types dynamically, should their payload attributes only be known at runtime.\n For monitoring a JAX-RS based REST API, the ContainerRequestFilter and ContainerResponseFilter contracts come in handy, as they allow to hook into the request handling logic before and after a REST request gets processed:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @Provider (1) public class FlightRecorderFilter implements ContainerRequestFilter, ContainerResponseFilter { @Override (2) public void filter(ContainerRequestContext requestContext) throws IOException { JaxRsInvocationEvent event = new JaxRsInvocationEvent(); if (!event.isEnabled()) { (3) return; } event.begin(); (4) requestContext.setProperty(JaxRsInvocationEvent.NAME, event); (5) } @Override (6) public void filter(ContainerRequestContext requestContext, ContainerResponseContext responseContext) throws IOException { JaxRsInvocationEvent event = (JaxRsInvocationEvent) requestContext .getProperty(JaxRsInvocationEvent.NAME); if (event == null || !event.isEnabled()) { return; } event.end(); (7) event.path = String.valueOf(requestContext.getUriInfo().getPath()); if (event.shouldCommit()) { (8) event.method = requestContext.getMethod(); event.mediaType = String.valueOf(requestContext.getMediaType()); event.length = requestContext.getLength(); event.queryParameters = requestContext.getUriInfo() .getQueryParameters().toString(); event.headers = requestContext.getHeaders().toString(); event.javaMethod = getJavaMethod(requestContext); event.responseLength = responseContext.getLength(); event.responseHeaders = responseContext.getHeaders().toString(); event.status = responseContext.getStatus(); event.commit(); (9) } } private String getJavaMethod(ContainerRequestContext requestContext) { String propName = \"org.jboss.resteasy.core.ResourceMethodInvoker\"; ResourceMethodInvoker invoker = (ResourceMethodInvoker)requestContext.getProperty(propName); return invoker.getMethod().toString(); } }      1 Allows the filter to be picked up automatically by the JAX-RS implementation   2 Will be invoked before the request is processed   3 Nothing to do if the event type is not enabled for recordings currently   4 Begin the timing of the event   5 Store the event in the request context, so it can be obtained again later on   6 Will be invoked after the request has been processed   7 End the timing of the event   8 The event should be committed if it is enabled and its duration is within the threshold configured for it; in that case, populate all the payload attributes of the event based on the values from the request and response contexts   9 Commit the event with Flight Recorder    With that, our event class is pretty much ready to be used. There\u0026#8217;s only one more thing to do, and that is registering the new type with the Flight Recorder system. A Quarkus application start-up lifecycle method comes in handy for that:\n 1 2 3 4 5 6 7 @ApplicationScoped public class Metrics { public void registerEvent(@Observes StartupEvent se) { FlightRecorder.register(JaxRsInvocationEvent.class); } }    Note this step isn\u0026#8217;t strictly needed, the event type can also be used without explicit registration. But doing so will later on allow to apply specific settings for the event in Mission Control (see below), also if no event of this type has been emitted yet.\n   Creating JFR Recordings Now let\u0026#8217;s capture some JAX-RS API events using Flight Recorder and inspect them in Mission Control.\n To do so, make sure to have Mission Control installed. Just as with OpenJDK, there are different builds for Mission Control to choose from. If you\u0026#8217;re in the Fedora/RHEL universe, there\u0026#8217;s a repository package which you can install, e.g. like this for the Fedora JMC package:\n 1 sudo dnf module install jmc:7/default    Alternatively, you can download builds for different platforms from Oracle; some more info about these builds can be found in this blog post by Marcus Hirt. There\u0026#8217;s also the Liberica Mission Control build by BellSoft and Zulu Mission Control by Azul. The AdoptOpenJDK provides snapshot builds of JMC 8 as well as an Eclipse update site for installing JMC into an existing Eclipse instance.\n If you\u0026#8217;d like to follow along and run these steps yourself, check out the source code from GitHub and then perform the following commands:\n 1 2 cd example-service \u0026amp;\u0026amp; mvn clean package \u0026amp;\u0026amp; cd .. docker-compose up --build    This builds the project using Maven and spins up the following services using Docker Compose:\n   example-service: The Todo example application\n  todo-db: The Postgres database used by the Todo service\n  prometheus and grafana: For monitoring live events later on\n   Then go to http://localhost:8080/todo, where you should see the Todo web application:\n   Now fire up Mission Control. The example service run via Docker Compose is configured so you can connect to it on localhost. In the JVM Browser, create a new connection with host \"localhost\" and port \"1898\". Hit \"Test connection\", which should yield \"OK\", then click \"Finish\".\n   Create a new recording by expanding the localhost:1898 node in the JVM Explorer, right-clicking on \"Flight Recorder\" and choosing \"Start Flight Recording\u0026#8230;\u0026#8203;\". Confirm the default settings, which will create a recording with a duration of one minute. Go back to the Todo web application and perform a few tasks like creating some new todos, editing and deleting them, or filtering the todo list.\n Either wait for the recording to complete or stop it by right-clicking on the recording name and selecting \"Stop\". Once the recording is done, it will be opened automatically. Now you could dive into all the logged events for the OS, the JVM etc, but as we\u0026#8217;re interested in our custom JAX-RS events, Choose \"Event Browser\" in the outline view and expand the \"JAX-RS\" category. You will see the events for all your REST API invocations, including information such as duration of the request, the HTTP method, the resource path and much more:\n   In a real-world use case, you could now use this information for instance to identify long-running requests and correlate these events with other data points in the Flight Recording, such as method profiling and memory allocation data, or sub-optimal SQL statements in your database.\n     If your application is running in production, it might not be feasible to connect to it via Mission Control from your local workstation. The jcmd utility comes in handy in that case; part of the JDK, you can use it to issue diagnostic commands against a running JVM.\n Amongst many other things, it allows you to start and stop Flight Recordings. On the environment with your running application, first run jcmd -l, which will show you the PIDs of all running Java processes. Having identified the PID of the process you\u0026#8217;d like to examine, you can initiate a recording like so:\n 1 2 jcmd \u0026lt;PID\u0026gt; JFR.start delay=5s duration=30s \\ name=MyRecording filename=my-recording.jfr    This will start a recording of 30 seconds, beginning in 5 seconds from now. Once the recording is done, you could copy the file to your local machine and load it into Mission Control for further analysis. To learn more about creating Flight Recordings via jcmd, refer to this great cheat sheet.\n     Another useful tool in the belt is the jfr command, which was introduced in JDK 12. It allows you to filter and examine the binary Flight Recording files. You also can use it to extract parts of a recording and convert them to JSON, allowing them to be processed with other tools. E.g. you could convert all the JAX-RS events to JSON like so:\n 1 jfr print --json --categories JAX-RS my-recording.jfr      Event Settings Sometimes it\u0026#8217;s desirable to configure detailed behaviors of a given event type. For the JAX-RS invocation event it might for instance make sense to only log invocations of particular paths in a specific recording, allowing for a smaller recording size and keeping the focus on a particular subset of all invocations. JFR supports this by the notion of event settings. Such settings can be specified when creating a recording; based on the active settings, particular events will be included or excluded in the recording.\n Inspired by the JavaDoc of @SettingDefinition let\u0026#8217;s see what\u0026#8217;s needed to enhance JaxRsInvocationEvent with that capability. The first step is to define a subclass of jdk.jfr.SettingControl, which serves as the value holder for our setting:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public class PathFilterControl extends SettingControl { private Pattern pattern = Pattern.compile(\".*\"); (1) @Override (2) public void setValue(String value) { this.pattern = Pattern.compile(value); } @Override (3) public String combine(Set\u0026lt;String\u0026gt; values) { return String.join(\"|\", values); } @Override (4) public String getValue() { return pattern.toString(); } (5) public boolean matches(String s) { return pattern.matcher(s).matches(); } }      1 A regular expression pattern that\u0026#8217;ll be matched against the path of incoming events; by default all paths are included (.*)   2 Invoked by the JFR runtime to set the value for this setting   3 Invoked when multiple recordings are running at the same time, combining the settings values   4 Invoked by the runtime for instance when getting the default value of the setting   5 Matches the configured setting value against a particular path    On the event class itself a method with the following characteristics must be declared which will receive the setting by the JFR runtime:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 class JaxRsInvocationEvent extends Event { @Label(\"Path\") public String path; // other members... @Label(\"Path Filter\") @SettingDefinition (1) protected boolean pathFilter(PathFilterControl pathFilter) { (2) return pathFilter.matches(path); } }      1 Tags this as a setting   2 The method must be public, take a SettingControl type as its single parameter and return boolean    This method will be invoked by the JFR runtime during the shouldCommit() call. It passes in the setting value of the current recording so it can applied to the path value of the given event. In case the filter returns true, the event will be added to the recording, otherwise it will be ignored.\n We also could use such setting to control the inclusion or exclusion of specific event attributes. For that, the setting definition method would always have to return true, but depending on the actual setting it might set particular attributes of the event class to null. For instance this might come in handy if we wanted to log the entire request/response body of our REST API. Doing this all the time might be prohibitive in terms of recording size, but it might be enabled for a particlar short-term recording for analyzing some bug.\n Now let\u0026#8217;s see how the path filter can be applied when creating a new recording in Mission Control. The option is a bit hidden, but here\u0026#8217;s how you can enable it. First, create a new Flight Recording, then choose \"Template Manager\" in the dialogue:\n   Duplicate the \"Continuous\" template and edit it:\n   Click \"Advanced\":\n   Expand \"JAX-RS\" \u0026#8594; \"JAX-RS Invocation\" and put .*(new|edit).* into the Path Filter control:\n   Now close the last two dialogues. In the \"Start Flight Recording\" dialogue make sure to select your new template under \"Event Settings\"; although you\u0026#8217;ve edited it before, it won\u0026#8217;t be selected automatically. I lost an hour or so wondering why my settings were not applied\u0026#8230;\u0026#8203; .\n Lastly, click \"Finish\" to begin the recording:\n   Perform some tasks in the Todo web app and stop the recording. You should see only the REST API calls for the new and edit operations, whereas no events should be shown for the list and delete operations of the API.\n     In order to apply specific settings when creating a recording on the CLI using jcmd, edit the settings as described above. Then go to the Template Manager and export the profile you\u0026#8217;d like to use. When starting the recording via jcmd, specify the settings file via the settings=/path/to/settings.jfc parameter.\n       JFR Event Streaming Flight Recorder files are great for analyzing performance characteristics in an \"offline\" approach: you can take recordings in your production environment and ship them to your work station or a remote support team, without requiring live access to the running application. This is also an interesting mode for open-source projects, where maintainers typically don\u0026#8217;t have access to running applications of their users. Exchanging Flight Recordings (limited to a sensible subset of information, so to avoid exposure of confidential internals) might allow open source developers to gain insight into characteristics of their libraries when deployed to production at their users.\n But there\u0026#8217;s another category of use cases for event data sourced from applications, the JVM and the operating system, where the recording file approach doesn\u0026#8217;t quite fit: live monitoring and alerting of running applications. E.g. operations teams might want to set up dashboards showing the most relevant application metrics in \"real-time\", without having to create any recording files first. A related requirement is alerting, so to be notified when metrics reach a certain threshold. For instance it might be desirable to be alterted if the request duration of our JAX-RS API goes beyond a defined value such as 100 ms.\n This is where JEP 349 (\"JFR Event Streaming\") comes in. It\u0026#8217;ll be part of Java 14 and its stated goal is to \"provide an API for the continuous consumption of JFR data on disk, both for in-process and out-of-process applications\". That\u0026#8217;s exactly what we need for our monitoring/dashboarding use case. Using the Streaming API, Flight Recorder events of the running application can be exposed to external consumers, without having to explicitly load any recording files.\n Now it may be prohibitively expensive to stream each and every event with all its detailed information to remote clients. But that\u0026#8217;s not needed for monitoring purposes anyways. Instead, we can expose metrics based on our events, such as the total number and frequency of REST API invocations, or the average and 99th percentile duration of the calls.\n   MicroProfile Metrics The following shows a basic implementation of exposing these metrics for the JAX-RS API events to Prometheus/Grafana, where they can be visualized using a dashboard. Being based on Quarkus, the Todo web application can leverage all the MicroProfile APIs. On of them is the MicroProfile Metrics API, which defines a \"unified way for Microprofile servers to export Monitoring data (\"Telemetry\") to management agents\".\n While the MicroProfile Metrics API is used in an annotation-driven fashion often-times, it also provides a programmatic API for registering metrics. This can be leveraged to expose metrics based on the JAX-RS Flight Recorder events:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 @ApplicationScoped public class Metrics { @Inject (1) MetricRegistry metricsRegistry; private RecordingStream recordingStream; (2) public void onStartup(@Observes StartupEvent se) { recordingStream = new RecordingStream(); (3) recordingStream.enable(JaxRsInvocationEvent.NAME); recordingStream.onEvent(JaxRsInvocationEvent.NAME, event -\u0026gt; { (4) String path = event.getString(\"path\") .replaceAll(\"(\\\\/)([0-9]+)(\\\\/?)\", \"$1{param}$3\"); (5) String method = event.getString(\"method\"); String name = path + \"-\" + method; Metadata metadata = metricsRegistry.getMetadata().get(name); if (metadata == null) { metricsRegistry.timer(Metadata.builder() (6) .withName(name) .withType(MetricType.TIMER) .withDescription(\"Metrics for \" + path + \" (\" + method + \")\") .build()).update(event.getDuration().toNanos(), TimeUnit.NANOSECONDS); } else { (7) metricsRegistry.timer(name).update(event.getDuration() .toNanos(), TimeUnit.NANOSECONDS); } }); recordingStream.startAsync(); (8) } public void stop(@Observes ShutdownEvent se) { recordingStream.close(); (9) try { recordingStream.awaitTermination(); } catch (InterruptedException e) { throw new RuntimeException(e); } } }      1 Inject the MicroProfile Metrics registry   2 A stream providing push access to JFR events   3 Initialize the stream upon application start-up, so it includes the JAX-RS invocation events   4 For each JaxRsInvocationEvent this callback will be invoked   5 To register a corresponding metric, any path parameters are replaced with a constant placeholder, so that e.g. all invocations of the todo/{id}/edit path are exposed via one single metric instead of having separate ones for Todo 1, Todo 2 etc.   6 If the metric for the specific path hasn\u0026#8217;t been registered yet, then do so; it\u0026#8217;s a metric of type TIMER, allowing metric consumers to track the duration of calls of that particular path   7 If the metric for the path has been registered before, update its value with the duration of the incoming event   8 Start the stream asynchronously, not blocking the onStartup() method   9 Close the JFR event stream upon application shutdown    When connecting to the running application using JMC now, you\u0026#8217;ll see a continuous recording, which serves as the basis for the event stream. It only contains events of the JaxRsInvocationEvent type.\n MicroProfile Metrics exposes any application-provided metrics in the Prometheus format under the /metrics/application endpoint; for each operation of the REST API, e.g. POST to /todo/{id}/edit, the following metrics are provided:\n   request rate per second, minute, five minutes and 15 minutes\n  min, mean and max duration as well as standard deviation\n  total invocation count\n  duration of 75th, 95th, 99th etc. percentiles\n     Once the endpoint is provided, it\u0026#8217;s not difficult to set up a scraping process for ingesting the metrics into the Prometheus time-series database. You can find the required Prometheus configuration in the accompanying source code repository.\n While Prometheus provides some visualization capabilities itself, it is often used together with Grafana, which allows to build nicely looking dashboards via a rather intuitive UI. Here\u0026#8217;s an example dashboard showing the duration and invocation numbers for the different methods in the Todo REST API:\n   Again you can find the complete configuration for Grafana including the definition of that dashboard in the example repo. It will automatically be loaded when using the Docker Compose set-up shown above. Based on that you could easily expand the dashboard for other metrics and set up alerts, too.\n Combining the monitoring of live key metrics with the deep insights possible via detailed JFR recordings enable a very powerful workflow for analysing performance issues in production:\n   When setting up the continuous recording that serves as the basis for the metrics, have it contain all the event types you\u0026#8217;d need to gain insight into GC or memory issues etc.; specify a maximum size via RecordingStream#setMaxSize(), so to avoid an indefinitely growing recording; you\u0026#8217;ll probably need to experiment a bit to find the right trade-off between number of enabled events, duration that\u0026#8217;ll be covered by the recording and the required disk space\n  Only expose a relevant subset of the events as metrics to Prometheus/Grafana, such as the JAX-RS API invocation events in our example\n  Set up an alert in Grafana on the key metrics, e.g. mean duration of the REST calls, or 99th percentile thereof\n  If the alert triggers, take a dump of the last N minutes of the continuous recording via JMC or jcmd (using the JFR.dump command), and analyze that detailed recording to understand what was happening in the time leading to the alert\n     Summary and Related Work Flight Recorder and Mission Control are excellent tools providing deep insight into the performance characteristics of Java applications. While there\u0026#8217;s a large amount of data and highly valuable information provided out the box, JFR and JMC also allow for the recording of custom, application-specific events. With its low overhead, JFR can be enabled on a permanent basis in production environments. Combined with the Event Streaming API introduced in Java 14, this opens up an attractive, very performant alternative to other means of capturing analysis information at application runtime, such as logging libraries. Providing live key metrics derived from JFR events to tools such as Prometheus and Grafana enables monitoring and alerting in \"real-time\".\n For many enterprises that are still on Java 11 or even 8, it\u0026#8217;ll still be far out into the future until they might adopt the streaming API. But with more and more companies joining the OpenJDK efforts, it might be a possiblity that this useful feature gets backported to earlier LTS releases, just as the open-sourced version of Flight Recorder itself got backported to Java 8.\n There are quite a few posts and presentations about JFR and JMC available online, but many of them refer to older versions of those tools, before they got open-sourced. Here are some up-to-date resources which I found very helpful:\n   Continuous Monitoring with JDK Flight Recorder: a talk from QCon SF 2019 by Mikael Vidstedt\n  Flight Recorder \u0026amp; Mission Control at Code One 2019: a compilation of several great sessions on these two tools at last year\u0026#8217;s Code One, put together by Marcus Hirt\n  Digging Into Sockets With Java Flight Recorder: blog post by Petr Bouda on identifying performance bottlenecks with JFR in a Netty-based web application\n   Lastly, the Red Hat OpenJDK team is working on some very interesting projects around JFR and JMC, too. E.g. they\u0026#8217;ve built a datasource for Grafana which lets you examine the events of a JFR file. They also work on tooling to simplify the usage of JFR in container-based environments such as Kubernetes and OpenShift, including a K8s Operator for controlling Flight Recordings and a web-based UI for managing JFR in remote JVMs. Should you happen to be at the FOSDEM conference in Brussels on the next weekend, be sure to not miss the JMC \u0026amp; JFR - 2020 Vision session by Red Hat engineer Jie Kang.\n If you\u0026#8217;d like to experiment with JDK Flight Recorder and JDK Mission Control based on the Todo web application yourself, you can find the complete source code for this post on GitHub.\n Many thanks to Mario Torre and Jie Kang for reviewing an early draft of this post.\n  ","id":47,"publicationdate":"Jan 29, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe \u003ca href=\"https://openjdk.java.net/jeps/328\"\u003eJDK Flight Recorder\u003c/a\u003e (JFR) is an invaluable tool for gaining deep insights into the performance characteristics of Java applications.\nOpen-sourced in JDK 11, JFR provides a low-overhead framework for collecting events from Java applications, the JVM and the operating system.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIn this blog post we\u0026#8217;re going to explore how custom, application-specific JFR events can be used to monitor a REST API, allowing to track request counts, identify long-running requests and more.\nWe\u0026#8217;ll also discuss how the JFR \u003ca href=\"https://openjdk.java.net/jeps/349\"\u003eEvent Streaming API\u003c/a\u003e new in Java 14 can be used to export live events,\nmaking them available for monitoring and alerting via tools such as Prometheus and Grafana.\u003c/p\u003e\n\u003c/div\u003e","tags":["java","monitoring","microprofile","jakartaee","quarkus"],"title":"Monitoring REST APIs with Custom JDK Flight Recorder Events","uri":"https://www.morling.dev/blog/rest-api-monitoring-with-custom-jdk-flight-recorder-events/"},{"content":"","id":48,"publicationdate":"Jan 29, 2020","section":"tags","summary":"","tags":null,"title":"quarkus","uri":"https://www.morling.dev/tags/quarkus/"},{"content":"","id":49,"publicationdate":"Jan 29, 2020","section":"tags","summary":"","tags":null,"title":"Tags","uri":"https://www.morling.dev/tags/"},{"content":"","id":50,"publicationdate":"Jan 20, 2020","section":"tags","summary":"","tags":null,"title":"bean-validation","uri":"https://www.morling.dev/tags/bean-validation/"},{"content":"Record types are one of the most awaited features in Java 14; they promise to \"provide a compact syntax for declaring classes which are transparent holders for shallowly immutable data\". One example where records should be beneficial are data transfer objects (DTOs), as e.g. found in the remoting layer of enterprise applications. Typically, certain rules should be applied to the attributes of such DTO, e.g. in terms of allowed values. The goal of this blog post is to explore how such invariants can be enforced on record types, using annotation-based constraints as provided by the Bean Validation API.\n Record Invariants and Bean Validation Records (a preview feature as of Java 14) help to cut down the ceremony when defining plain data holder objects. In a nutshell, you solely need to declare the attributes that should make up the state of the record type (\"components\" in terms of JEP 359), and quite a few things you\u0026#8217;d otherwise have to implement by hand will be created for you automatically:\n   a private final field and a corresponding read accessor for each component\n  a constructor for passing in all component values\n  toString(), equals() and hashCode() methods.\n   As an example, here\u0026#8217;s a record Car with three components:\n 1 2 3 public record Car(String manufacturer, String licensePlate, int seatCount) { }    Now let\u0026#8217;s assume a few class invariants should be applied to this record (inspired by an example from the Hibernate Validator reference guide):\n   manufacturer is a non-blank string\n  license plate is never null and has a length of 2 to 14 characters\n  seatCount is at least 2\n   Class invariants like these are specific conditions or rules applying to the state of a class (as manifesting in its fields), which always are guaranteed to be satisfied for the lifetime of an instance of the class.\n The Bean Validation API defines a way for expressing and validating constraints using Java annotations. By putting constraint annotations to the components of a record type, it\u0026#8217;s a perfect means of describing the invariants from above:\n 1 2 3 4 5 public record Car( @NotBlank String manufacturer, @NotNull @Size(min = 2, max = 14) String licensePlate, @Min(2) int seatCount) { }    Of course declaring constraints using annotations by itself won\u0026#8217;t magically enforce these invariants. In order to do so, the javax.validation.Validator API must be invoked at suitable points in the object lifecycle, so to avoid any of the invariants to be violated. As records are immutable, it is sufficient to validate the constraints once when creating a new Car instance. If no constraints are violated, the created instance is guaranteed to always satisfy its invariants.\n   Implementation The key question now is how to validate the invariants while constructing new Car instances. This is where Bean Validation\u0026#8217;s API for method validation comes in: it allows to validate pre- and post-conditions that should be satisfied when a Java method or constructor gets invoked. Pre-conditions are expressed by applying constraints to method and constructor parameters, whereas post-conditions are expressed by putting constraints to a method or constructor itself.\n This can be leveraged for enforcing record invariants: as it turns out, any annotations on the components of a record type are also copied to the corresponding parameters of the generated constructor. I.e. the Car record implicitly has a constructor which looks like this:\n 1 2 3 4 5 6 7 8 9 public Car( @NotBlank String manufacturer, @NotNull @Size(min = 2, max = 14) String licensePlate, @Min(2) int seatCount) { this.manufacturer = manufacturer; this.licensePlate = licensePlate; this.seatCount = seatCount; }    That\u0026#8217;s exactly what we need: by validating these parameter constraints upon instantiation of the Car class, we can make sure that only valid objects can ever be created, ensuring that the record type\u0026#8217;s invariants are always guaranteed.\n What\u0026#8217;s missing is a way for automatically validating them upon constructor invocation. The idea for that is to enhance the byte code of the implicit Car constructor so that it passes the incoming parameter values to Bean Validation\u0026#8217;s ExecutableValidator#validateConstructorParameters() method and raises a constraint violation exception in case of any invalid parameter values.\n We\u0026#8217;re going to use the excellent ByteBuddy library for this job. Here\u0026#8217;s a slightly simplified implementation for invoking the executable validator (you can find the complete source code of this example in this GitHub repository):\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 public class ValidationInterceptor { private static final Validator validator = Validation (1) .buildDefaultValidatorFactory() .getValidator(); public static \u0026lt;T\u0026gt; void validate(@Origin Constructor\u0026lt;T\u0026gt; constructor, @AllArguments Object[] args) { (2) Set\u0026lt;ConstraintViolation\u0026lt;T\u0026gt;\u0026gt; violations = validator (3) .forExecutables() .validateConstructorParameters(constructor, args); if (!violations.isEmpty()) { String message = violations.stream() (4) .sorted(ValidationInterceptor::compare) .map(cv -\u0026gt; getParameterName(cv) + \" - \" + cv.getMessage()) .collect(Collectors.joining(System.lineSeparator())); throw new ConstraintViolationException( (5) \"Invalid instantiation of record type \" + constructor.getDeclaringClass().getSimpleName() + System.lineSeparator() + message, violations); } } private static int compare(ConstraintViolation\u0026lt;?\u0026gt; o1, ConstraintViolation\u0026lt;?\u0026gt; o2) { return Integer.compare(getParameterIndex(o1), getParameterIndex(o2)); } private static String getParameterName(ConstraintViolation\u0026lt;?\u0026gt; cv) { // traverse property path to extract parameter name } private static int getParameterIndex(ConstraintViolation\u0026lt;?\u0026gt; cv) { // traverse property path to extract parameter index } }      1 Obtain a Bean Validation Validator instance   2 The @Origin and @AllArguments annotations are the hint to ByteBuddy that the invoked constructor and parameter values should be passed to this method from within the enhanced constructor   3 Validate the passed constructor arguments using Bean Validation   4 If there\u0026#8217;s at least one violated constraint, create a message comprising all constraint violation messages, ordered by parameter index   5 Raise a ConstraintViolationException, containing the message created before as well as all the constraint violations    Having implemented the validation interceptor, the code of the record constructor must be enhanced by ByteBuddy, so that it invokes the inceptor. ByteBuddy provides different ways for doing so, e.g. at application start-up using a Java agent. For this example, we\u0026#8217;re going to employ build-time enhancement via the ByteBuddy Maven plug-in. The enhancement logic itself is implemented in a custom net.bytebuddy.build.Plugin:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 public class ValidationWeavingPlugin implements Plugin { @Override public boolean matches(TypeDescription target) { (1) return target.getDeclaredMethods() .stream() .anyMatch(m -\u0026gt; m.isConstructor() \u0026amp;\u0026amp; hasConstrainedParameter(m)); } @Override public Builder\u0026lt;?\u0026gt; apply(Builder\u0026lt;?\u0026gt; builder, TypeDescription typeDescription, ClassFileLocator classFileLocator) { return builder.constructor(this::hasConstrainedParameter) (2) .intercept(SuperMethodCall.INSTANCE.andThen( MethodDelegation.to(ValidationInterceptor.class))); } private boolean hasConstrainedParameter(MethodDescription method) { return method.getParameters() (3) .asDefined() .stream() .anyMatch(p -\u0026gt; isConstrained(p)); } private boolean isConstrained( ParameterDescription.InDefinedShape parameter) { (4) return !parameter.getDeclaredAnnotations() .asTypeList() .filter(hasAnnotation(annotationType(Constraint.class))) .isEmpty(); } @Override public void close() throws IOException { } }      1 Determines whether a type should be enhanced or not; this is the case if there\u0026#8217;s at least one constructor that has one more more constrained parameters   2 Applies the actual enhancement: into each constrained constructor the call to ValidationInterceptor gets injected   3 Determines whether a method or constructor has at least one constrained parameter   4 Determines whether a parameter has at least one constraint annotation (an annotation meta-annotated with @Constraint; for the sake of simplicity the case of constraint inheritance is ignored here)    The next step is to configure the ByteBuddy Maven plug-in in the pom.xml of the project:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;net.bytebuddy\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;byte-buddy-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${version.bytebuddy}\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;transform\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;transformations\u0026gt; \u0026lt;transformation\u0026gt; \u0026lt;plugin\u0026gt; dev.morling.demos.recordvalidation.implementation.ValidationWeavingPlugin \u0026lt;/plugin\u0026gt; \u0026lt;/transformation\u0026gt; \u0026lt;/transformations\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt;    This plug-in runs in the process-classes phase by default, so it can access and enhance the class files generated during compilation. If you were to build the project now, you could use the javap tool to examine the byte code of the Car class,and you\u0026#8217;d see that the implicit constructor of that class contains an invocation of the ValidationInterceptor#validate() method.\n As an example, let\u0026#8217;s consider the following attempt to instantiate a Car object, which violates the invariants of that record type:\n 1 Car invalid = new Car(\"\", \"HH-AB-123\", 1);    A constraint violation like this will be thrown immediately:\n 1 2 3 4 5 javax.validation.ConstraintViolationException: Invalid instantiation of record type Car manufacturer - must not be blank seatCount - must be greater than or equal to 2 at dev.morling.demos.recordvalidation.RecordValidationTest.canValidate(RecordValidationTest.java:20)    If all constraints are satisfied, no exception will be thrown and the caller obtains the new Car instance, whose invariants are guaranteed to be met for the remainder of the object\u0026#8217;s lifetime.\n   Advantages Having shown how Bean Validation can be leveraged to enforce the invariants of Java record types, it is time to reflect: is this this approach worth the additional complexity incurred by adding a library such as Bean Validation and hooking it up using byte code enhancement? After all, you could also validate incoming parameter values using methods such as Objects#requireNonNull().\n As so often, you need to make such decision based on your specific requirements and needs. Here are some advantages I can see about the Bean Validation approach:\n   Invariants become part of the API: Constraint annotations on public API members such as the implicit record constructor are easily discoverable by users of such type; they are listed in generated JavaDoc, you can see them when hovering over an invocation in your IDE (once records are supported); when used on the DTOs of a REST layer, the invariants could also be added to automatically generated API documentation. All this makes it easy for users of the type to understand the invariants and also avoids potential inconsistencies between a manual validation implementation and corresponding hand-written documentation\n  Providing constraint metadata: The Bean Validation constraint meta-data API can be used to obtain information about the constraints of Java types; for instance this can be used to implement client-side validation of constraints in a web application\n  Less code: Putting constraint annotations directly to the record components themselves avoids the need for implementing these checks manually in an explicit canonical constructor\n  I18N support: Bean Validation provides means of internationalizing constraint violation messages; if your record types are instantiated based on user input (e.g. when using them as data types in a REST API), this allows for localized error messages in the UI\n  Returning all constraints at once: For UIs it\u0026#8217;s typically beneficial to return all the constraint violations at once instead of showing them one by one; while doable in a hand-written implementation, it requires a bit of effort, whereas you get this \"for free\" when using Bean Validation which always returns a set of all the violations\n  Lots of ready-made constraints: Bean Validation comes with a range of constraints out of the box; in addition libraries such as Hibernate Validator and others provide many more ready-to-use constraints, coming in handy for instance when implementing domain-specific value types with complex validation rules:\n1 2 3 public record EmailAddress( @Email @NotNull @Size(min=1, max=250) String value) { }      Support for validation groups: Bean Validation\u0026#8217;s concept of validation groups allows you to validate only sub-sets of constraints in specific contexts; e.g. based on location and applying legal requirements\n  Dynamic constraint definition: Using Hibernate Validator, constraints can also be declared dynamically using a fluent API. This can be very useful when your validation requirements vary at runtime, e.g. if you need to apply different constraint configurations for different tenants.\n     Limitations One area where this current proof-of-concept implementation falls a bit short is the validation of invariants that apply to multiple components. For instance consider a record type representing an interval with a begin and an end attribute, where you\u0026#8217;d like to enforce the invariant that end is larger than begin.\n Bean Validation addresses this sort of requirement via class-level constraints and, for method and constructor validation, cross-parameter constraints. Class-level constraints are not really suitable for our purposes, because we want to validate the invariants before an object instance is created.\n Cross-parameter constraints on the other hand are exactly what we\u0026#8217;d need. As they must be given on a constructor or method, the canonical constructor of a record must be explicitly declared in this case. Using Hibernate Validator\u0026#8217;s @ParameterScriptAssert constraint, the invariant from above could be expressed like so:\n 1 2 3 4 5 6 public record Interval(int begin, int end) { @ParameterScriptAssert(lang=\"javascript\", script=\"end \u0026gt; begin\") public Interval { } }    This works as expected, but there\u0026#8217;s one caveat: any annotations from the record components are not propagated to the corresponding parameters of the canoncial constructor in this case. This means that any constraints given on the individual components would be lost. Right now it\u0026#8217;s not quite clear to me whether that\u0026#8217;s an intended behavior or rather a bug in the current record implementation.\n If indeed it is intentional, than there\u0026#8217;d be no way other than specifying the constraints explicitly on the parameters of a fully manually implemented constructor:\n 1 2 3 4 5 6 7 8 public record Interval(int begin, int end) { @ParameterScriptAssert(lang=\"javascript\", script=\"end \u0026gt; begin\") public Interval(@Positive int begin, @Positive int end) { this.begin = begin; this.end = end; } }    This works, but of course we\u0026#8217;re losing a bit of the conciseness promised by records.\n Update, Jan 20, 2020, 20:57: Turns out, the current behavior indeed is not intended (see JDK-8236597) and in a future Java version the shorter version of the code shown above should work.\n   Wrap-Up In this blog post we\u0026#8217;ve explored how invariants on Java 14 record types can be enforced using the Bean Validation API. With just a bit of byte code magic the task gets manageable: by validating invariants expressed by constraint annotations on record components right at instantiation time, only valid record instances will ever be exposed to callers. Key for that is the fact that any annotations from record components are automatically propagated to the corresponding parameters of the canonical record constructor. That way they can be validated using Bean Validation\u0026#8217;s method validation API. It remains to be seen, whether invariants based on multiple record components also can be enforced as easily.\n From the perspective of the Bean Validation specification, it\u0026#8217;ll surely make sense to explore support for record types. While not as powerful as enforcing invariants at construction time via byte code enhancement, it might also be useful to support the validation of component values via their read accessors. For that, the notion of \"properties\" would have to be relaxed, as the read accessors of records don\u0026#8217;t have the JavaBeans get prefix currently expected by Bean Validation. It also should be considered to expand the Bean Validation metadata API accordingly.\n I would also be very happy to learn about your thoughts around this topic. While Bean Validation 3.0 (as part of Jakarta EE 9) in all likelyhood won\u0026#8217;t bring any changes besides the transition to the jakarta.* package namespace, this may be an area where we could evolve the specification for Jakarta EE 10.\n If you\u0026#8217;d like to experiment with the validation of record types yourself, you can find the complete source code on GitHub.\n   ","id":51,"publicationdate":"Jan 20, 2020","section":"blog","summary":"Record types are one of the most awaited features in Java 14; they promise to \"provide a compact syntax for declaring classes which are transparent holders for shallowly immutable data\". One example where records should be beneficial are data transfer objects (DTOs), as e.g. found in the remoting layer of enterprise applications. Typically, certain rules should be applied to the attributes of such DTO, e.g. in terms of allowed values. The goal of this blog post is to explore how such invariants can be enforced on record types, using annotation-based constraints as provided by the Bean Validation API.","tags":["bean-validation","jakartaee"],"title":"Enforcing Java Record Invariants With Bean Validation","uri":"https://www.morling.dev/blog/enforcing-java-record-invariants-with-bean-validation/"},{"content":"When Java 9 was introduced in 2017, it was the last major version published under the old release scheme. Since then, a six month release cadence has been adopted. This means developers don\u0026#8217;t have to wait years for new APIs and language features, but they can get their hands onto the latest additions twice a year. In this post I\u0026#8217;d like to describe how you can try out new language features such as Java 13 text blocks in the test code of your project, while keeping your main code still compatible with older Java versions.\n One goal of the increased release cadence is to shorten the feedback loop for the OpenJDK team: have developers in the field try out new functionality early on, collect feedback based on that, adjust as needed. To aid with that process, the JDK has two means of publishing preliminary work before new APIs and language features are cast in stone:\n   Incubator JDK modules\n  Preview language and VM features\n   An example for the former is the new HTTP client API, which was an incubator module in JDK 9 and 10, before it got standardized as a regular API in JDK 11. Examples for preview language features are switch expressions (added as a preview feature in Java 12) and text blocks (added in Java 13).\n Now especially text blocks are a feature which many developers have missed in Java for a long time. They are really useful when embedding other languages, or just any kind of longer text into your Java program, e.g. multi-line SQL statements, JSON documents and others. So you might want to go and use them as quickly as possible, but depending on your specific situation and requirements, you may no be able to move to Java 13 just yet.\n In particular when working on libraries, compatibility with older Java versions is a high priority in order to not cut off a large number of potential users. E.g. in the JetBrains Developer Ecosystem Survey from early 2019, 83% of participants said that Java 8 is a version they regularly use. This matches with what I\u0026#8217;ve observed myself during conversations e.g. at conferences. Now this share may have reduced a bit since then (I couldn\u0026#8217;t find any newer numbers), but at this point in time it still seems save to say that libraries should support Java 8 to not limit their audience in a signficant way.\n So while building on Java 13 is fine, requiring it at runtime for libraries isn\u0026#8217;t. Does this mean as a library author you cannot use text blocks then for many years to come? For your main code (i.e. the one shipped to users) it indeed does mean that, but things look different when it comes to test code.\n An Example One case where text blocks come in extremely handy is testing of REST APIs, where JSON requests need to created and responses may have to be compared to a JSON string with the expected value. Here\u0026#8217;s an example of using text blocks in a test of a Quarkus-based REST service, implemented using RESTAssured and JSONAssert:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @QuarkusTest public class TodoResourceTest { @Test public void canPostNewTodoAndReceiveId() throws Exception { given() .when() .body(\"\"\" (1) { \"title\" : \"Learn Java\", \"completed\" : false } \"\"\" ) .contentType(ContentType.JSON) .post(\"/hello\") .then() .statusCode(201) .body(matchesJson(\"\"\" (2) { \"id\" : 1, \"title\" : \"Learn Java\", \"completed\" : false } \"\"\") ); } }      1 Text block with the JSON request to send   2 Text block with the expected JSON response    Indeed that\u0026#8217;s much nicer to read, e.g. when comparing the request JSON to the code you\u0026#8217;d typically write without text blocks. Concatenating multiple lines, escaping quotes and explicitly specifying line breaks make this quite cumbersome:\n 1 2 3 4 5 6 .body( \"{\\n\" + \" \\\"title\\\" : \\\"Learn Java 13\\\",\\n\" + \" \\\"completed\\\" : false\\n\" + \"}\" )    Now let\u0026#8217;s see what\u0026#8217;s needed in terms of configuration to enable usage of Java 13 text blocks for tests, while keeping the main code of a project compatible with Java 8.\n   Configuration Two options of the Java compiler javac come into play here:\n   --release: specifies the Java version to compile for\n  --enable-preview: allows to use language features currently in \"preview\" status such as text blocks as of Java 13/14\n       The --release option was introduced in Java 9 and should be preferred over the more widely known pair of --source and --target. The reason being that --release will prevent any accidental usage of APIs only introduced in later versions.\n E.g. say you were to write code such as List.of(\"Foo\", \"Bar\"); the of() methods on java.util.List were only introduced in Java 9, so compiling with --release 8 will raise a compilation error in this case. When using the older options, this situation wouldn\u0026#8217;t be detected at compile time, making the problem only apparent when actually running the application on the older Java version.\n     Build tools typically allow to use different configurations for the compilation of main and test code. E.g. here is what you\u0026#8217;d use for Maven (you can find the complete source code of the example in this GitHub repo):\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 ... \u0026lt;properties\u0026gt; ... \u0026lt;maven.compiler.release\u0026gt;8\u0026lt;/maven.compiler.release\u0026gt; (1) ... \u0026lt;/properties\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; ... \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.8.1\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;default-testCompile\u0026lt;/id\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;release\u0026gt;13\u0026lt;/release\u0026gt; (2) \u0026lt;compilerArgs\u0026gt;--enable-preview\u0026lt;/compilerArgs\u0026gt; (3) \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; ... \u0026lt;/plugins\u0026gt; ... \u0026lt;/build\u0026gt; ...      1 Compile for release 8 by default, i.e. the main code   2 Compile test code for release 13   3 Also pass the --enable-preview option when compiling the test code    Also at runtime preview features must be explicitly enabled. Therefore the java command must be accordingly configured when executing the tests, e.g. like so when using the Maven Surefire plug-in:\n 1 2 3 4 5 6 7 8 9 ... \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-surefire-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.22.1\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;argLine\u0026gt;--enable-preview\u0026lt;/argLine\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; ...    With this configuration in place, text blocks can now be used in tests as the one above, but not in the main code of the program. Doing so would result in a compilation error.\n Note your IDE might still let you do this kind of mistake. At least Eclipse chose for me the maximum of main (8) and test code (13) release levels when importing the project. But running the build on the command line via Maven or on your CI server will detect this situation.\n As Java 13 now is required to build this code base, it\u0026#8217;s a good idea to make this prerequisite explicit in the build process itself. The Maven enforcer plug-in comes in handy for that, allowing to express this requirement using its Java version rule:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ... \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-enforcer-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0-M3\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;enforce-java\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;enforce\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;rules\u0026gt; \u0026lt;requireJavaVersion\u0026gt; \u0026lt;version\u0026gt;[13,)\u0026lt;/version\u0026gt; \u0026lt;/requireJavaVersion\u0026gt; \u0026lt;/rules\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; ...    The plug-in will fail the build when being run on a version before Java 13.\n   Should You Do This? Having seen how you can use preview features in test code, the question is: should you actually do this? A few things should be kept in mind for answering that. First of all, preview features are really that, a preview. This means that details may change in future Java revisions. Or, albeit unlikely, such feature may even be dropped altogether, should the JDK team arrive at the conclusion that it is fundamentally flawed.\n Another important factor is the minimum Java language version supported by the JDK compiler. As of Java 13, the oldest supported release is 7; i.e. using JDK 13, you can produce byte code that can be run with Java versions as old as Java 7. In order to keep the Java compiler maintainable, support for older versions is dropped every now and then. Right now, there\u0026#8217;s no formal process in place which would describe when support for a specific version is going to be removed (defining such policy is the goal of JEP 182).\n As per JDK developer Joe Darcy, \"there are no plans to remove support for --release 7 in JDK 15\". Conversely, this means that support for release 7 theoretically could be removed in JDK 16 and support for release 8 could be removed in JDK 17. In that case you\u0026#8217;d be caught between a rock and a hard place: Once you\u0026#8217;re on a non-LTS (\"long-term support\") release like JDK 13, you\u0026#8217;ll need to upgrade to JDK 14, 15 etc. as soon as they are out, in order to not be cut off from bug fixes and security patches. Now while doing so, you\u0026#8217;d be forced to increase the release level of your main code, once support for release 8 gets dropped, which may not desirable. Or you\u0026#8217;d have to apply some nice awk/sed magic to replace all those shiny text blocks with traditional concatenated and escaped strings, so you can go back to the current LTS release, Java 11. Not nice, but surely doable.\n That being said, this all doesn\u0026#8217;t seem like a likely scenario to me. JEP 182 expresses a desire \"that source code 10 or more years old should still be able to be compiled\"; hence I think it\u0026#8217;s save to assume that JDK 17 (the next release planned to receive long-term support) will still support release 8, which will be seven years old when 17 gets released as planned in September 2021. In that case you\u0026#8217;d be on the safe side, receiving update releases and being able to keep your main code Java 8 compatible for quite a few years to come.\n Needless to say, it\u0026#8217;s a call that you need to make, deciding for yourself wether the benefits of using new language features such as text blocks is worth it in your specific situation or not.\n  ","id":52,"publicationdate":"Jan 13, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eWhen Java 9 was introduced in 2017,\nit was the last major version published under the old release scheme.\nSince then, a \u003ca href=\"https://www.infoq.com/news/2017/09/Java6Month/\"\u003esix month release cadence\u003c/a\u003e has been adopted.\nThis means developers don\u0026#8217;t have to wait years for new APIs and language features,\nbut they can get their hands onto the latest additions twice a year.\nIn this post I\u0026#8217;d like to describe how you can try out new language features such as \u003ca href=\"http://openjdk.java.net/jeps/355\"\u003eJava 13 text blocks\u003c/a\u003e in the test code of your project,\nwhile keeping your main code still compatible with older Java versions.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Using Java 13 Text Blocks (Only) for Your Tests","uri":"https://www.morling.dev/blog/using-java-13-text-blocks-for-tests/"},{"content":"One of the long-awaited features in Quarkus was support for server-side templating: until recently, Quarkus supported only client-side web frameworks which obtain there data by calling a REST API on the backend. This has changed with Quarkus 1.1: it comes with a brand-new template engine named Qute, which allows to build web applications using server-side templates.\n When looking at frameworks for building web applications, there\u0026#8217;s two large categories:\n   client-side solutions based on JavaScript such as React, vue.js or Angular\n  server-side frameworks such as Spring Web MVC, JSF or MVC 1.0 (in the Java world)\n   Both have their indivdual strengths and weaknesses and it\u0026#8217;d be not very wise to always prefer one over the other. Instead, the choice should be based on specific requirements (e.g. what kind of interactivity is needed) and prerequisites (e.g. the skillset of the team building the application).\n Being mostly experienced with Java, server-side solutions are appealing to me, as they allow me to use the language I know and tooling (build tools, IDEs) I\u0026#8217;m familiar and most productive with. So when Qute was announced, it instantly caught my attention and I had to give it a test ride. In this post I want to share some of the experiences I made.\n Note this isn\u0026#8217;t a comprehensive tutorial for building web apps with Qute, instead, I\u0026#8217;d like to discuss a few things that stuck out to me. You can find a complete working example here on GitHub. It implements a basic CRUD application for managing personal todos, persisted in a Postgres database. Here\u0026#8217;s a video that shows the demo in action:\n    The Basics The Qute engine is based on RESTEasy/JAX-RS. As such, Qute web applications are implemented by defining resource types with methods answering to specific HTTP verbs and accept headers. The only difference being, that HTML pages are returned instead of JSON as in your typical REST-ful data API. The individual pages are created by processing template files. Here\u0026#8217;s a basic example for returning all the Todo records in our application:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 @Path(\"/todo\") public class TodoResource { @Inject Template todos; @GET (1) @Consumes(MediaType.TEXT_HTML) (2) @Produces(MediaType.TEXT_HTML) public TemplateInstance listTodos() { return todos.data(\"todos\", Todo.findAll().list()); (3) } }      1 Processes HTTP GET requests for /todo   2 This method consumes and produces the text/html media type   3 Obtain all todos from the database and feed them to the todos template    The Todo class is as JPA entity implemented via Hibernate Panache:\n 1 2 3 4 5 6 7 @Entity public class Todo extends PanacheEntity { public String title; public int priority; public boolean completed; }    Panache is a perfect fit for this kind of CRUD applications. It helps with common tasks such as id mapping, and by means of the active record pattern you get query methods like findAll() \"for free\".\n To produce an HTML page for displaying the result list, the todos template is used. Templates are located under src/main/resources/templates. As you would expect it, changes to template files are immediatly picked up when running Quarkus in Dev Mode. By default, the template name is derived from the field name of the injected Template instance, i.e. in this case the src/main/resources/templates/todos.html template will be used. It could look like this:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\"en\"\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\"utf-8\"\u0026gt; \u0026lt;!-- CSS ... --\u0026gt; \u0026lt;link rel=\"stylesheet\" href=\"...\"\u0026gt; \u0026lt;title\u0026gt;My Todos\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\"container\"\u0026gt; \u0026lt;h1\u0026gt;My Todos\u0026lt;/h1\u0026gt; \u0026lt;table class=\"table table-striped table-bordered\"\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th scope=\"col\" class=\"fit\"\u0026gt;Id\u0026lt;/th\u0026gt; \u0026lt;th scope=\"col\" \u0026gt;Title\u0026lt;/th\u0026gt; \u0026lt;th scope=\"col\" class=\"fit\"\u0026gt;Priority\u0026lt;/th\u0026gt; \u0026lt;th scope=\"col\" class=\"fit\"\u0026gt;Completed\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; {#if todos.size == 0} (1) \u0026lt;tr\u0026gt; \u0026lt;td colspan=\"4\"\u0026gt;No data found.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; {#else} {#for todo in todos} (2) \u0026lt;tr\u0026gt; \u0026lt;th scope=\"row\"\u0026gt;#{todo.id}\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt; {todo.title} (3) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; {todo.priority} (4) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; (5) \u0026lt;div class=\"custom-control custom-checkbox\"\u0026gt; \u0026lt;input type=\"checkbox\" class=\"custom-control-input\" disabled id=\"completed-{todo.id}\" {#if todo.completed}checked{/if}\u0026gt; \u0026lt;label class=\"custom-control-label\" for=\"completed-{todo.id}\"\u0026gt;\u0026lt;/label\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; {/for} {/if} \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;      1 If the injected todos list is empty, display a placeholder row   2 Otherwise, iterate over the todos list and add a table row for each one   3 Table cell for title   4 Table cell for priority   5 Table cell for completion status, rendered as a checkbox    If you\u0026#8217;ve worked with other templating engine before, this will look very familiar to you. You can refer to injected objects and their properties to display their values, have conditional logic, iterate over collections etc. A very nice aspect about Qute templates is that they are processed at build time, following the Quarkus notion of \"compile-time boot\". This means if there is an error in a template such as unbalanced control keywords, you\u0026#8217;ll find out about this at build time instead of only at runtime.\n The reference documentation describes the syntax and all options in depth. Note that things are still in flux here, e.g. I couldn\u0026#8217;t work with boolean operators in conditions.\n   Combining HTML and Data APIs Thanks to HTTP content negotiation, you can easily combine resource methods for returning HTML and JSON for API-style consumers in a single endpoint. Just add another resource method for handling the required media type, e.g. \"application/json\":\n 1 2 3 4 5 6 @GET @Produces(MediaType.APPLICATION_JSON) @Consumes(MediaType.APPLICATION_JSON) public List\u0026lt;Todo\u0026gt; listTodosJson() { return Todo.findAll().list(); }    A standard HTTP request issued by a web browser would now be answered with the HTML page, whereas an AJAX request with the \"application/json\" accept header (or a manual invocation via curl) would yield the JSON representation. I really like that idea of considering HTML and JSON-based representations as two different \"views\" of the same API essentially.\n   Template Organization If a web application has multiple pages or \"views\", chances are there are many similarities between those. E.g. there might be a common header and footer for all pages, or one and the same form is used on multiple pages.\n To avoid duplication in the templates in such cases, Qute supports the notion of includes. E.g. let\u0026#8217;s say there\u0026#8217;s a common form for creating new and editing existing todos. This can be put into its own template:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 (1) \u0026lt;form action=\"/todo/{#if update}{todo.id}/edit{#else}new{/if}\" method=\"POST\" name=\"todoForm\" enctype=\"multipart/form-data\"\u0026gt; \u0026lt;div class=\"form-row align-items-center\"\u0026gt; \u0026lt;div class=\"col-sm-3 my-1\"\u0026gt; \u0026lt;label class=\"sr-only\" for=\"title\"\u0026gt;Title\u0026lt;/label\u0026gt; (2) \u0026lt;input type=\"text\" name=\"title\" class=\"form-control\" id=\"title\" placeholder=\"Title\" required autofocus {#if update}value=\"{todo.title}\"{/if}\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"col-auto my-1\"\u0026gt; \u0026lt;select class=\"custom-select\" name=\"priority\"\u0026gt; \u0026lt;option disabled value=\"\"\u0026gt;Priority\u0026lt;/option\u0026gt; {#for prio in priorities} \u0026lt;option value=\"{prio}\" {#if todo.priority == prio}selected{/if}\u0026gt;{prio}\u0026lt;/option\u0026gt; {/for} \u0026lt;/select\u0026gt; \u0026lt;/div\u0026gt; (3) {#if update} \u0026lt;div class=\"col-auto my-1\"\u0026gt; \u0026lt;div class=\"form-check\"\u0026gt; \u0026lt;input type=\"checkbox\" name=\"completed\" class=\"form-check-input\" id=\"completed\" {#if todo.completed}checked{/if}\u0026gt; \u0026lt;label class=\"form-check-label\" for=\"completed\"\u0026gt;Completed\u0026lt;/label\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {/if} (4) \u0026lt;button type=\"submit\" class=\"btn btn-primary\"\u0026gt;{#if update}Update{#else}Create{/if}\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt;      1 Post to different path for update and create   2 Display existing title and priority in case of an update   3 Show checkbox for completion status in case of an update   4 Choose button caption depending on use case    In order to display this form right under the table with all todos, the template can simply be included like so:\n 1 2 \u0026lt;h2\u0026gt;New Todo\u0026lt;/h2\u0026gt; {#include todo-form.html}{/include}    It\u0026#8217;s also possible to extract the outer shell of multiple pages into a shared template (\"template inheritance\"). This allows to extract common headers and footers into one single template with placeholders for the inner parts.\n For that, create a template with the common outer structure:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\"en\"\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\"utf-8\"\u0026gt; \u0026lt;!-- CSS ... --\u0026gt; \u0026lt;link rel=\"stylesheet\" href=\"...\"\u0026gt; \u0026lt;title\u0026gt;{#insert title}Default Title{/}\u0026lt;/title\u0026gt; (1) \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\"container\"\u0026gt; \u0026lt;h1\u0026gt;{#insert title}Default Title{/}\u0026lt;/h1\u0026gt; (1) {#insert contents}No contents!{/} (2) \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;      1 Derived templates define a section title which will be inserted here   2 Derived templates define a section contents which will be inserted here    Other templates can then extend the base one, e.g. like so for the \"Edit Todo\" page:\n 1 2 3 4 5 6 {#include base.html} (1) {#title}Edit Todo #{todo.id}{/title} (2) {#contents} (3) {#include todo-form.html}{/include} (4) {/contents} {/include}      1 Include the base template   2 Define the title section   3 Define the contents section   4 Include the template for displaying the todo form    As so often, a balance needs to be found between extracting common parts and still being able to comprehend the overall structure without having to pursue a large number of template references. But in any case with includes and inserts Qute puts the neccessary tools into your hands.\n   Error Handling For a great user experience robust error handling is a must. E.g. might happen that a user loads the \"Edit Todo\" dialog and while they\u0026#8217;re in the process of editing, that record gets deleted by someone else. When saving, a proper error message should be displayed to the first user. Here\u0026#8217;s the resource method implementation for that:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @POST @Consumes(MediaType.MULTIPART_FORM_DATA) @Transactional @Path(\"/{id}/edit\") public Object updateTodo( @PathParam(\"id\") long id, @MultipartForm TodoForm todoForm) { Todo loaded = Todo.findById(id); (1) if (loaded == null) { (2) return error.data(\"error\", \"Todo with id \" + id + \" has been deleted after loading this form.\"); } loaded = todoForm.updateTodo(loaded); (3) return Response.status(301) (4) .location(URI.create(\"/todo\")) .build(); }      1 Load the todo record to be updated   2 If it doesn\u0026#8217;t exist, render the \"error\" template   3 Otherwise, update the record; as loaded is an attached entity, no call to persist is needed   4 redirect the user to the main page, avoiding issues with reloading etc. (post-redirect-get pattern)    Note that TemplateInstance as returned from the Template#data() method doesn\u0026#8217;t extend the JAX-RS Response class. Therefore the return type of the method must be declared as Object in this case.\n   Search Thanks to Hibernate Panache it\u0026#8217;s quite simple to refine the todo list and only return those whose title matches a given search term. Also ordering the list in some meaningful way would be nice. All we need is an optional query parameter for specifying the search term and a custom query method:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @GET @Consumes(MediaType.TEXT_HTML) @Produces(MediaType.TEXT_HTML) public TemplateInstance listTodos(@QueryParam(\"filter\") String filter) { return todos.data(\"todos\", find(filter)); } @GET @Produces(MediaType.APPLICATION_JSON) @Consumes(MediaType.APPLICATION_JSON) public List\u0026lt;Todo\u0026gt; listTodosJson(@QueryParam(\"filter\") String filter) { return find(filter); } private List\u0026lt;Todo\u0026gt; find(String filter) { Sort sort = Sort.ascending(\"completed\") (1) .and(\"priority\", Direction.Descending) .and(\"title\", Direction.Ascending); if (filter != null \u0026amp;\u0026amp; !filter.isEmpty()) { (2) return Todo.find(\"LOWER(title) LIKE LOWER(?1)\", sort, \"%\" + filter + \"%\").list(); } else { return Todo.findAll(sort).list(); (3) } }      1 First sort by completion status, then priority, then by title   2 If a filter is given, apply the search term lower-cased and with wildcards, i.e. using a WHERE clause such as where lower(todo0_.title) like lower(%searchterm%)   3 Otherwise, return all todos    To enter the search term, a form is added next to the table of todos:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 (1) \u0026lt;form action=\"/todo\" method=\"GET\" name=\"search\"\u0026gt; \u0026lt;div class=\"form-row align-items-center\"\u0026gt; \u0026lt;div class=\"col-sm-3 my-1\"\u0026gt; \u0026lt;label class=\"sr-only\" for=\"filter\"\u0026gt;Search\u0026lt;/label\u0026gt; (2) \u0026lt;input type=\"text\" name=\"filter\" class=\"form-control\" id=\"filter\" placeholder=\"Search By Title\" required {#if filtered}value=\"{filter}\"{/if}\u0026gt; \u0026lt;/div\u0026gt; (3) \u0026lt;input class=\"btn btn-primary\" value=\"Search\" type=\"submit\"\u0026gt;\u0026amp;nbsp; \u0026lt;a class=\"btn btn-secondary {#if !filtered}disabled{/if}\" href=\"/todo\" role=\"button\"\u0026gt;Clear Filter\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt;      1 Invoke this page with the entered search as query parameter   2 Input for the search term; show the previously entered term, if any   3 A button for clearing the result list if a search term has been entered; otherwise the button will be disabled      Smoother User Experience via Unpoly The last thing I wanted to explore is how the usability and performance of the application can be improved by means of some client-side enhancements. By default, a web app rendered on the server-side like ours requires full page loads when going from one page to the other. This is where single page applications (SPAs) implemented with client-side frameworks shine: just parts of the document object model tree in the browser will be replaced e.g. when loading a result list via AJAX, resulting in a much smoother and faster user experience.\n Does this mean we have to give up on server-side rendering altogether if we\u0026#8217;re after this kind of UX? Luckily not, as small helper libraries such as Unpoly, Intercooler or Turbolinks can be leveraged to replace just page fragments instead of requiring full page loads. This results in a smooth SPA-like user experience without having to opt into the full client-side programming model. For the Todo example I\u0026#8217;ve obtained great results using Unpoly. After importing its JavaScript file, all that\u0026#8217;s needed is to add the up-target attribute to links or forms.\n E.g. here\u0026#8217;s the form for entering the search term with that modification:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 (1) \u0026lt;form action=\"/todo\" method=\"GET\" name=\"search\" up-target=\".container\"\u0026gt; \u0026lt;div class=\"form-row align-items-center\"\u0026gt; \u0026lt;div class=\"col-sm-3 my-1\"\u0026gt; \u0026lt;label class=\"sr-only\" for=\"filter\"\u0026gt;Search\u0026lt;/label\u0026gt; \u0026lt;input type=\"text\" name=\"filter\" class=\"form-control\" id=\"filter\" placeholder=\"Search By Title\" required {#if filtered}value=\"{filter}\"{/if}\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;input class=\"btn btn-primary\" value=\"Search\" type=\"submit\"\u0026gt;\u0026amp;nbsp; (2) \u0026lt;a class=\"btn btn-secondary {#if !filtered}disabled{/if}\" href=\"/todo\" role=\"button\" up-target=\".container\"\u0026gt;Clear Filter\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt;      1 When receiving the result of the form submission, replace the \u0026lt;div\u0026gt; with CSS class container of the current page with the one from the response   2 Do the same when following the \"Clear Filter\" link    The magic trick of Unpoly is that links and forms with the up-target attribute are intercepted by Unpoly and executed via AJAX calls. The specified fragments from the result page are then used to replace parts of the already loaded page, instead of having the browser load the full response page. The result is the fast user experience shown in the video above.\n Unpoly also allows to show page fragments in modal dialogs, allowing to remain on the same page also when showing forms such as the one for editing a todo:\n   Note that if JavaScript is disabled, the application gracefully falls back to full page loads. I.e. it will still be fully functional, just with a slightly degraded user experience. The same would happen when accessing the edit dialog directly via its URL or when opening the \"Edit\" link in a new tab or window:\n     Bonus: Using WebJars In a thread on Twitter James Ward brought up the idea of pulling in required resources such as Bootstrap via WebJars instead of getting them from a CDN. WebJars is a useful utility for obtaining all sorts of client-side libraries with Java build tools such as Maven or Gradle.\n For Bootstrap, the following dependency must be added to the Maven pom.xml file:\n 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.webjars\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;bootstrap\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.4.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;    The Bootstrap CSS can then be included within the base.html template like so:\n 1 2 3 4 5 6 7 ... \u0026lt;head\u0026gt; ... \u0026lt;link rel=\"stylesheet\" href=\"/webjars/bootstrap/4.4.1/css/bootstrap.min.css\"\u0026gt; ... \u0026lt;/head\u0026gt; ...    This is all that\u0026#8217;s needed in order to use Bootstrap via WebJars. Note this will work on the JVM and also with a native binary via GraalVM: WebJars resources are located under META-INF/resources, and Quarkus automatically adds all resources from there when building a native image.\n   Wrap Up This concludes my quick tour through server-side web applications with Quarkus and its new Qute extension. Where only web applications based on REST APIs called by client-side web applications were supported before, Qute is a great addition to the list of Quarkus extensions, allowing to choose different architecture styles based on your needs and preferences.\n Note that Qute currently is in \"Experimental\" state, i.e. it\u0026#8217;s a great time to give it a try and share your feedback, but be prepared for possible immaturities and potential changes down the road. E.g. I noticed that complex boolean expressions in template conditions aren\u0026#8217;t support yet. Also it would be great to get build-time feedback upon invalid variable references in templates.\n To learn more, refer to the Qute guide and its reference documentation. You can find the complete source code of the Todo example including instructions for building and running in this GitHub repo.\n  ","id":53,"publicationdate":"Jan 3, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eOne of the long-awaited features in Quarkus was support for server-side templating:\nuntil recently, Quarkus supported only client-side web frameworks which obtain there data by calling a REST API on the backend.\nThis has changed with \u003ca href=\"https://quarkus.io/blog/quarkus-1-1-0-final-released/\"\u003eQuarkus 1.1\u003c/a\u003e: it comes with a brand-new template engine named \u003ca href=\"https://quarkus.io/guides/qute\"\u003eQute\u003c/a\u003e,\nwhich allows to build web applications using server-side templates.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Quarkus Qute – A Test Ride","uri":"https://www.morling.dev/blog/quarkus-qute-test-ride/"},{"content":"As a software engineer, I like to automate tedious tasks as much as possible. The deployment of this website is no exception: it is built using the Hugo static site generator and hosted on GitHub Pages; so wouldn\u0026#8217;t it be nice if the rendered website would automatically be published whenever an update is pushed to its source code repository?\n With the advent of GitHub Actions, tasks like this can easily be implemented without having to rely on any external CI service. Instead, many ready-made actions can be obtained from the GitHub marketplace and easily be configured as per our needs. E.g. triggered by a push to a specified branch in a GitHub repository, they can execute tasks like project builds, tests and many others, running in virtual machines based on Linux, Windows and even macOS. So let\u0026#8217;s see what\u0026#8217;s needed for building a Hugo website and deploying it to GitHub Pages.\n GitHub Actions To the Rescue Using my favourite search engine, I came across two GitHub actions which do everything we need:\n   GitHub Actions for Hugo\n  GitHub Actions for GitHub Pages\n   There are multiple alternatives for GitHub Pages deployment. I chose this one basically because it seems to be the most popular one (as per number of GitHub stars), and because it\u0026#8217;s by the same author as the Hugo one, so they should nicely play together.\n   Registering a Deploy Key In order for the GitHub action to deploy the website, a GitHub deploy key must be registered.\n To do so, create a new SSH key pair on your machine like so:\n ssh-keygen -t rsa -b 4096 -C \"$(git config user.email)\" -f gh-pages -N \"\"   This will create two files, the public key (gh-pages.pub) and the private key (gh-pages). Go to https://github.com/\u0026lt;your-user-or-organisation\u0026gt;/\u0026lt;your-repo\u0026gt;/settings/keys and click \"Add deploy key\". Paste in the public part of your key pair and check the \"Allow write access\" box.\n Now go to https://github.com/\u0026lt;your-user-or-organisation\u0026gt;/\u0026lt;your-repo\u0026gt;/settings/secrets and click \"Add new secret\". Choose ACTIONS_DEPLOY_KEY as the name and paste the private part of your key pair into the \"Value\" field.\n The key will be stored in an encrypted way as per GitHub\u0026#8217;s documentation Nevertheless I\u0026#8217;d recommend to use a specific key pair just for this purpose, instead of re-using any other key pair. That way, impact will be reduced to this particular usage, should the private key get leaked somehow.\n   Defining the Workflow With the key in place, it\u0026#8217;s time to set up the actual GitHub Actions workflow. This is simply done by creating the file .github/workflows/gh-pages-deployment.yml in your repository with the following contents. GitHub Actions workflows are YAML files, because YOLO ;)\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 name: GitHub Pages on: (1) push: branches: - master jobs: build-deploy: runs-on: ubuntu-18.04 steps: - uses: actions/checkout@v1 (2) with: submodules: true - name: Install Ruby Dev (3) run: sudo apt-get install ruby-dev - name: Install AsciiDoctor and Rouge run: sudo gem install asciidoctor rouge - name: Setup Hugo (4) uses: peaceiris/actions-hugo@v2 with: hugo-version: '0.62.0' - name: Build (5) run: hugo - name: Deploy (6) uses: peaceiris/actions-gh-pages@v2 env: ACTIONS_DEPLOY_KEY: ${{ secrets.ACTIONS_DEPLOY_KEY }} PUBLISH_BRANCH: gh-pages PUBLISH_DIR: ./public      1 Run this action whenever changes are pushed to the master branch   2 The first step in the job: check out the source code   3 Install AsciiDoctor (in case you use Hugo with AsciiDoc files, like I do) and Rouge, a Ruby gem for syntax highlighting; I\u0026#8217;m installing the gems instead of Ubuntu packages in order to get current versions   4 Set up Hugo via the aforementioned GitHub Actions for Hugo   5 Run the hugo command; here you could add parameters such as -F for also building future posts   6 Deploy the website to GitHub pages; the contents of Hugo\u0026#8217;s build directory public will be pushed to the gh-pages branch of the upstream repository, using the deploy key configured before    And that\u0026#8217;s all we need; once the file is committed and pushed to the upstream repository, the deployment workflow will be executed upon each push to the master branch.\n You can find the complete workflow definition used for publishing this website here. Also check out the documentation of GitHub Actions for Hugo and GitHub Actions for GitHub Pages to learn more about their capabilities and the options they offer.\n  ","id":54,"publicationdate":"Dec 26, 2019","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eAs a software engineer, I like to automate tedious tasks as much as possible.\nThe deployment of this website is no exception:\nit is built using the \u003ca href=\"https://gohugo.io/\"\u003eHugo\u003c/a\u003e static site generator and hosted on \u003ca href=\"https://pages.github.com/\"\u003eGitHub Pages\u003c/a\u003e;\nso wouldn\u0026#8217;t it be nice if the rendered website would automatically be published whenever an update is pushed to its source code repository?\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Automatically Deploying a Hugo Website via GitHub Actions","uri":"https://www.morling.dev/blog/automatically-deploying-hugo-website-via-github-actions/"},{"content":"I have contributed to a wide range of open-source projects over the last years. Here\u0026#8217;s a selection of projects I have been involved with.\n  Debezium  Debezium is a platform for change data capture; it lets you stream changes out of different databases such as Postgres, MySQL, MongoDB and SQL Server into Apache Kafka. I am the current lead of the Debezium project.\n Quarkus  Quarkus is a \"Kubernetes Native Java stack tailored for OpenJDK HotSpot and GraalVM, crafted from the best of breed Java libraries and standards\". My contributions to Quarkus are centered around its extension for Kafka Streams, which I initially created.\n JfrUnit  JfrUnit is a JUnit extension for asserting JDK Flight Recorder events. It comes handy for ensuring the right custom JFR events are emitted by a JVM-based library or application as well as for identifying potential performance regressions by tracking JFR events for memory allocation, network I/O, etc.\n kcctl  kcctl is a command-line client for working with Kafka Connect, allowing to examine the state of the Connect cluster and individual connectors, register and start/stop connectors, etc. It is based on Quarkus and provided as a native binary for Linux, macOS, and Windows via GraalVM.\n ModiTect, Layrry, and Deptective  ModiTect is a family of Maven and Gradle plug-ins around the Java Module System, e.g. for creating module descriptors and building modular runtime images via jlink. Layrry is an API and launcher for modularized Java applications, leveraging the Java Module System\u0026#8217;s notion of module layers, e.g. allowing to work with multiple versions of one dependency. Deptective is a plug-in for the Java compiler (javac) for enforcing package dependencies within Java projects based on a declarative architecture definition.\n MapStruct  MapStruct is a compile-time code generator for bean-to-bean mappings. Based on annotated Java interfaces, MapStruct generates mapping code that it is fully type-safe and very efficient by avoiding any usage of reflection. I was the creator and initial project lead of MapStruct.\n Bean Validation and Hibernate Validator  Bean Validation is a Java specification which lets you express constraints on object models via annotations. Originally developed at the JCP, it\u0026#8217;s now part of the Jakarta EE umbrella at the Eclipse foundation. I have been the spec lead for Bean Validation 2.0 (JSR 380) and the lead of the reference implementation Hibernate Validator.\n Other Hibernate Projects  As part of the Hibernate team, I\u0026#8217;ve contributed to different projects such as Hibernate OGM (an effort to access NoSQL stores with JPA), Hibernate Search (full-text search for domain models based on Apache Lucene and Elasticsearch) and Hibernate ORM.\n   ","id":55,"publicationdate":"Dec 26, 2019","section":"","summary":"I have contributed to a wide range of open-source projects over the last years. Here\u0026#8217;s a selection of projects I have been involved with.\n  Debezium  Debezium is a platform for change data capture; it lets you stream changes out of different databases such as Postgres, MySQL, MongoDB and SQL Server into Apache Kafka. I am the current lead of the Debezium project.\n Quarkus  Quarkus is a \"","tags":null,"title":"Projects","uri":"https://www.morling.dev/projects/"},{"content":"It has been quite a while since the last post on my old personal blog; since then, I\u0026#8217;ve mostly focused on writing about my day-work on the Debezium blog as well as some posts about more general technical topics on the Hibernate team blog.\n Now recently I had some ideas for things I wanted to write about, which didn\u0026#8217;t feel like a good fit for neither of those two. So it was time to re-boot a personal blog. The previous Blogger based one really, really feels outdated by now. Plus, I also wanted to have more control over how things work, and also be able to publish a list of projects I work on, conference talks I gave etc. So I decided to build the site using Hugo, a static site generator, and also use a nice new shiny dev domain. And here we are, welcome to morling.dev!\n Stay tuned for more posts every now and then about anything related to open source, the projects I work on and software engineering in general. Onwards!\n","id":56,"publicationdate":"Dec 26, 2019","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIt has been quite a while since the last post on my old \u003ca href=\"http://musingsofaprogrammingaddict.blogspot.com/\"\u003epersonal blog\u003c/a\u003e;\nsince then, I\u0026#8217;ve mostly focused on writing about my day-work on the \u003ca href=\"https://debezium.io/blog/\"\u003eDebezium blog\u003c/a\u003e as well as \u003ca href=\"https://in.relation.to/gunnar-morling/\"\u003esome posts\u003c/a\u003e about more general technical topics on the Hibernate team blog.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Time for a New Blog","uri":"https://www.morling.dev/blog/time-for-new-blog/"},{"content":"I am an open-source software engineer, currently working for Red Hat, where I am leading the Debezium project, a platform for change data capture.\n I have been a long-time committer to multiple open-source projects, including Hibernate, MapStruct and Deptective; I also serve as the spec lead for Bean Validation 2.0 (first at the JCP, now under the Jakarta EE umbrella at the Eclipse Foundation).\n Named a Java Champion, I\u0026#8217;m regularly speaking at conferences such as QCon, JavaOne, Red Hat Summit, JavaZone, JavaLand and Kafka Summit.\n Occasionally, I blog about topics related to software engineering.\n ","id":57,"publicationdate":"Dec 25, 2019","section":"","summary":"I am an open-source software engineer, currently working for Red Hat, where I am leading the Debezium project, a platform for change data capture.\n I have been a long-time committer to multiple open-source projects, including Hibernate, MapStruct and Deptective; I also serve as the spec lead for Bean Validation 2.0 (first at the JCP, now under the Jakarta EE umbrella at the Eclipse Foundation).\n Named a Java Champion, I\u0026#8217;m regularly speaking at conferences such as QCon, JavaOne, Red Hat Summit, JavaZone, JavaLand and Kafka Summit.","tags":null,"title":"About Me","uri":"https://www.morling.dev/about/"},{"content":"","id":58,"publicationdate":"Jan 1, 0001","section":"categories","summary":"","tags":null,"title":"Categories","uri":"https://www.morling.dev/categories/"}]