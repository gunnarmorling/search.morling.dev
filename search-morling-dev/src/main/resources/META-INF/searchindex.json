[{"content":"","id":0,"publicationdate":"Oct 14, 2020","section":"blog","summary":"","tags":null,"title":"Blogs","uri":"https://www.morling.dev/blog/"},{"content":"Layers are sort of the secret sauce of the Java platform module system (JPMS): by providing fine-grained control over how individual JPMS modules and their classes are loaded by the JVM, they enable advanced usages like loading multiple versions of a given module, or dynamically adding and removing modules at application runtime.\n The Layrry API and launcher provides a small plug-in API based on top of layers, which for instance can be used to dynamically add plug-ins contributing new views and widgets to a running JavaFX application. If such plug-in gets removed from the application again, all its classes need to be unloaded by the JVM, avoiding an ever-increasing memory consumption if for instance a plug-in gets updated multiple times.\n In this blog post I\u0026#8217;m going to explore how to ensure classes from removed plug-in layers are unloaded in a timely manner, and how to find the culprit in case some class fails to be unloaded.\n Do We Really Need Plug-ins? Before diving into the details of class unloading, let\u0026#8217;s spend some time to think about the use cases for dynamic plug-ins in Java applications to begin with. I would argue that for typical backend applications this need mostly has diminished. At large, the industry is moving away from application servers and their model around \"deploying\" applications (which you could consider as some kind of \"plug-in\") into a running server process. Instead, there\u0026#8217;s a strong trend towards immutable application packages, based on stacks like Quarkus or Spring Boot, embedding the web server, the application as well as its dependencies, often-times deployed as container images.\n The advantages of this approach centered around immutable images manifold, e.g. in terms of security (no interface for deploying applications is needed) and governance (it\u0026#8217;s always exactly clear which version of the application is running). Updates\u0026#8201;\u0026#8212;\u0026#8201;i.e. the deployment of a new revision of the container image\u0026#8201;\u0026#8212;\u0026#8201;can be put in place e.g. with help of a proxy in front of a cluster of application nodes, which are updated in a rolling manner. That way, there\u0026#8217;s no downtime of the service that\u0026#8217;ll impact the user. Also techniques like canary releases and A/B testing, as well as rolling back to specific earlier versions of an application become a breeze that way.\n The situation is different though when it comes to client applications. When thinking of your favourite editor, IDE or web browser for instance, requiring a restart when installing or updating a plug-in is not desirable. Instead, it should be possible to add plug-ins (or new plug-in versions) to a running application instance and be usable immediately, without interrupting the flow of the user. The same applies for many IoT scenarios, where e.g. an application consuming sensor measurements should be updateable without any downtime.\n   Plug-ins in Layered Java Applications JPMS addresses this requirement via the notion of module layers:\n  A layer is created from a graph of modules in a Configuration and a function that maps each module to a ClassLoader. Creating a layer informs the Java virtual machine about the classes that may be loaded from the modules so that the Java virtual machine knows which module that each class is a member of.\n   Layers are the perfect means of adding new code into a running Java application: they can be added and removed dynamically, and code in an already running layer can invoke functionality from a dynamically added layer in different ways, e.g. via reflection or by using the service loader API. Layrry exposes this functionality via a very basic plug-in API:\n public interface PluginLifecycleListener { void pluginAdded(PluginDescriptor plugin); void pluginRemoved(PluginDescriptor plugin); }   public class PluginDescriptor { public String getName() { ... } public ModuleLayer getModuleLayer() { ... } }   A plug-in in this context is a JPMS layer containing one or more modules (either explicit or automatic) which all are loaded via a single class loader. A Layrry-based application can implement the PluginLifecycleListener service contract in order to be notified whenever a plug-in is added or removed. Plug-ins are loaded from configured directories in the file system which are monitored by Layrry (other means of (un-)installing plug-ins may be added in future versions of Layrry).\n Installing a plug-in is as easy as copying its JAR(s) into a sub-folder of such monitored directory. Layrry will copy the plug-in contents to a temporary directory, create a layer with all the plug-ins JARs, and notify any registered plug-in listeners about the new layer. These will typically use the service loader API then to interact with application-specific services which model its extension points, e.g. to contribute visual UI components in case of a desktop application.\n The reverse process happens when a plug-in gets un-installed: the user removes a plug-in\u0026#8217;s directory, and all listeners will be notified by the Layrry about the removal. They should release all references to any classes from the removed plug-in, rendering it avaible for garbage collection.\n   Class Unloading in Practice There is no API in the Java platform for explicitly unloading a given class. Instead, \"a class or interface may be unloaded if and only if its defining class loader may be reclaimed by the garbage collector\" (JLS, chapter 12.7). This means in a layered Java application any classes in a layer that got removed can be unloaded as soon as the layer\u0026#8217;s class loader is subject to GC. Most importantly, no class in a still running layer must keep a (strong) reference to any class of the removed layer; otherwise this class would hinder collecting the removed layer\u0026#8217;s loader and its classes.\n As an example, let\u0026#8217;s look at the modular-tiles demo, a JavaFX application which uses the Layrry plug-in API for dynamically adding and removing tiles with different widgets like clocks and gauges to its graphical UI. The tiles themselves are implemented using the fabulous TilesFX project by Gerrit Grundwald.\n If you want to follow along, check out the source code of the demo and build it as per the instructions in the README file. Then run the Layrry launcher with the -Xlog:class+unload=info option, so to be notified about any unloaded classes in the system output:\n java -Xlog:class+unload=info \\ -jar path/to/layrry-launcher-1.0-SNAPSHOT-all.jar \\ --layers-config staging/layers.toml \\ --properties staging/versions.properties   Now add and remove some tiles plug-ins a few times:\n cp -r staging/plugins-prepared/* staging/plugins rm -rf staging/plugins/*   The widgets will show up and disappear in the JavaFX UI, but what about class unloading in the logs? In all likelyhood, nothing! This is because without any further configuration, the G1 garbage collector (which is used by the JDK by default since Java 9) will unload classes only during a full garbage collection, which may only run after a long time (if at all), if there\u0026#8217;s no substantial object allocation happening.\n     JEP 158: Unified JVM Logging The -Xlog option has been defined by JEP 158, added to the JDK with Java 9, which provides a \"common logging system for all components of the JVM\". The new unified options should be preferred over the legacy options like -XX:+TraceClassLoading and -XX:+TraceClassUnloading. Usage of -Xlog is described in detail in the java man page; also Nicolai Parlog discusses JEP 158 in great depth in this blog post.\n     So at this point you could trigger a GC explicitly, e.g. via jcmd:\n jcmd \u0026lt;pid\u0026gt; GC.run   But of course that\u0026#8217;s not too desirable when running things in production. Instead, if you\u0026#8217;re on JDK 12 or later, you can use the new G1PeriodicGCInterval option for triggering a periodic GC:\n java -Xlog:class+unload=info \\ -XX:G1PeriodicGCInterval=5000 \\ -jar path/to/layrry-launcher-1.0-SNAPSHOT-all.jar \\ --layers-config staging/layers.toml \\ --properties staging/versions.properties   Introduced via JEP 346 (\"Promptly Return Unused Committed Memory from G1\"), this will periodically initiate a concurrent GC cycle (or optionally even a full GC). Add and remove some plug-ins again, and after some time you should see messages about the unloaded classes in the log:\n ... [138.912s][info][class,unload] unloading class org.kordamp.tiles.sparkline.SparklineTilePlugin 0x0000000800de1840 [138.912s][info][class,unload] unloading class org.kordamp.tiles.gauge.GaugeTilePlugin 0x0000000800de2040 [138.913s][info][class,unload] unloading class org.kordamp.tiles.clock.ClockTilePlugin 0x0000000800de2840 ...   From what I observed, class unloading doesn\u0026#8217;t happen on every concurrent GC cycle; it might take a few cycles after a plug-in has been removed until its classes are unloaded. If you\u0026#8217;re not using G1, but the new low-pause concurrent collectors Shenandoah or ZGC, they\u0026#8217;ll be able to concurrently unload classes without any special configuration needed. Note that class unloading is not a mandatory operation which would have to be provided by every GC implementation. E.g. initial ZGC releases did not support class unloading, which would have rendered them unsuitable for this use case.\n     JEP 371: Hidden Classes As mentioned above, regular classes can only be unloaded if their defining class loader become subject to garbage collection. This can be an issue for frameworks and libraries which generate lots of classes dynamically at runtime, e.g. script language implementations or solutions like Presto, which generates a class for each query.\n The traditional workaround is to generate each class using its own dedicated class loader, which then can be discarded specifically. This solves the GC issue, but it isn\u0026#8217;t ideal in terms of overall memory consumption and speed of class generation. Hence, JDK 15 defines a notion of Hidden Classes (JEP 371), which are not created by class loaders and thus can be unloaded eagerly: \"when all instances of the hidden class are reclaimed and the hidden class is no longer reachable, it may be unloaded even though its notional defining loader is still reachable\".\n You can find some more information on hidden classes in this tweet thread and this code example on GitHub.\n     But who wants to stare at logs in the system output, that\u0026#8217;s so 2010! So let\u0026#8217;s fire up JDK Mission Control and trigger a recording via the JDK Flight Recorder (JFR) to observe what\u0026#8217;s going on in more depth.\n JFR can capture class unloading events, you need to make sure though to enable this event type, which is not the case by default. In order to do so, start a recording, then go to the Template Manager, edit or create a flight recording template and check the Enabled box for the events under Java Virtual Machine \u0026#8594; Class Loading. With the recorder running, add and remove some tiles plug-ins to the running application.\n Once the recording is finished, you should see class unloading events under JVM Internals \u0026#8594; Class Loading:\n   In this case, the classes from a set of plug-ins were unloaded at 16:48:11, which correlates to the periodic GC cycle running at that time and spending a slightly increased time for cleaning up class loader data:\n   As a good Java citizen, Layrry itself also emits JFR events whenever a plug-in layer is added or removed, which helps to track the need for classes to be unloaded:\n     If Things Go Wrong Now let\u0026#8217;s look at the situation where some class failed to unload after its plug-in layer was removed. Common reasons for that include remaining references from classes in a still running layer to classes in the removed layer, threads started by a class in the removed layer which were not stopped, and JVM shutdown hooks registered by code in the removed layer.\n This is known as a class loader leak and is problematic as it means more and more memory will be consumed and cannot be freed as plug-ins are added and removed, which eventually may lead to an OutOfMemoryError. So how could you detect and analyse this situation? An OutOfMemoryError in production would surely be an indicator that there must be a memory or class loader leak somewhere. It\u0026#8217;s also a good idea to regularly examine JFR recording files (e.g. in your testing or staging environment): the absence of any class unloading event despite the removal of plug-ins should trigger an investigation.\n As far as analysing the situation is concerned, examining a heap dump of the application will typically yield insight into the cause rather quickly. Take a heap dump using jcmd as shown above, then load the dump into a tool such as Eclipse MAT. In Eclipse MAT, the \"Duplicate Classes\" action is a great starting point. If one class has been loaded by multiple class loaders, but failed to unload, it\u0026#8217;s a pretty strong indicator that something is wrong:\n   The next step is to analyse the shortest path from the involved class loaders to a GC root:\n   Some object on that path must hold on to a reference to a class or the class loader of the removed plug-in, preventing the loader to be GC-ed. In the case at hand, it\u0026#8217;s the leakingPlugins field in the PluginRegistry class, to which each plug-in is added upon addition of the layer, but then apparently its coffee-deprived author forgot to remove the plug-in from that collection within the pluginRemoved() event handler ;)\n As a quick side note, there\u0026#8217;s a really cool plug-in for Eclipse MAT written by Vladimir Sitnikov, which allows you to query heap dumps using SQL. It maps each class to its own \"table\", so that e.g. classes loaded more than once could be selected using the following SQL query on the java.lang.Class class:\n select c.name, listagg(toString(c.\"@classLoader\")) as 'loaders', count(*) as 'count' from \"java.lang.Class\" c where c.name \u0026lt;\u0026gt; '' group by c.name having count(*) \u0026gt; 1   Resulting in the same list of classes as above:\n   This could come in very handy for more advanced heap dump analyses, which cannot be done using Eclipse MAT\u0026#8217;s built-in query capabilities.\n   Learning More Via module layers, JPMS provides the foundation for dynamic plug-in architectures, as demonstrated by Layrry. Removing layers at runtime requires some care and consideration, so to avoid class loader leaks which eventually may lead to OutOfMemoryErrors. As so often, JDK Mission Control, JFR, and Eclipse MAT prove to be invaluable tools in the box of every Java developer, helping to ensure class unloading in your layered applications is done correctly, and if it is not, helping to understand and fix the underlying issue.\n Here are some more resources about class unloading and analysing class loader leaks:\n   Shenandoah GC in JDK 14, Part 2: Concurrent roots and class unloading: A blog post touching on class unloading in Shenandoah by Roman Kennke\n  ZGC Concurrent Class Unloading: A conference talk by Erik Österlund\n  class loader leaks: A series of blog posts by Mattias Jiderhamn\n  ClassLoader \u0026amp; memory leaks: a Java love story: A post about heap dump analysis by Aloïs Micard\n   Lastly, if you\u0026#8217;d like to explore the dynamic addition and removal of JPMS layers to a running application yourself, the modular-tiles demo app is a great starting point. Its source code can be found on GitHub.\n  ","id":1,"publicationdate":"Oct 14, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eLayers are sort of the secret sauce of the Java platform module system (JPMS):\nby providing fine-grained control over how individual JPMS modules and their classes are loaded by the JVM,\nthey enable advanced usages like loading multiple versions of a given module, or dynamically adding and removing modules at application runtime.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe \u003ca href=\"/blog/introducing-layrry-runner-and-api-for-modularized-java-applications/\"\u003eLayrry\u003c/a\u003e API and launcher provides a small plug-in API based on top of layers,\nwhich for instance can be used to dynamically add plug-ins contributing new views and widgets to a running JavaFX application.\nIf such plug-in gets removed from the application again,\nall its classes need to be unloaded by the JVM, avoiding an ever-increasing memory consumption if for instance a plug-in gets updated multiple times.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIn this blog post I\u0026#8217;m going to explore how to ensure classes from removed plug-in layers are unloaded in a timely manner,\nand how to find the culprit in case some class fails to be unloaded.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Class Unloading in Layered Java Applications","uri":"https://www.morling.dev/blog/class-unloading-in-layered-java-applications/"},{"content":"","id":2,"publicationdate":"Oct 14, 2020","section":"","summary":"","tags":null,"title":"Gunnar Morling","uri":"https://www.morling.dev/"},{"content":"Lately I\u0026#8217;ve been fascinated by the possibility to analyse the assembly code emitted by the Java JIT (just-in-time) compiler. So far I had only looked only into Java class files using javap; diving into the world of assembly code feels a bit like Alice must have felt when falling down the rabbit whole into wonderland.\n My motivation for this exploration was trying to understand what is faster in Java: a switch statement over strings, or a lookup in a hash map. Solely looking at Java bytecode isn\u0026#8217;t going far enough to answer this question, as the difference lies in the actual assembly statements executed on the CPU. I\u0026#8217;ll keep the details around that for another time; in this post I\u0026#8217;m just going quickly to share what I learned in regards to building a tool needed for this exercise, hsdis.\n hsdis is a disassembler library which can be used with the java runtime as well as tools such as JitWatch to analyse the code produced by the Java JIT compiler. For licensing reasons though it doesn\u0026#8217;t come as a binary with the JDK. Instead, you need it to build yourself from source. Instructions for doing so are spread across a few different places, but I couldn\u0026#8217;t find any 100% current information, in particular as OpenJDK has moved to git and GitHub just recently.\n So here is what you need to do in order to build hsdis for OpenJDK 15; in my case I\u0026#8217;m running on macOS, slightly different steps may apply for other platforms. First, get the OpenJDK source code and check out the version for which you want to build hsdis:\n git clone git@github.com:openjdk/jdk.git git checkout jdk-15+36 # Current stable JDK 15 build   The source location of hsdis has changed with the move from Mercurial to git:\n cd src/utils/hsdis   In order to build hsdis, you\u0026#8217;ll need the GNU Binutils, a collection of several binary tools:\n wget https://ftp.gnu.org/gnu/binutils/binutils-2.35.tar.gz tar xvf binutils-2.35.tar.gz   Then run the actual hsdis build (macOS comes with all the required tools like make):\n make BINUTILS=binutils-2.35 ARCH=amd64   This will take a few minutes; if all goes well, there\u0026#8217;ll be hsdis binary in the build directory, in my case this is build/macosx-amd64/hsdis-amd64.dylib. Copy the library to lib/server of our JDK:\n sudo cp build/macosx-amd64/hsdis-amd64.dylib $JAVA_HOME/lib/server       If you\u0026#8217;re on Linux, you also can provide the hsdis tool via the LD_LIBRARY_PATH environment variable:\n export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:path/to/hsdis/build/linux-amd64   Note this won\u0026#8217;t work on current macOS versions unfortunately due to its System Integrity Protection feature (SIP). Thanks to Brice Dutheil for this tip!\n     Congrats! You now can use the XX:+PrintAssembly flag of the java command to examine the assembly code of your Java program. Let\u0026#8217;s give it a try. Create a Java source file with the following contents:\n public class PrintAssemblyTest { public static void main(String... args) { PrintAssemblyTest hello = new PrintAssemblyTest(); for(int i = 0; i \u0026lt;= 10_000_000; i++) { hello.hello(i); } } private void hello(int i) { if (i % 1_000_000 == 0) { System.out.println(\"Hello, \" + i); } } }   Compile and run it like so:\n javac PrintAssemblyTest.java java -XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly \\ -XX:+TraceClassLoading -XX:+LogCompilation \\ PrintAssemblyTest   You should then find the assembly code of the hello() method somewhere in the output:\n ============================= C2-compiled nmethod ============================== ----------------------------------- Assembly ----------------------------------- Compiled method (c2) 1409 106 4 PrintAssemblyTest::hello (20 bytes) total in heap [0x000000011e3fce90,0x000000011e3fd148] = 696 relocation [0x000000011e3fcfe8,0x000000011e3fcff8] = 16 main code [0x000000011e3fd000,0x000000011e3fd080] = 128 stub code [0x000000011e3fd080,0x000000011e3fd098] = 24 oops [0x000000011e3fd098,0x000000011e3fd0a0] = 8 metadata [0x000000011e3fd0a0,0x000000011e3fd0a8] = 8 scopes data [0x000000011e3fd0a8,0x000000011e3fd0d0] = 40 scopes pcs [0x000000011e3fd0d0,0x000000011e3fd140] = 112 dependencies [0x000000011e3fd140,0x000000011e3fd148] = 8 -------------------------------------------------------------------------------- [Constant Pool (empty)] -------------------------------------------------------------------------------- [Entry Point] # {method} {0x000000010d74c4b0} 'hello' '(I)V' in 'PrintAssemblyTest' # this: rsi:rsi = 'PrintAssemblyTest' # parm0: rdx = int # [sp+0x30] (sp of caller) 0x000000011e3fd000: mov 0x8(%rsi),%r10d 0x000000011e3fd004: shl $0x3,%r10 0x000000011e3fd008: movabs $0x800000000,%r11 0x000000011e3fd012: add %r11,%r10 0x000000011e3fd015: cmp %r10,%rax 0x000000011e3fd018: jne 0x0000000116977100 ; {runtime_call ic_miss_stub} 0x000000011e3fd01e: xchg %ax,%ax [Verified Entry Point] 0x000000011e3fd020: mov %eax,-0x14000(%rsp) 0x000000011e3fd027: push %rbp 0x000000011e3fd028: sub $0x20,%rsp ;*synchronization entry ; - PrintAssemblyTest::hello@-1 (line 10) 0x000000011e3fd02c: movslq %edx,%r10 0x000000011e3fd02f: mov %edx,%r11d 0x000000011e3fd032: sar $0x1f,%r11d 0x000000011e3fd036: imul $0x431bde83,%r10,%r10 0x000000011e3fd03d: sar $0x32,%r10 0x000000011e3fd041: mov %r10d,%r10d 0x000000011e3fd044: sub %r11d,%r10d 0x000000011e3fd047: imul $0xf4240,%r10d,%r10d ;*irem {reexecute=0 rethrow=0 return_oop=0} ; - PrintAssemblyTest::hello@3 (line 10) 0x000000011e3fd04e: cmp %r10d,%edx 0x000000011e3fd051: je 0x000000011e3fd063 ;*ifne {reexecute=0 rethrow=0 return_oop=0} ; - PrintAssemblyTest::hello@4 (line 10) 0x000000011e3fd053: add $0x20,%rsp 0x000000011e3fd057: pop %rbp 0x000000011e3fd058: mov 0x110(%r15),%r10 0x000000011e3fd05f: test %eax,(%r10) ; {poll_return} 0x000000011e3fd062: retq 0x000000011e3fd063: mov %edx,%ebp 0x000000011e3fd065: sub %r10d,%ebp ;*irem {reexecute=0 rethrow=0 return_oop=0} ; - PrintAssemblyTest::hello@3 (line 10) 0x000000011e3fd068: mov $0xffffff45,%esi 0x000000011e3fd06d: mov %edx,(%rsp) 0x000000011e3fd070: data16 xchg %ax,%ax 0x000000011e3fd073: callq 0x0000000116979080 ; ImmutableOopMap {} ;*ifne {reexecute=1 rethrow=0 return_oop=0} ; - (reexecute) PrintAssemblyTest::hello@4 (line 10) ; {runtime_call UncommonTrapBlob} 0x000000011e3fd078: hlt 0x000000011e3fd079: hlt 0x000000011e3fd07a: hlt 0x000000011e3fd07b: hlt 0x000000011e3fd07c: hlt 0x000000011e3fd07d: hlt 0x000000011e3fd07e: hlt 0x000000011e3fd07f: hlt [Exception Handler] 0x000000011e3fd080: jmpq 0x0000000116a22d80 ; {no_reloc} [Deopt Handler Code] 0x000000011e3fd085: callq 0x000000011e3fd08a 0x000000011e3fd08a: subq $0x5,(%rsp) 0x000000011e3fd08f: jmpq 0x0000000116978ca0 ; {runtime_call DeoptimizationBlob} 0x000000011e3fd094: hlt 0x000000011e3fd095: hlt 0x000000011e3fd096: hlt 0x000000011e3fd097: hlt --------------------------------------------------------------------------------   Interpreting the output is left as an exercise for the astute reader ;-) A great resource for getting started doing so is the post PrintAssembly output explained! by Jean-Philippe Bempel.\n With hsdis in place, you also can use the excellent JitWatch tool for analysing the assembly code, which e.g. not only provides an easy way to navigate from source code to byte code to assembly code, but also comes with helpful tooltips explaining the meaning of the different assembly mnemonics.\n","id":3,"publicationdate":"Oct 5, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eLately I\u0026#8217;ve been fascinated by the possibility to analyse the assembly code emitted by the Java JIT (just-in-time) compiler.\nSo far I had only looked only into Java class files using \u003cem\u003ejavap\u003c/em\u003e;\ndiving into the world of assembly code feels a bit like Alice must have felt when falling down the rabbit whole into wonderland.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Building hsdis for OpenJDK 15","uri":"https://www.morling.dev/blog/building-hsdis-for-openjdk-15/"},{"content":"I\u0026#8217;m excited to share the news about an open-source utility I\u0026#8217;ve been working on lately: JmFrX, a tool for capturing JMX data with JDK Flight Recorder.\n When using JMX (Java Management Extensions), The Java platform\u0026#8217;s standard for monitoring and managing applications, JmFrX allows you to periodically record the attributes from any JMX MBean into JDK Flight Recorder (JFR) files, which you then can analyse using JDK Mission Control (JMC).\n This is useful for a number of reasons:\n   You can track changes to the values of JMX MBean attributes over time without resorting to external monitoring tools\n  You can analyze JMX data from offline JFR recording files in cases where you cannot directly connect to the running application\n  You can export JMX data as live data streams using the JFR event streaming API introduced in Java 14\n   In this blog post I\u0026#8217;m going to explain how to use JmFrX for recording JMX data in your applications, point out some interesting JmFrX implemention details, and lastly will discuss some potential steps for future development of the tool.\n Why JmFrX? JDK Flight Recorder is a \"low-overhead data collection framework for troubleshooting Java applications and the HotSpot JVM\". In combination with the JDK Mission Control client application it allows to gain deep insights into the performance characteristics of Java applications.\n In addition to the built-in metrics and event types, JFR also allows to define and emit custom event types. JFR got open-sourced in JDK 11; since then, developers in the Java eco-system began to support this, enabling users to work with JFR and JMC for analyzing the runtime behavior of 3rd party libraries and frameworks. For instance, JUnit 5.7 produces JFR events related to the execution lifecycle of unit tests.\n At the same time, many library authors are not (yet) in a position where they could easily emit JFR events from their tools, as for instance they might wish to keep compatibility with older Java versions. They might already expose JMX MBeans though which often provide fine-grained information about the execution state of Java applications. This is where JmFrX comes in: by periodically capturing the attribute values from a given set of JMX MBeans, it allows to capture this information in JFR recordings.\n JmFrX isn\u0026#8217;t the first effort that seeks to bridge JMX and JFR; JDK Mission Control project lead Marcus Hirt discusses a similar project in a blog post in 2016. But unlike the implementation described by Marcus in this post, JmFrX is based on the public and supported APIs for defining, configuring and emitting JFR events, as available since OpenJDK 11.\n   How To Use JmFrX In order to use JmFrX, make sure to run OpenJDK 11 or newer. OpenJDK 8 also contains the open-sourced Flight Recorder bits as of release 8u262 (from July this year); so this should work, too, but I haven\u0026#8217;t tested it yet.\n Until a stable release will be provided, you can obtain JmFrX snapshot builds via JitPack. For that, add the JitPack repository to your pom.xml when using Apache Maven (or apply equivalent configuration for your preferred build tool):\n ... \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;jitpack.io\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://jitpack.io\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; ...   Then add the JmFrX dependency:\n ... \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.github.gunnarmorling\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jmfrx\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;master-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; ...   The next step is registering the JmFrX event type with JFR in the start-up routine of your program. This could for instance be done in the main() method, the static initializer of a class loaded early on, an eagerly initialized Spring or CDI bean, etc. A Java agent for this purpose will be provided as part of this project soon.\n When building applications with Quarkus, you could use an application start-up event like so:\n @ApplicationScoped public class EventRegisterer { public void registerEvent(@Observes StartupEvent se) { Jmfrx.getInstance().register(); } public void unregisterEvent(@Observes ShutdownEvent se) { Jmfrx.getInstance().unregister(); } }   Now start your application and create a JFR configuration file which enables the JmFrX event type. To do so, open JDK Mission Control, and choose your running application in the JVM Browser. Then perform these steps:\n   Right-click the target JVM \u0026#8594; Select Start Flight Recording\u0026#8230;\u0026#8203;\n  Click on Template Manager\n  Copy the Continuous setting and click Edit for modifying this copy\n  Expand the JMX and JMX Dump nodes\n  Make sure the JMX Dump event type is Enabled; choose a period for dumping the chosen JMX MBeans (by default 60 s) and specify the MBeans whose data should be captured; that\u0026#8217;s done by means of a regular expression, which matches one or more JMX object names, for instance .*OperatingSystem.*:\n       Close the two last dialogues by clicking OK and OK\n  Important: Make sure that the template you edited is selected under Event settings\n  Click Finish to begin the recording\n   Once the recording is complete, open the recording file in JDK Mission Control and go to the Event Browser. You should see periodic events corresponding to the selected MBeans under the JMX node:\n   When not using JDK Mission Control to initiate recordings, but the jcmd utility on the command line, also follow the same steps as above for creating a configuration as described above. But then, instead of starting the recording, export the configuration file from the template manager and specify its name to jcmd via the settings=/path/to/settings.jfc parameter.\n Now using JmFrX to observe JMX data from for the java.lang MBeans like Runtime and OperatingSystem in JFR isn\u0026#8217;t too exciting yet, as there\u0026#8217;s dedicated JFR event types which contain most of that information. But things get more interesting when capturing data from custom MBean types, as e.g. here for the stream threads metrics from a Kafka Streams application:\n     Customizing Event Formats By default, JmFrX will propagate the raw attribute values from a JMX MBean to the corresponding JFR event. This makes sure that all the information can be retrieved from recordings, but the data format can be a bit unwieldy, e.g. when it comes to data amounts in bytes, or time periods in milli-seconds since epoch.\n To address this, JFR supports a range of metadata annotations such as @DataAmount, @Timespan, or @Percentage, which allow to format event attributes. This information then is used by JMC for instance when displaying events in the browser (see event Properties to the left in the screenshot above).\n JmFrX integrates with this metadata facility via the notion of event profiles, which describe the data format of one MBean type and its attributes. When creating an event for a given JMX MBean, JmFrX will look for a corresponding event profile and apply its settings. Event profiles are defined by implementing the EventProfileContributor SPI. As an example here\u0026#8217;s a subset of the the built-in profile definition for the OperatingSystem MBean:\n public class JavaLangEventProfileContributor implements EventProfileContributor { @Override public void contributeProfiles(EventProfileBuilder builder) { builder.addEventProfile(\"java.lang:type=OperatingSystem\") (1) .addAttributeProfile(\"TotalSwapSpaceSize\", long.class, new AnnotationElement(DataAmount.class, DataAmount.BYTES), (2) v -\u0026gt; v) .addAttributeProfile(\"FreeSwapSpaceSize\", long.class, new AnnotationElement(DataAmount.class, DataAmount.BYTES), v -\u0026gt; v) (3) .addAttributeProfile(\"CpuLoad\", double.class, new AnnotationElement(Percentage.class), v -\u0026gt; v) .addAttributeProfile(\"ProcessCpuLoad\", double.class, new AnnotationElement(Percentage.class), v -\u0026gt; v) .addAttributeProfile(\"SystemCpuLoad\", double.class, new AnnotationElement(Percentage.class), v -\u0026gt; v) .addAttributeProfile(\"ProcessCpuTime\", long.class, new AnnotationElement(Timespan.class, Timespan.NANOSECONDS), v -\u0026gt; v ); } }     1 Profiles are linked via the MBean name   2 The atribute type is specified via an AnnotationElement for one of the JFR type metadata annotations   3 If needed, the actual value can be modified too, e.g. to convert it into another data type, or to shift its value into an expected range (for instance 0 to 1 for percentage values)    Once you\u0026#8217;ve defined the event profiles for your MBean type(s), don\u0026#8217;t forget to register the contributor type either as a service implementation in your module-info.java descriptor (when building a modular Java application):\n module com.example { requires jdk.jfr; requires dev.morling.jmfrx; provides dev.morling.jmfrx.spi.EventProfileContributor with com.example.MyEventProfileContributor; }   When building an application using the traditional classpath, register the names of all profile contributors in the META-INF/services/dev.morling.jmfrx.spi.EventProfileContributor file.\n There\u0026#8217;s a small (yet hopefully growing) set of event profiles built into JmFrX. But as event profile contributors are discovered using the Java service loader mechanism, you can also easily plug in event profiles for other MBean types, e.g. for the JMX MBeans of Apache Kafka or Kafka Connect, or application servers like WildFly.\n Also your pull requests for contributing event profiles for common JMX applications to JmFrX itself will be very welcomed!\n   How It Works If you solely want to use JmFrX, you can pretty much stop reading this post at this point. But if you\u0026#8217;re curious about how it is working internally, stay with me for a bit longer: JmFrX uses two lesser known JFR features which also might be interesting for your own application-specific event types, periodic JFR events and dynamic event types.\n Unlike most JFR event types which are emitted when some specific JVM or application functionality is executed, periodic events are produced in a regular interval. The default interval (which can be overridden by the user) is specified using the @Period annotation on the event type definition:\n @Name(JmxDumpEvent.NAME) @Label(\"JMX Dump\") @Category(\"JMX\") @Description(\"Periodically dumps specific JMX MBeans\") @StackTrace(false) @Period(\"60 s\") public class JmxDumpEvent extends Event { public static final String NAME = \"dev.morling.jmfrx.JmxDumpEvent\"; // event implementation ... }   Upon application start-up, JmFrX registers this event type with the JFR environment:\n ... private Runnable hook; public void register() { hook = () -\u0026gt; { (1) JmxDumpEvent dumpEvent = new JmxDumpEvent(); if (!dumpEvent.isEnabled()) { return; } dumpEvent.begin(); // retrieve data from matching MBean(s) and create event(s) ... dumpEvent.commit(); }; FlightRecorder.addPeriodicEvent(JmxDumpEvent.class, hook); (2) } public void unregister() { FlightRecorder.removePeriodicEvent(hook); (3) } ...     1 The event hook implementation   2 Register the periodic event   3 Unregister the periodic event    The regular expression for specifying the MBean name(s) is passed to the event type as a SettingControl. You can learn more about event settings in my post on custom JFR event types.\n When the periodic event hook runs, it must create one event for each captured MBean. As JmFrX cannot know which MBean(s) you\u0026#8217;re interested in, it\u0026#8217;s not an option to pre-define these event types and their structure.\n This is where dynamic JFR event types come in: Using the EventFactory class, event types can be defined at runtime. Under the covers, JFR will create a corresponding Event sub-class dynamically using the ASM API. Here\u0026#8217;s the relevant JmFrX code which defines the event type for a given MBean:\n ... public static EventDescriptor getDescriptorFor(String mBeanName) { MBeanServer mbeanServer = ManagementFactory.getPlatformMBeanServer(); try { ObjectName objectName = new ObjectName(mBeanName); MBeanInfo mBeanInfo = mbeanServer.getMBeanInfo(objectName); List\u0026lt;AnnotationElement\u0026gt; eventAnnotations = Arrays.asList( (1) new AnnotationElement(Category.class, getCategory(objectName)), new AnnotationElement(StackTrace.class, false), new AnnotationElement(Name.class, getName(objectName)), new AnnotationElement(Label.class, getLabel(objectName)), new AnnotationElement(Description.class, mBeanInfo.getDescription()) ); List\u0026lt;AttributeDescriptor\u0026gt; fields = getFields(objectName, mBeanInfo); List\u0026lt;ValueDescriptor\u0026gt; valueDescriptors = fields.stream() (2) .map(AttributeDescriptor::getValueDescriptor) .collect(Collectors.toList()); return new EventDescriptor(EventFactory.create(eventAnnotations, valueDescriptors), fields); } catch (Exception e) { throw new RuntimeException(e); } } ...     1 Define event metadata like name, label, category etc. via the JFR metadata annotations   2 For each MBean attribute, an attribute is added to the event type; its definition is based on the information in the corresponding event profile, if present    The actual implemention is slightly more complex, as it deals with integrating metadata from JmFrX event profiles and more. You can find the complete code in the EventProfile class.\n   Takeaways JmFrX is a small utility which allows you to capture JMX data with JDK Flight Recorder. It\u0026#8217;s open-source (Apache License, version 2), you can find the source code on GitHub. With the wide usage of JMX for application monitoring in the Java world, JmFrX can help to bring that information into JFR recordings, making it available for offline investigations and analyses.\n Potential next steps for JmFrX include more meaningful handling of tabular and composite JMX data, adding a Java agent for registering the event type, providing some more built-in event profiles and publishing a stable release on Maven Central. Eventually, the JmFrX project might move over to the rh-jmc-team GitHub organization, which is is managed by Red Hat\u0026#8217;s OpenJDK team and contains many other very useful projects around JDK Flight Recorder and Mission Control.\n Your feedback on and contributions to JmFrX will be very welcomed!\n  ","id":4,"publicationdate":"Aug 18, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eI\u0026#8217;m excited to share the news about an open-source utility I\u0026#8217;ve been working on lately:\n\u003ca href=\"https://github.com/gunnarmorling/jmfrx\"\u003eJmFrX\u003c/a\u003e,\na tool for capturing JMX data with JDK Flight Recorder.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eWhen using JMX (\u003ca href=\"https://en.wikipedia.org/wiki/Java_Management_Extensions\"\u003eJava Management Extensions\u003c/a\u003e), The Java platform\u0026#8217;s standard for monitoring and managing applications,\nJmFrX allows you to periodically record the attributes from any JMX MBean into \u003ca href=\"https://openjdk.java.net/jeps/328\"\u003eJDK Flight Recorder\u003c/a\u003e (JFR) files,\nwhich you then can analyse using \u003ca href=\"https://openjdk.java.net/projects/jmc/\"\u003eJDK Mission Control\u003c/a\u003e (JMC).\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Introducing JmFrX: A Bridge From JMX to JDK Flight Recorder","uri":"https://www.morling.dev/blog/introducing-jmfrx-a-bridge-from-jmx-to-jdk-flight-recorder/"},{"content":"I have built a custom search functionality for this blog, based on Java and the Apache Lucene full-text search library, compiled into a native binary using the Quarkus framework and GraalVM. It is deployed as a Serverless application running on AWS Lambda, providing search results without any significant cold start delay. If you thought Java wouldn\u0026#8217;t be the right language for this job, keep reading; in this post I\u0026#8217;m going to give an overview over the implementation of this feature and my learnings along the way.\n Having a search functionality for my blog has been on my mind for quite some time; I\u0026#8217;d like to give users the opportunity to find specific contents on this blog right here on this site, without having to use an external search engine. That\u0026#8217;s not only nice in terms of user experience, but also having insight into the kind of information readers look for on this blog should help me to identify interesting things to write about in the future.\n Now this blog is a static site\u0026#8201;\u0026#8212;\u0026#8201;generated using Hugo, hosted on GitHub Pages\u0026#8201;\u0026#8212;\u0026#8201;which makes this an interesting challenge. I didn\u0026#8217;t want to rely on an external search service (see \"Why No External Search Service\" below for the reasoning), and also a purely client-side solution as described in this excellent blog post didn\u0026#8217;t seem ideal. While technically fascinating, I didn\u0026#8217;t like the fact that it requires shipping the entire search index to the client for executing search queries. Also things like result highlighting, customized result scoring, word stemming, fuzzy search and more seemed a bit more than I\u0026#8217;d be willing to implement on the client.\n All these issues have largely been solved on the server-side by libraries such as Apache Lucene for quite some time. Using a library like Lucene means implementing a custom server-side process, though. How to deploy such service? Operating a VM 24/7 with my search backend for what\u0026#8217;s likely going to be not more than a few dozen queries per month seemed a bit like overkill.\n So after some consideration I decided to implement my own search functionality, based on the highly popular Apache Lucene library, deployed as a Serverless application, which is started on-demand if a user runs a query on my website. In the remainder of this post I\u0026#8217;m going to describe the solution I came up with and how it works.\n If you like, you can try it out right now, this post is about this little search input control at the top right of this page!\n     Why No External Search Service? When tweeting about my serverless search experiment, one of the questions was \"What\u0026#8217;s wrong with Algolia?\". To be very clear, there\u0026#8217;s nothing wrong with it at all. External search services like Algolia, Google Custom Search, or an Elasticsearch provider such as Bonsai promise an easy-to-use, turn-key search functionality which can be a great choice for your specific use case.\n However, I felt that none of these options would provide me the degree of control and customizability I was after. I also ruled out any \"free\" options, as they\u0026#8217;d either mean having ads or paying for the service with the data of myself or that of my readers. And to be honest, I also just fancied the prospect of solving the problem by myself, instead of relying on an off-the-shelf solution.\n     Why Serverless? First of all, let\u0026#8217;s discuss why I opted for a Serverless solution. It boils down to three reasons:\n   Security: While it\u0026#8217;d only cost a few EUR per month to set up a VM with a cloud provider like Digital Ocean or Hetzner, having to manage a full operating system installation would require too much of my attention; I don\u0026#8217;t want someone to mine bitcoins or doing other nasty things on a box I run, just because I failed to apply some security patch\n  Cost: Serverless does not only promise to scale-out (and let\u0026#8217;s be honest, there likely won\u0026#8217;t be millions of search queries on my blog every month), but also scale-to-zero. As Serverless is pay-per-use and there are free tiers in place e.g. for AWS Lambda, this service ideally should cost me just a few cents per month\n  Learning Opportunity: Last but not least, this also should be a nice occasion for me to dive into the world of Serverless, by means of designing, developing and running a solution for a real-world problem, exploring how Java as my preferred programming language can be used for this task\n     Solution Overview The overall idea is quite simple: there\u0026#8217;s a simple HTTP service which takes a query string, runs the query against a Lucene index with my blog\u0026#8217;s contents and returns the search results to the caller. This service gets invoked via JavaScript from my static blog pages, where results are shown to the user.\n The Lucene search index is read-only and gets rebuilt whenever I update the blog. It\u0026#8217;s baked into the search service deployment package, which that way becomes fully immutable. This reduces complexities and the attack surface at runtime. Surely that\u0026#8217;s not an approach that\u0026#8217;s viable for more dynamic use cases, but for a blog that\u0026#8217;s updated every few weeks, it\u0026#8217;s perfect. Here\u0026#8217;s a visualization of the overall flow:\n   The search service is deployed as a Serverless function on AWS Lambda. One important design goal for me is to avoid lock-in to any specific cloud provider: the solution should be portable and also be usable with container-based Serverless approaches like Knative.\n Relying on a Serverless architecture means its start-up time must be a matter of milli-seconds rather than seconds, so to not have a user wait for a noticeable amount of time in case of a cold start. While substantial improvements have been made in recent Java versions to improve start-up times, it\u0026#8217;s still not ideal for this kind of use case. Therefore, the application is compiled into a native binary via Quarkus and GraalVM, which results in a start-up time of ~30 ms on my laptop, and ~180 ms when deployed to AWS Lambda. With that we\u0026#8217;re in a range where a cold start won\u0026#8217;t impact the user experience in any significant way.\n The Lambda function is exposed to callers via the AWS API Gateway, which takes incoming HTTP requests, maps them to calls of the function and converts its response into an HTTP response which is sent back to the caller.\n Now let\u0026#8217;s dive down a bit more into the specific parts of the solution. Overall, there are four steps involved:\n   Data extraction: The blog contents to be indexed must be extracted and converted into an easy-to-process data format\n  Search backend implementation: A small HTTP service is needed which exposes the search functionality of Apache Lucene, which in particular requires some steps to enable Lucene being used in a native GraalVM binary\n  Integration with the website: The search service must be integrated into the static site on GitHub Pages\n  Deployment: Finally, the search service needs to be deployed to AWS API Gateway and Lambda\n     Data Extraction The first step was to obtain the contents of my blog in an easily processable format. Instead of requiring something like a real search engine\u0026#8217;s crawler, I essentially only needed to have a single file in a structured format which then can be passed on to the Lucene indexer.\n This task proved rather easy with Hugo; by means of a custom output format it\u0026#8217;s straight-forward to produce a JSON file which contains the text of all my blog pages. In my config.toml I declared the new output format and activate it for the homepage (largely inspired by this write-up):\n [outputFormats.SearchIndex] mediaType = \"application/json\" baseName = \"searchindex\" isPlainText = true notAlternative = true [outputs] home = [\"HTML\",\"RSS\", \"SearchIndex\"]   The template in layouts/_default/list.searchindex.json isn\u0026#8217;t too complex either:\n {{- $.Scratch.Add \"searchindex\" slice -}} {{- range $index, $element := .Site.Pages -}} {{- $.Scratch.Add \"searchindex\" (dict \"id\" $index \"title\" $element.Title \"uri\" $element.Permalink \"tags\" $element.Params.tags \"section\" $element.Section \"content\" $element.Plain \"summary\" $element.Summary \"publicationdate\" ($element.Date.Format \"Jan 2, 2006\")) -}} {{- end -}} {{- $.Scratch.Get \"searchindex\" | jsonify -}}   The result is this JSON file:\n [...{\"content\":\"The JDK Flight Recorder (JFR) is an invaluable tool...\",\"id\":12,\"publicationdate\":\"Jan 29, 2020\",\"section\":\"blog\",\"summary\":\"\\u003cdiv class=\\\"paragraph\\\"\\u003e\\n\\u003cp\\u003eThe \\u003ca href=\\\"https://openjdk.java.net/jeps/328\\\"\\u003eJDK Flight Recorder\\u003c/a\\u003e (JFR) is an invaluable tool...\",\"tags\":[\"java\",\"monitoring\",\"microprofile\",\"jakartaee\",\"quarkus\"],\"title\":\"Monitoring REST APIs with Custom JDK Flight Recorder Events\",\"uri\":\"https://www.morling.dev/blog/rest-api-monitoring-with-custom-jdk-flight-recorder-events/\"},...]   This file gets automatically updated whenever I republish the blog.\n   Search Backend Implementation My stack of choice for this kind of application is Quarkus. As a contributor, I am of course biased, but Quarkus is ideal for the task at hand: built and optimized from the ground up for implementing fast-starting and memory-efficient cloud-native and Serverless applications, it makes building HTTP services, e.g. based on JAX-RS, running on GraalVM a trivial effort.\n Now typically a Java library such as Lucene will not run in a GraalVM native binary out-of-the-box. Things like reflection or JNI usage require specific configuration, while other Java features like method handles are only supported partly or not at all.\n Apache Lucene in a GraalVM Native Binary Quarkus enables a wide range of popular Java libraries to be used with GraalVM, but at this point there\u0026#8217;s no extension yet which would take care of Lucene. So I set out to implement a small Quarkus extension for Lucene. Depending on the implementation details of the library in question, this can be a more or less complex and time-consuming endeavor. The workflow is like so:\n   compile down an application using the library into a native image\n  run into some sort of exception, e.g. due to types accessed via Java reflection (which causes the GraalVM compiler to miss them during call flow analysis so that they are missing from the generated binary image)\n  fix the issue e.g. by registering the types in question for reflection\n  rinse and repeat\n   The good thing there is that the list of Quarkus extensions is constantly growing, so that you hopefully don\u0026#8217;t have to go through this by yourself. Or if you do, consider publishing your extension via the Quarkus platform, saving others from the same work.\n For my particular usage of Lucene, I ran luckily into two issues only. The first is the usage of method handles in the AttributeFactory class for dynamically instantiating sub-classes of the AttributeImpl type, which isn\u0026#8217;t supported in that form by GraalVM. One way for dealing with this is to define substitutions, custom methods or classes which will override a specific original implementation. As an example, here\u0026#8217;s one of the substitution classes I had to create:\n @TargetClass(className = \"org.apache.lucene.util.AttributeFactory$DefaultAttributeFactory\") public final class DefaultAttributeFactorySubstitution { public DefaultAttributeFactorySubstitution() {} @Substitute public AttributeImpl createAttributeInstance(Class\u0026lt;? extends Attribute\u0026gt; attClass) { if (attClass == BoostAttribute.class) { return new BoostAttributeImpl(); } else if (attClass == CharTermAttribute.class) { return new CharTermAttributeImpl(); } else if (...) { ... } throw new UnsupportedOperationException(\"Unknown attribute class: \" + attClass); } }   During native image creation, the GraalVM compiler will discover all substitute classes and apply their code instead of the original ones.\n The other problem I ran into was the usage of method handles in the MMapDirectory class, which will be used by Lucene by default on Linux when obtaining a file-system backed index directory. I didn\u0026#8217;t explore how to circumvent that, instead I opted for using the SimpleFSDirectory implementation which proved to work fine in my native GraalVM binary.\n While this was enough in order to get Lucene going in a native image, you might run into different issues when using other libraries with GraalVM native binaries. Quarkus comes with a rich set of so-called build items which extension authors can use in order to enable external dependencies on GraalVM, e.g. for registering classes for reflective access or JNI, adding additional resources to the image, and much more. I recommend you take a look at the extension author guide in order to learn more.\n Besides enabling Lucene on GraalVM, that Quarkus extension also does two more things:\n   Parse the previously extracted JSON file, build a Lucene index from that and store that index in the file system; that\u0026#8217;s fairly standard Lucene procedure without anything noteworthy; I only had to make sure that the index fields are stored in their original form in the search index, so that they can be accessed at runtime when displaying fragments with the query hits\n  Register a CDI bean, which allows to obtain the index at runtime via @Inject dependency injection from within the HTTP endpoint class\n   A downside of creating binaries via GraalVM is the increased build time: creating a native binary for macOS via a locally installed GraalVM SDK takes about two minutes on my laptop. For creating a Linux binary to be used with AWS Lambda, I need to run the build in a Linux container, which takes about five minutes. But typically this task is only done once when actually deploying the application, whereas locally I\u0026#8217;d work either with the Quarkus Dev Mode (which does a live reload of the application as its code changes) or test on the JVM. In any case it\u0026#8217;s a price worth paying: only with start-up times in the range of milli-seconds on-demand Serverless cold starts with the user waiting for a response become an option.\n  The Search HTTP Service The actual HTTP service implementation for running queries is rather unspectacular; It\u0026#8217;s based on JAX-RS and exposes as simple endpoint which can be invoked with a given query like so:\n http \"https://my-search-service/search?q=java\" HTTP/1.1 200 OK Connection: keep-alive Content-Length: 4930 Content-Type: application/json Date: Tue, 21 Jul 2020 17:05:00 GMT { \"message\": \"ok\", \"results\": [ { \"fragment\": \"...plug-ins. In this post I\u0026amp;#8217;m going to explore how the \u0026lt;b\u0026gt;Java\u0026lt;/b\u0026gt; Platform Module System's notion of module layers can be leveraged for implementing plug-in architectures on the JVM. We\u0026amp;#8217;ll also discuss how Layrry, a launcher and runtime for layered \u0026lt;b\u0026gt;Java\u0026lt;/b\u0026gt; applications, can help with this task. A key requirement...\", \"publicationdate\": \"Apr 21, 2020\", \"title\": \"Plug-in Architectures With Layrry and the \u0026lt;b\u0026gt;Java\u0026lt;/b\u0026gt; Module System\", \"uri\": \"https://www.morling.dev/blog/plugin-architectures-with-layrry-and-the-java-module-system/\" }, { \"fragment\": \"...the current behavior indeed is not intended (see JDK-8236597) and in a future \u0026lt;b\u0026gt;Java\u0026lt;/b\u0026gt; version the shorter version of the code shown above should work. Wrap-Up In this blog post we\u0026amp;#8217;ve explored how invariants on \u0026lt;b\u0026gt;Java\u0026lt;/b\u0026gt; 14 record types can be enforced using the Bean Validation API. With just a bit...\", \"publicationdate\": \"Jan 20, 2020\", \"title\": \"Enforcing \u0026lt;b\u0026gt;Java\u0026lt;/b\u0026gt; Record Invariants With Bean Validation\", \"uri\": \"https://www.morling.dev/blog/enforcing-java-record-invariants-with-bean-validation/\" }, ... ] }   Internally it\u0026#8217;s using Lucene\u0026#8217;s MultiFieldQueryParser for parsing the query and running it against the \"title\" and \"content\" fields of the index. It is set to combine multiple terms using the logical AND operator by default (who ever would want the default of OR?), it supports phrase queries given in quotes, and a number of other query operators.\n Query hits are highlighted using the FastVectorHighlighter highlighter and SimpleHTMLFormatter as a fallback (not all kinds of queries can be processed by FastVectorHighlighter). The highlighter wraps the matched search terms in the returned fragment in \u0026lt;b\u0026gt; tags, which are styled appropriately in my website\u0026#8217;s CSS. I was prepared to do some adjustments to result scoring, but this wasn\u0026#8217;t necessary so far. Title matches are implicitly ranked higher than content matches due to the shorter length of the title field values.\n Implementing the service using a standard HTTP interface instead of relying on specific AWS Lambda contracts is great in terms of local testing as well as portability: I can work on the service using the Quarkus Dev Mode and invoke it locally, without having to deploy it into some kind of Lambda test environment. It also means that should the need arise, I can take this service and run it elsewhere, without requiring any code changes. As I\u0026#8217;ll discuss in a bit, Quarkus takes care of making this HTTP service runnable within the Lambda environment by means of a single dependency configuration.\n    Wiring Things Up Now it was time to hook up the search service into my blog. I wouldn\u0026#8217;t want to have the user navigate to the URL of the AWS API Gateway in their browser; this means that the form with the search text input field cannot actually be submitted. Instead, the default form handling must be disabled, and the search string be sent via JavaScript to the API Gateway URL.\n This means the search feature won\u0026#8217;t work for users who have JavaScript disabled in their browser. I deemed this an acceptable limitation; in order to avoid unnecessary confusion and frustration, the search text input field is hidden in that case via CSS:\n \u0026lt;noscript\u0026gt; \u0026lt;style type=\"text/css\"\u0026gt; .search-input { display:none; } \u0026lt;/style\u0026gt; \u0026lt;/noscript\u0026gt;   The implementation of the backend call is fairly standard JavaScript business using the XMLHttpRequest API, so I\u0026#8217;ll spare you the details here. You can find the complete implementation in my GitHub repo.\n There\u0026#8217;s one interesting detail to share though in terms of improving the user experience after a cold start. As mentioned above, the Quarkus application itself starts up on Lambda in about ~180 ms. Together with the initialization of the Lambda execution environment I typically see ~370 ms for a cold start. Add to that the network round-trip times, and a user will feel a slight delay. Nothing dramatical, but it doesn\u0026#8217;t have that snappy instant feeling you get when executing the search with a warm environment.\n Thinking about the typical user interaction though, the situation can be nicely improved: if a visitor puts the focus onto the search text input field, it\u0026#8217;s highly likely that they will submit a query shortly thereafter. We can take advantage of that and have the website send a small \"ping\" request right at the point when the input field obtains the focus. This gives us enough headstart to have the Lambda function being started before the actual query comes in. Here\u0026#8217;s the request flow of a typical interaction (the \"Other\" requests are CORS preflight requests):\n   Note how the search call is issued only a few hundred ms after the ping. Now you could beat this e.g. when navigating to the text field using your keyboard and if you were typing really fast. But most users will use their mouse or touchpad to put the cursor into the input, and then change to the keyboard to enter the query, which is time enough for this little trick to work.\n The analysis of the logs confirms that essentially all executed queries hit a warmed up Lambda function, making cold starts a non-issue. To avoid any unneeded warm-up calls, they are only done when entering the input field for the first time after loading the page, or when staying on the page for long enough, so that the Lambda might have shut down again due to lack of activity.\n Of course you\u0026#8217;ll be charged for the additional ping requests, but for the volume I expect, this makes no relevant difference whatsoever.\n   Deployment to AWS Lambda The last part of my journey towards a Serverless search function was deployment to AWS Lambda. I was exploring Heroku and Google Cloud Run as alternatives, too. Both allow you to deploy regular container images, which then are automatically scaled on demand. This results in great portability, as things hardly can get any more standard than plain Linux containers.\n With Heroku, cold start times proved problematic, though: I observed 5 - 6 seconds, which completely ruling it out. This wasn\u0026#8217;t a problem with Cloud Run, and it\u0026#8217;d surely work very well overall. In the end I went for AWS Lambda, as its entire package of service runtime, API Gateway and web application firewall seemed more complete and mature to me.\n With AWS Lambda, I observed cold start times of less than 0.4 sec for my actual Lambda function, plus the actual request round trip. Together with the warm-up trick described above, this means that a user practically never will get a cold start when executing the search.\n You shouldn\u0026#8217;t under-estimate the time needed though to get familiar with Lambda itself, the API Gateway which is needed for routing HTTP requests to your function and the interplay of the two.\n To get started, I configured some playground Lambda and API in the web console, but eventually I needed something along the lines of infrastructure-as-code, means of reproducible and automated steps for configuring and setting up all the required components. My usual go-to solution in this area is Terraform, but here I settled for the AWS Serverless Application Model (SAM), which is tailored specifically to setting up Serverless apps via Lambda and API Gateway and thus promised to be a bit easier to use.\n Building Quarkus Applications for AWS Lambda Quarkus supports multiple approaches for building Lambda-based applications:\n   You can directly implement Lambda\u0026#8217;s APIs like RequestHandler, which I wanted to avoid though for the sake of portability between different environments and cloud providers\n  You can use the Quarkus Funqy API for building portable functions which e.g. can be deployed to AWS, Azure Functions and Google Cloud Functions; the API is really straight-forward and it\u0026#8217;s a very attractive option, but right now there\u0026#8217;s no way to use Funqy for implementing an HTTP GET API with request parameters, which ruled out this option for my purposes\n  You can implement your Lambda function using the existing and well-known HTTP APIs of Vert.x, RESTEasy (JAX-RS) and Undertow; in this case Quarkus will take care of mapping the incoming function call to the matching HTTP endpoint of the application\n   Used together with the proxy feature of the AWS API Gateway, the third option is exactly what I was looking for. I can implement the search endpoint using the JAX-RS API I\u0026#8217;m familiar with, and the API Gateway proxy integration together with Quarkus' glue code will take care of everything else for running this. This is also great in terms of portability: I only need to add the io.quarkus:quarkus-amazon-lambda-http dependency to my project, and the Quarkus build will emit a function.zip file which can be deployed to AWS Lambda. I\u0026#8217;ve put this into a separate Maven build profile, so I can easily switch between creating the Lambda function deployment package and a regular container image with my REST endpoint which I can deploy to Knative and environments like OpenShift Serverless, without requiring any code changes whatsoever.\n The Quarkus Lambda extension also produces templates for the AWS SAM tool for deploying my stack. They are a good starting point which just needs a little bit of massaging; For the purposes of cost control (see further below), I added an API usage plan and API key. I also enabled CORS so that the API can be called from my static website. This made it necessary to disable the configuration of binary media types which the generated template contains by default. Lastly, I used a specific pre-configured execution role instead of the default AWSLambdaBasicExecutionRole.\n With the SAM descriptor in place, re-building and publishing the search service becomes a procedure of three steps:\n mvn clean package -Pnative,lambda -DskipTests=true \\ -Dquarkus.native.container-build=true sam package --template-file sam.native.yaml \\ --output-template-file packaged.yaml \\ --s3-bucket \u0026lt;my S3 bucket\u0026gt; sam deploy --template-file packaged.yaml \\ --capabilities CAPABILITY_IAM \\ --stack-name \u0026lt;my stack name\u0026gt;   The lambda profile takes care of adding the Quarkus Lambda HTTP extension, while the native profile makes sure that a native binary is built instead of a JAR to be run on the JVM. As I need to build a Linux binary for the Lambda function while running on macOS locally, I\u0026#8217;m using the -Dquarkus.native.container-build=true option, which will make the Quarkus build running in a container itself, producing a Linux binary no matter which platform this build itself is executed on.\n The function.zip file produced by the Quarkus build has a size of ~15 MB, i.e. it\u0026#8217;s uploaded and deployed to Lambda in a few seconds. Currently it also contains the Lucene search index, meaning I need to run the time-consuming GraalVM build whenever I want to update the index. As an optimization I might at some point extract the index into a separate Lambda layer, which then could be deployed by itself, if there were no code changes to the search service otherwise.\n  Identity and Access Management A big pain point for me was identity and access management (IAM) for the AWS API Gateway and Lambda. While the AWS IAM is really powerful and flexible, there\u0026#8217;s unfortunately no documentation, which would describe the minimum set of required permissions in order to deploy a stack like my search using SAM.\n Things work nicely if you use a highly-privileged account, but I\u0026#8217;m a strong believer into running things with only the least privileges needed for the job. For instance I don\u0026#8217;t want my Lambda deployer to set up the execution role, but rather have it using one I pre-defined. The same goes for other resources like the S3 bucket used for uploading the deployment package.\n Identifying the set of privileges actually needed is a rather soul-crushing experience of trial and error (please let me know in the comments below if there\u0026#8217;s a better way to do this), which gets complicated by the fact that different resources in the AWS stack expose insufficient privileges in inconsistent ways, or sometimes in no really meaningful way at all when configured via SAM. I spent hours identifying a lacking S3 privilege when trying to deploy a Lambda layer from the Serverless Application Repository.\n Hoping to spare others from this tedious work, here\u0026#8217;s the policy for my deployment role I came up with:\n {\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":[\"s3:PutObject\",\"s3:GetObject\"],\"Resource\":[\"arn:aws:s3:::\u0026lt;deployment-bucket\u0026gt;\",\"arn:aws:s3:::\u0026lt;deployment-bucket\u0026gt;/*\"]},{\"Effect\":\"Allow\",\"Action\":[\"lambda:CreateFunction\",\"lambda:GetFunction\",\"lambda:GetFunctionConfiguration\",\"lambda:AddPermission\",\"lambda:UpdateFunctionCode\",\"lambda:ListTags\",\"lambda:TagResource\",\"lambda:UntagResource\"],\"Resource\":[\"arn:aws:lambda:eu-central-1:\u0026lt;account-id\u0026gt;:function:search-morling-dev-SearchMorlingDev-*\"]},{\"Effect\":\"Allow\",\"Action\":[\"iam:PassRole\"],\"Resource\":[\"arn:aws:iam::\u0026lt;account-id\u0026gt;:role/\u0026lt;execution-role\u0026gt;\"]},{\"Effect\":\"Allow\",\"Action\":[\"cloudformation:DescribeStacks\",\"cloudformation:DescribeStackEvents\",\"cloudformation:CreateChangeSet\",\"cloudformation:ExecuteChangeSet\",\"cloudformation:DescribeChangeSet\",\"cloudformation:GetTemplateSummary\"],\"Resource\":[\"arn:aws:cloudformation:eu-central-1:\u0026lt;account-id\u0026gt;:stack/search-morling-dev/*\",\"arn:aws:cloudformation:eu-central-1:aws:transform/Serverless-2016-10-31\"]},{\"Effect\":\"Allow\",\"Action\":[\"apigateway:POST\",\"apigateway:PATCH\",\"apigateway:GET\"],\"Resource\":[\"arn:aws:apigateway:eu-central-1::/restapis\",\"arn:aws:apigateway:eu-central-1::/restapis/*\"]},{\"Effect\":\"Allow\",\"Action\":[\"apigateway:POST\",\"apigateway:GET\"],\"Resource\":[\"arn:aws:apigateway:eu-central-1::/usageplans\",\"arn:aws:apigateway:eu-central-1::/usageplans/*\",\"arn:aws:apigateway:eu-central-1::/apikeys\",\"arn:aws:apigateway:eu-central-1::/apikeys/search-morling-dev-apikey\"]}]}   Perhaps this could be trimmed down some more, but I felt it\u0026#8217;s good enough for my purposes.\n  Performance At this point I haven\u0026#8217;t conducted any systematic performance testing yet. There\u0026#8217;s definitely a significant difference in terms of latency between running things locally on my (not exactly new) laptop and on AWS Lambda. Where the app starts up in ~30 ms locally, it\u0026#8217;s ~180 ms when deployed to Lambda. Note this is only the number reported by Quarkus itself, the entire cold start duration of the application on Lambda, i.e. including the time required for fetching the code to the execution environment and starting the container, is ~370 ms (with 256 MB RAM assigned). Due to the little trick described above, though, a visitor is very unlikely to ever experience this delay when executing a query.\n Similarly, there\u0026#8217;s a substantial difference in terms of request execution duration. Still, when running a quick test of the deployed service via Siege, the vast majority of Lambda executions clocked in well below 100 ms (depending on the number of query hits which need result highlighting), putting them into the lowest bracket of billed Lambda execution time. As I learned, Lambda allocates CPU resources proportionally to assigned RAM, meaning assigning twice as much RAM should speed up execution, also if my application actually does not need that much memory. Indeed, with 512 MB RAM assigned, Lambda execution is down to ~30 - 40 ms after some warm-up, which is more than good enough for my purposes.\n Raw Lambda execution of course is only one part of the overall request duration, on top of that some time is spent in the API Gateway and on the wire to the user; The service is deployed in the AWS eu-central-1 region (Frankfurt, Germany), yielding roundtrip times for me, living a few hundred km away, between 50 - 70 ms (again with 512 MB RAM). With longer distances, network latencies outweigh the Lambda execution time: My good friend Eric Murphy from Seattle in the US reported a roundtrip time of ~240 ms when searching for \"Java\", which I think is still quite good, given the long distance.\n  Cost Control The biggest issue for me as a hobbyist when using pay-per-use services like AWS Lambda and API Gateway is cost control. Unlike typical enterprise scenarios where you might be willing to accept higher cost for your service in case of growing demand, in my case I\u0026#8217;d rather set up a fixed spending limit and shut down my search service for the rest of the month, once that has been reached. I absolutely cannot have an attacker doing millions and millions of calls against my API which could cost me a substantial amount of money.\n Unfortunately, there\u0026#8217;s no easy way on AWS for setting up a maximum spending after which all service consumption would be stopped. Merely setting up a budget alert won\u0026#8217;t cut it either, as this won\u0026#8217;t help me while sitting on a plane for 12h (whenever that will be possible again\u0026#8230;\u0026#8203;) or being on vacation for three weeks. And needless to say, I don\u0026#8217;t have an ops team monitoring my blog infrastructure 24/7 either.\n So what to do to keep costs under control? An API usage plan is the first part of the answer. It allows you to set up a quota (maximum number of calls in a given time frame) which is pretty much what I need. Any calls beyond the quota are rejected by the API Gateway and not charged.\n There\u0026#8217;s one caveat though: a usage plan is tied to an API key, which the caller needs to pass using the X-API-Key HTTP request header. The idea being that different usage plans can be put in place for different clients of an API. Any calls without the API key are not charged either. Unfortunately though this doesn\u0026#8217;t play well with CORS preflight requests as needed in my particular use case. Such requests will be sent by the browser before the actual GET calls to validate that the server actually allows for that cross-origin request. CORS preflight requests cannot have any custom request headers, though, meaning they cannot be part of a usage plan. The AWS docs are unclear whether those preflight requests are charged or not, and in a way it seems unfair if they were charged given there\u0026#8217;s no way to prevent this situation. But at this point it is fair to assume they are charged and we need a way to prevent having to pay for a gazillion preflight calls by a malicious actor.\n In good software developer\u0026#8217;s tradition I turned to Stack Overflow for finding help, and indeed I received a nice idea: A budget alert can be linked with an SNS topic, to which a message will be sent once the alert triggers. Then another Lambda function can be used to set the allowed rate of API invocations to 0, effectively disabling the API, preventing any further cost to pile up. A bit more complex than I was hoping for, but it does the trick. Thanks a lot to Harish for providing this nice answer on Stack Overflow and his blog! I implemented this solution and sleep much better now.\n Note that you should set the alert to a lower value than what you\u0026#8217;re actually willing to spend, as billing happens asynchronously and requests might come in some more time until the alert triggers: as per Corey Quinn, there\u0026#8217;s an \"8-48 hour lag between 'you incur the charge' and 'it shows up in the billing system where an alert can see it and thus fire'\". It\u0026#8217;s therefore also a good idea to reduce the allowed request rate. E.g. in my case I\u0026#8217;m not expecting really that there\u0026#8217;d be more than let\u0026#8217;s say 25 concurrent requests (unless this post hits the Hackernews front page of course), so setting the allowed rate to that value helps to at least slow down the spending until the alert triggers.\n With these measures in place, there should (hopefully!) be no bad surprises at the end of the month. Assuming a (very generously estimated) number of 10K search queries per month, each returning a payload of 5 KB, I\u0026#8217;d be looking at an invoice over EUR 0.04 for the API Gateway, while the Lambda executions would be fully covered by the AWS free tier. That seems manageable :)\n    Wrap-Up and Outlook Having rolled out the search feature for this blog a few days ago, I\u0026#8217;m really happy with the outcome. It was a significant amount of work to put everything together, but I think a custom search is a great addition to this site which hopefully proves helpful to my readers. Serverless is a perfect architecture and deployment option for this use case, being very cost-efficient for the expected low volume of requests, and providing a largely hands-off operations experience for myself.\n With AOT compilation down to native binaries and enabling frameworks like Quarkus, Java definitely is in the game for building Serverless apps. Its huge eco-system of libraries such as Apache Lucene, sophisticated tooling and solid performance make it a very attractive implementation choice. Basing the application on Quarkus makes it a matter of configuration to switch between creating a deployment package for Lambda and a regular container image, avoiding any kind of lock-in into a specific platform.\n Enabling libraries for being used in native binaries can be a daunting task, but over time I\u0026#8217;d expect either library authors themselves to do the required adjustment to smoothen that experience, and of course the growing number of Quarkus extensions also helps to use more and more Java libraries in native apps. I\u0026#8217;m also looking forward to Project Leyden, which aims at making AOT compilation a part of the Java core platform.\n The deployment to AWS Lambda and API Gateway was definitely more involved than I had anticipated; things like IAM and budget control are more complex than I think they could and should be. That there is no way to set up a hard spend capping is a severe shortcoming; hobbyists like myself should be able to explore this platform without having to fear any surprise AWS bills. It\u0026#8217;s particular bothersome that API usage plans are no 100% safe way to enforce API quotas, as they cannot be applied to unauthorized CORS pref-flight requests and custom scripting is needed in order to close this loophole.\n But then this experiment also was an interesting learning experience for me; working on libraries and integration solutions most of the time during my day job, I sincerely enjoyed the experience of designing a service from the ground-up and rolling it out into \"production\", if I may dare to use that term here.\n While the search functionality is rolled out on my blog, ready for you to use, there\u0026#8217;s a few things I\u0026#8217;d like to improve and expand going forward:\n   CI pipeline: Automatically re-building and deploying the search service after changes to the contents of my blog; this should hopefully be quite easy using GitHub Actions\n  Performance improvements: While the performance of the query service definitely is good enough, I\u0026#8217;d like to see whether and how it could be tuned here and there. Tooling might be challenging there; where I\u0026#8217;d use JDK Flight Recorder and Mission Control with a JVM based application, I\u0026#8217;m much less familiar with equivalent tooling for native binaries. One option I\u0026#8217;d like to explore in particular is taking advantage of Quarkus bytecode recording capability: bytecode instructions for creating the in-memory data structure of the Lucene index could be recorded at build time and then just be executed at application start-up; this might be the fastest option for loading the index in my special use case of a read-only index\n  Serverless comments: Currently I\u0026#8217;m using Disqus for the commenting feature of the blog. It\u0026#8217;s not ideal in terms of privacy and page loading speed, which is why I\u0026#8217;m looking for alternatives. One idea could be a custom Serverless commenting functionality, which would be very interesting to explore, in particular as it shifts the focus from a purely immutable application to a stateful service that\u0026#8217;ll require some means of modifiable, persistent storage\n   In the meantime, you can find the source code of the Serverless search feature on GitHub. Feel free to take the code and deploy it to your own website!\n Many thanks to Hans-Peter Grahsl and Eric Murphy for their feedback while writing this post!\n  ","id":5,"publicationdate":"Jul 29, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003e\u003cem\u003eI have built a custom search functionality for this blog,\nbased on Java and the Apache Lucene full-text search library,\ncompiled into a native binary using the Quarkus framework and GraalVM.\nIt is deployed as a Serverless application running on AWS Lambda,\nproviding search results without any significant cold start delay.\nIf you thought Java wouldn\u0026#8217;t be the right language for this job, keep reading;\nin this post I\u0026#8217;m going to give an overview over the implementation of this feature and my learnings along the way.\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"How I Built a Serverless Search for My Blog","uri":"https://www.morling.dev/blog/how-i-built-a-serverless-search-for-my-blog/"},{"content":"Ahead-of-time compilation (AOT) is the big topic in the Java ecosystem lately: by compiling Java code to native binaries, developers and users benefit from vastly improved start-up times and reduced memory usage. The GraalVM project made huge progress towards AOT-compiled Java applications, and Project Leyden promises to standardize AOT in a future version of the Java platform.\n This makes it easy to miss out on significant performance improvements which have been made on the JVM in recent Java versions, in particular when it comes to faster start-up times. Besides a range of improvements related to class loading, linking and bytecode verification, substantial work has been done around class data sharing (CDS). Faster start-ups are beneficial in many ways: shorter turnaround times during development, quicker time-to-first-response for users in coldstart scenarios, cost savings when billed by CPU time in the cloud.\n With CDS, class metadata is persisted in an archive file, which during subsequent application starts is mapped into memory. This is faster than loading the actual class files, resulting in reduced start-up times. When starting multiple JVM processes on the same host, read-only archives of class metadata can also be shared between the VMs, so that less memory is consumed overall.\n Originally a partially commercial feature of the Oracle JDK, CDS was completely open-sourced in JDK 10 and got incrementally improved since then in a series of Java improvement proposals:\n   JEP 310, Application Class-Data Sharing (AppCDS), in JDK 10: \"To improve startup and footprint, extend the existing [CDS] feature to allow application classes to be placed in the shared archive\"\n  JEP 341, Default CDS Archives, in JDK 12: \"Enhance the JDK build process to generate a class data-sharing (CDS) archive, using the default class list, on 64-bit platforms\"\n  JEP 350, Dynamic CDS Archives, in JDK 13: \"Extend application class-data sharing to allow the dynamic archiving of classes at the end of Java application execution. The archived classes will include all loaded application classes and library classes that are not present in the default, base-layer CDS archive\"\n   In the remainder of this blog post we\u0026#8217;ll discuss how to automatically create AppCDS archives as part of your (Maven) project build, based on the improvements made with JEP 350. I.e. Java 13 or later is a prerequisite for this. To learn more about using CDS with the current LTS release JDK 11 and about CDS in general, refer to the excellent blog post on everything CDS by Nicolai Parlog.\n Manually Creating CDS Archives At first let\u0026#8217;s see what\u0026#8217;s needed to manually create and use an AppCDS archive (note I\u0026#8217;m going to use \"AppCDS\" and \"CDS\" somewhat interchangeably for the sake of brevity). Subsequently, we\u0026#8217;ll discuss how the task can be automated in a Maven project build.\n To have an example to work with which goes beyond a plain \"Hello World\", I\u0026#8217;ve created a small web application for managing personal to-dos, using the Quarkus stack. If you\u0026#8217;d like to follow along, clone the repo and build the project:\n git clone git@github.com:gunnarmorling/quarkus-cds.git cd quarkus-cds mvn clean verify -DskipTests=true   The application uses a Postgres database for persisting the to-dos; fire it up via Docker:\n cd compose docker run -d -p 5432:5432 --name pgdemodb \\ -v $(pwd)/init.sql:/docker-entrypoint-initdb.d/init.sql \\ -e POSTGRES_USER=todouser \\ -e POSTGRES_PASSWORD=todopw \\ -e POSTGRES_DB=tododb postgres:11   The next step is to run the application and create the CDS archive file. Do so by passing the -XX:ArchiveClassesAtExit option:\n java -XX:ArchiveClassesAtExit=target/app-cds.jsa \\ (1) -jar target/todo-manager-1.0.0-SNAPSHOT-runner.jar     1 Triggers creation of a CDS archive at the given location upon application shutdown    Only loaded classes will be added to the archive. As classloading on the JVM happens lazily, you must invoke some functionality in your application in order to cause all the relevant classes to be loaded. For that to happen, open the application\u0026#8217;s API endpoint in a browser or invoke it via curl, httpie or similar:\n http localhost:8080/api   Stop the application by hitting Ctrl+C. This will create the CDS archive under target/app-cds.jsa. In our case it should have a size of about 41 MB. Also observe the log messages about classes which were skipped from archiving:\n ... [190.220s][warning][cds] Skipping java/lang/invoke/LambdaForm$MH+0x0000000800bd0c40: Hidden or Unsafe anonymous class [190.220s][warning][cds] Skipping java/lang/invoke/LambdaForm$DMH+0x0000000800fdc840: Hidden or Unsafe anonymous class [190.220s][warning][cds] Pre JDK 6 class not supported by CDS: 46.0 antlr/TokenStreamIOException ...   Mostly this is about hidden or anonymous classes which cannot be archived; there\u0026#8217;s not so much you can do about that (apart from using less Lambda expressions perhaps\u0026#8230;\u0026#8203;).\n The hint on old classfile versions is more actionable: only classes using classfile format 50 (= JDK 1.6) or newer are supported by CDS. In the case at hand, the classes from Antlr 2.7.7 are using classfile format 46 (which was introduced in Java 1.2) and thus cannot be added to the CDS archive. Note this also applies to any subclasses, even if they themselves use a newer classfile format version.\n It\u0026#8217;s thus a good idea to check whether you can upgrade to newer versions of your dependencies, as this may result in more classes becoming available for CDS, resulting in better start-up times in turn.\n   Using the CDS Archive Now let\u0026#8217;s run the application again, this time using the previously created CDS archive:\n java -XX:SharedArchiveFile=target/app-cds.jsa \\ (1) -Xlog:class+load:file=target/classload.log \\ (2) -Xshare:on \\ (3) -jar target/todo-manager-1.0.0-SNAPSHOT-runner.jar     1 The path to the CDS archive   2 classloading logging allows to verify whether the CDS archive gets applied as expected   3 While class data sharing is enabled by default on JDK 12 and newer, explicitely enforcing it will ensure an error is raised if something is wrong, e.g. a mismatch of Java versions between building and using the archive    When examining the classload.log file, you should see how most class metadata is obtained from the CDS archive (\"source: shared object file\"), while some classes such as the ancient Antlr classes are loaded just as usual from the corresponding JAR:\n [0.016s][info][class,load] java.lang.Object source: shared objects file [0.016s][info][class,load] java.io.Serializable source: shared objects file [0.016s][info][class,load] java.lang.Comparable source: shared objects file [0.016s][info][class,load] java.lang.CharSequence source: shared objects file ... [2.555s][info][class,load] antlr.Parser source: file:/.../antlr.antlr-2.7.7.jar ...   Note it is vital that the exact same Java version is used as when creating the archive, otherwise an error will be raised. Unfortunately, this also means that AppCDS archives cannot be built cross-platform. This would be very useful, e.g. when building a Java application on macOS or Windows, which should be packaged in a Linux container. If you are aware of a way for doing so, please let me know in the comments below.\n     CDS and the Java Module System Beginning with Java 11, not only classes from the classpath can be added to CDS archives, but also classes from the module path of a modularized Java application. One important detail to consider there is that the --upgrade-module-path and --patch-module options will cause CDS to be disabled or disallowed (with -Xshare:on) is specified. This is to avoid a mismatch of class metadata in the CDS archive and classes brought in by a newer module version.\n       Creating CDS Archives in Your Maven Build Manually creating a CDS archive is not very efficient nor reliable, so let\u0026#8217;s see how the task can be automated as part of your project build. The following shows the required configuration when using Apache Maven, but of course the same approach could be implemented with Gradle or any other build system.\n The basic idea is the follow the same steps as before, but executed as part of the Maven build:\n  start up the application with the -XX:ArchiveClassesAtExit option\n  invoke some application functionality to initiate the loading of all relevant classes\n  stop the application\n       It might appear as a compelling idea to produce the CDS archive as part of regular test execution, e.g. via JUnit. This will not work though, as the classpath at the time of using the CDS archive must be not miss any entries from the classpath at the time of creating it. As during test execution all the test-scoped dependencies will be part of the classpath, any CDS archive created that way couldn\u0026#8217;t be used when running the application later on without those test dependencies.\n     Steps 1. and 3. can be automated with help of the Process-Exec Maven plug-in, binding it to the pre-integration-test and post-integration-test build phases, respectively. While I was thinking of using the more widely known Exec plug-in initially, this turned out to not be viable as there\u0026#8217;s no way for stopping any forked process in a later build phase.\n Here\u0026#8217;s the relevant configuration:\n ... \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;com.bazaarvoice.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;process-exec-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.9\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; (1) \u0026lt;id\u0026gt;app-cds-creation\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;pre-integration-test\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;start\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;name\u0026gt;todo-manager\u0026lt;/name\u0026gt; \u0026lt;healthcheckUrl\u0026gt;http://localhost:8080/\u0026lt;/healthcheckUrl\u0026gt; (2) \u0026lt;arguments\u0026gt; \u0026lt;argument\u0026gt;java\u0026lt;/argument\u0026gt; (3) \u0026lt;argument\u0026gt;-XX:ArchiveClassesAtExit=app-cds.jsa\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;-jar\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt; ${project.build.directory}/${project.artifactId}-${project.version}-runner.jar \u0026lt;/argument\u0026gt; \u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;execution\u0026gt; (4) \u0026lt;id\u0026gt;stop-all\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;post-integration-test\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;stop-all\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; ...     1 Start up the application in the pre-integration-test build phase   2 The health-check URL is used to await application start-up before proceeding with the next build phase   3 Assemble the java invocation   4 Stop the application in the post-integration-test build phase    What remains to be done is the automation of step 2, the invocation of the required application logic so to trigger the loading of all relevant classes. This can be done with help of the Maven Surefire plug-in. A simple \"integration test\" via REST Assured does the trick:\n public class ExampleResourceAppCds { @Test public void getAll() { given() .when() .get(\"/api\") .then() .statusCode(200); } }   We just need to configure a specific execution of the plug-in, which only picks up any test classes whose names end with *AppCds.java, so to keep them apart from actual integration tests:\n ... \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-failsafe-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0-M4\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;integration-test\u0026lt;/goal\u0026gt; \u0026lt;goal\u0026gt;verify\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;includes\u0026gt; \u0026lt;include\u0026gt;**/*AppCds.java\u0026lt;/include\u0026gt; \u0026lt;/includes\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; ...   And that\u0026#8217;s all we need; when now building the project via mvn clean verify, a CDS archive will be created at target/app-cds.jsa. You can find the complete example project and steps for building/running it on GitHub.\n   What Do You Gain? Creating a CDS archive is nice, but is it also worth the effort? In order to answer this question, I\u0026#8217;ve done some measurements of the \"time-to-first-response\" metric, following the Quarkus guide on measuring performance. I.e. instead of awaiting some rather meaningless \"start-up complete\" status, which could arbitrarily be tweaked by means of lazy initialization, this measures the time until the application is actually ready to handle the first incoming request after start-up.\n I\u0026#8217;ve done measurements on OpenJDK 1.8.0_252 (AdoptOpenJDK build), OpenJDK 14.0.1 (upstream build, without and with AppCDS), and OpenJDK 15-ea-b26 (upstream build, with AppCDS). Please see the README file of the example repo for the exact steps.\n Here are the numbers, averaged over ten runs each:\n   Update, June 12th: I had originally classload logging enabled for the OpenJDK 14 AppCDS runs, which added an unneccessary overhead (thanks a lot to Claes Redestad for pointing this out!). The numbers and chart have been updated accordingly. I\u0026#8217;ve also added numbers for OpenJDK 15-ea.\n Time-to-first-response values are 2s 267ms, 2s 162ms, 1s 669ms 1s 483ms, and 1s 279ms. I.e. on my machine (2014 MacBook Pro), with this specific workload, there\u0026#8217;s an improvement of ~100ms just by upgrading to the current JDK, and of another ~500ms ~700ms by using AppCDS.\n With OpenJDK 15 things will further improve. The latest EA build at the time of writing (b26) shortens time-to-first-response by another ~200ms. The upcoming EA build 27 should bring another improvement, as Lambda proxy classes will be added to AppCDS archives then.\n That all is definitely a nice improvement, in particular as we get it essentially for free, without any changes to the actual application itself. You should contrast this with the additional size of the application distribution, though. E.g. when obtaining the application as a container image from a remote container registry, downloading the additional ~40 MB might take longer than the time saved during application start-up. Typically, this will only affect the first start-up of on a particular node, though, after which the image will be cached locally.\n As always when it comes to any kinds of performance numbers, please take these numbers with a grain of salt, do your own measurements, using your own applications and in your own environment.\n     Addressing Different Workload Profiles If your application supports different \"work modes\", e.g. \"online\" and \"batch\", which work with a largely differing set of classes, you also might consider to create different CDS archives for the specific workloads. This might give you a good balance between additional size and realized improvements of start-up times, when for instance dealing with at large monolithic application instead of more fine-grained microservices.\n       Wrap-Up AppCDS provides Java developers with a useful tool for reducing start-up times of their applications, without requiring any code changes. For the example discussed, we could observe an improvement of the time-to-first-response metric by about 30% when running with OpenJDK 14. Other users reported even bigger improvements.\n We didn\u0026#8217;t discuss any potential memory improvements due to CDS when sharing class metadata between multiple JVMs on one host. In containerized server applications, with each JVM being packaged in its own container image, this won\u0026#8217;t play a role. It could make a difference on desktop systems, though. For instance multiple instances of the Java language server, as leveraged by VSCode and other editors, could benefit from that.\n That all being said, when raw start-up time is your primary concern, e.g. in a serverless or Function-based setting, you should look at AOT compilation with GraalVM (or Project Leyden in the future). This will bring down start-up times to a completely different level; for example the todo manager application would return a first response within a few 10s of milliseconds when executed as a native image via GraalVM.\n But AOT is not always an option, nor does it always make sense: the JVM may offer a better latency than native binaries, external dependencies migh not be ready for usage in AOT-compiled native images yet, or you simply might want to be able to benefit from all the JVM goodness, like familiar debugging tools, the JDK Flight Recorder, or JMX. In that case, CDS can give you a nice start-up time improvement, solely by means of adding a few steps to your build process.\n Besides class data sharing in OpenJDK, there are some other related techniques for improving start-up times which are worth exploring:\n   Eclipse OpenJ9 has its own implementation of class data sharing\n  Alibaba\u0026#8217;s Dragonwell distribution of the OpenJDK comes with JWarmUp, a tool for speeding up initial JIT compilations\n   To learn more about AppCDS, a long yet insightful post is this one by Vladimir Plizga. Volker Simonis did another interesting write-up. Also take a look at the CDS documentation in the reference docs of the java command.\n Lastly, the Quarkus team is working on out-of-the-box support for CDS archives. This could fully automate the creation of an archive for all required classes without any further configuration, making it even easier to benefit from the start-up time improvements promised by CDS.\n  ","id":6,"publicationdate":"Jun 11, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eAhead-of-time compilation (AOT) is \u003cem\u003ethe\u003c/em\u003e big topic in the Java ecosystem lately:\nby compiling Java code to native binaries, developers and users benefit from vastly improved start-up times and reduced memory usage.\nThe \u003ca href=\"https://www.graalvm.org/\"\u003eGraalVM\u003c/a\u003e project made huge progress towards AOT-compiled Java applications,\nand \u003ca href=\"https://mail.openjdk.java.net/pipermail/discuss/2020-April/005429.html\"\u003eProject Leyden\u003c/a\u003e promises to standardize AOT in a future version of the Java platform.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThis makes it easy to miss out on significant performance improvements which have been made on the JVM in recent Java versions,\nin particular when it comes to \u003ca href=\"https://cl4es.github.io/2019/11/20/OpenJDK-Startup-Update.html\"\u003efaster start-up times\u003c/a\u003e.\nBesides a range of improvements related to class loading, linking and bytecode verification,\nsubstantial work has been done around \u003ca href=\"https://docs.oracle.com/en/java/javase/14/vm/class-data-sharing.html\"\u003eclass data sharing\u003c/a\u003e (CDS).\nFaster start-ups are beneficial in many ways:\nshorter turnaround times during development,\nquicker time-to-first-response for users in coldstart scenarios,\ncost savings when billed by CPU time in the cloud.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eWith CDS, class metadata is persisted in an archive file,\nwhich during subsequent application starts is mapped into memory.\nThis is faster than loading the actual class files, resulting in reduced start-up times.\nWhen starting multiple JVM processes on the same host, read-only archives of class metadata can also be shared between the VMs, so that less memory is consumed overall.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Building Class Data Sharing Archives with Apache Maven","uri":"https://www.morling.dev/blog/building-class-data-sharing-archives-with-apache-maven/"},{"content":"Do you remember Angus \"Mac\" MacGyver? The always creative protagonist of the popular 80ies/90ies TV show, who could solve about any problem with nothing more than a Swiss Army knife, duct tape, shoe strings and a paper clip?\n The single message transformations (SMTs) of Kafka Connect are almost as versatile as MacGyver\u0026#8217;s Swiss Army knife:\n   How to change the timezone or format of date/time message fields?\n  How to change the topic a specific message gets sent to?\n  How to filter out specific records?\n   SMTs can be the answer to these and many other questions that come up in the context of Kafka Connect. Applied to source or sink connectors, SMTs allow to modify Kafka records before they are sent to Kafka, or after they are consumed from a topic, respectively.\n In this post I\u0026#8217;d like to focus on some interesting (hopefully anyways) usages of SMTs. Those use cases are mostly based on my experiences from using Kafka Connect with Debezium, an open-source platform for change data capture (CDC). I also got some great pointers on interesting SMT usages when asking the community about this on Twitter some time ago:\n    I definitely recommend to check out the thread; thanks a lot to all who replied! In order to learn more about SMTs in general, how to configure them etc., refer to the resources given towards the end of this post.\n For each category of use cases, I\u0026#8217;ve also asked our sympathetic TV hero for his opinion on the usefulness of SMTs for the task at hand. You can find his rating at the end of each section, ranging from 📎 (poor fit) to 📎📎📎📎📎 (perfect fit).\n Format Conversions Probably the most common application of SMTs is format conversion, i.e. adjustments to type, format and representation of data. This may apply to entire messages, or to specific message attributes. Let\u0026#8217;s first look at a few examples for converting individual message attribute formats:\n   Timestamps: Different systems tend to have different assumptions of how timestamps should be typed and formatted. Debezium for instance represents most temporal column types as milli-seconds since epoch. Change event consumers on the other hand might expect such date and time values using Kafka Connect\u0026#8217;s Date type, or as an ISO-8601 formatted string, potentially using a specific timezone\n  Value masking: Sensitive data might have be to masked or truncated, or specific fields should even be removed altogether; the org.apache.kafka.connect.transforms.MaskField and ReplaceField SMTs shipping with Kafka Connect out of the box come in handy for that\n  Numeric types: Similar to timestamps, requirements around the representation of (decimal) numbers may differ between systems; e.g. Kafka Connect\u0026#8217;s Decimal type allows to convey arbitrary-precision decimals, but its binary representation of numbers might not be supported by all sink connectors and consumers\n  Name adjustments: Depending on the chosen serialization formats, specific field names might be unsupported; when working with Apache Avro for instance, field names must not start with a number\n   In all these cases, either existing, ready-made SMTs or bespoke implementations can be used to apply the required attribute type and/or format conversions.\n When using Kafka Connect for integrating legacy services and databases with newly built microservices, such format conversions can play an important role for creating an anti-corruption layer: by using better field names, choosing more suitable data types or by removing unneeded fields, SMTs can help to shield a new service\u0026#8217;s model from the oddities and quirks of the legacy world.\n But SMTs cannot only modify the representation of single fields, also the format and structure of entire messages can be adjusted. E.g. Kafka Connect\u0026#8217;s ExtractField transformation allows to extract a single field from a message and propagate that one. A related SMT is Debezium\u0026#8217;s SMT for change event flattening. It can be used to convert the complex Debezium change event structure with old and new row state, metadata and more, into a flat row representation, which can be consumed by many existing sink connectors.\n SMTs also allow to fine-tune schema namespaces; that can be of interest when working with a schema registry for managing schemas and their versions, and specific schema namespaces should be enforced for the messages on given topics. Two more, very useful examples of SMTs in this category are kafka-connect-transform-xml and kafka-connect-json-schema by Jeremy Custenborder, which will take XML or text and produce a typed Kafka Connect Struct, based on a given XML schema or JSON schema, respectively.\n Lastly, as a special kind of format conversion, SMTs can be used to modify or set the key of Kafka records. This may be desirable if a source connector doesn\u0026#8217;t produce any meaningful key, but one can be extracted from the record value. Also changing the message key can be useful, when considering subsequent stream processing. Choosing matching keys right at the source side e.g. allows for joining multiple topics via Kafka Streams, without the need for re-keying records.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎📎\u0026nbsp;\u0026nbsp; SMTs are the perfect tool for format conversions of Kafka Connect records\n   Ensuring Backwards Compatibility Changes to the schema of Kafka records can potentially be disruptive for consumers. If for instance a record field gets renamed, a consumer must be adapted accordingly, reading the value using the new field name. In case a field gets dropped altogether, consumers must not expect this field any longer.\n Message transformations can help with such transition from one schema version to the next, thus reducing the coupling of the lifecycles of message producers and consumers. In case of a renamed field, an SMT could add the field another time, using the original name. That\u0026#8217;ll allow consumers to continue reading the field using the old name and to be upgraded to use the new name at their own pace. After some time, once all consumers have been adjusted, the SMT can be removed again, only exposing the new field name going forward. Similarly, a field that got removed from a message schema could be re-added, e.g. using some sort of constant placeholder value. In other cases it might be possible to derive the field value from other, still existing fields. Again consumers could then be updated at their own pace to not expect and access that field any longer.\n It should be said though that there are limits for this usage: e.g. when changing the type of a field, things quickly become tricky. One option could be a multi-step approach where at first a separate field with the new type is added, before renaming it again as described above.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎\u0026nbsp;\u0026nbsp; SMTs can primarily help to address basic compatibility concerns around schema evolution\n   Filtering and Routing When applied on the source side, SMTs allow to filter out specific records produced by the connector. They also can be used for controlling the Kafka topic a record gets sent to. That\u0026#8217;s in particular interesting when filtering and routing is based on the actual record contents. In an IoT scenario for instance where Kafka Connect is used to ingest data from some kind of sensors, an SMT might be used to filter out all sensor measurements below a certain threshold, or route measurement events above a threshold to a special topic.\n Debezium provides a range of SMTs for record filtering and routing:\n   The logical topic routing SMT allows to send change events originating from multiple tables to the same Kafka topic, which can be useful when working with partition tables in Postgres, or with data that is sharded into multiple tables\n  The Filter and ContentBasedRouter SMTs let you use script expressions in languages such as Groovy or JavaScript for filtering and routing change events based on their contents; such script-based approach can be an interesting middleground between ease-of-use (no Java code must be compiled and deployed to Kafka Connect) and expressiveness; e.g. here is how the routing SMT could be used with GraalVM\u0026#8217;s JavaScript engine for routing change events from a table with purchase orders to different topics in Kafka, based on the order type:\n... transforms=route transforms.route.type=io.debezium.transforms.ContentBasedRouter transforms.route.topic.regex=.*purchaseorders transforms.route.language=jsr223.graal.js transforms.route.topic.expression= value.after.ordertype == 'B2B' ? 'b2b_orders' : 'b2c_orders' ...     The outbox event router comes in handy when implementing the transactional outbox pattern for data propagation between microservices: it can be used to send events originating from a single outbox table to a specific Kafka topic per aggregate (when thinking of domain driven design) or event type\n   There are also two SMTs for routing purposes in Kafka Connect itself: RegexRouter which allows to re-route records two different topics based on regular expressions, and TimestampRouter for determining topic names based on the record\u0026#8217;s timestamp.\n While routing SMTs usually are applied to source connectors (defining the Kafka topic a record gets sent to), it can also make sense to use them with sink connectors. That\u0026#8217;s the case when a sink connector derives the name of downstream table names, index names or similar from the topic name.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎📎\u0026nbsp;\u0026nbsp; Message filtering and topic routing\u0026#8201;\u0026#8212;\u0026#8201;no problem for SMTs\n   Tombstone Handling Tombstone records are Kafka records with a null value. They carry special semantics when working with compacted topics: during log compaction, all records with the same key as a tombstone record will be removed from the topic.\n Tombstones will be retained on a topic for a configurable time before compaction happens (controlled via delete.retention.ms topic setting), which means that also Kafka Connect sink connectors need to handle them. Unfortunately though, not all connectors are prepared for records with a null value, typically resulting in NullPointerExceptions and similar. A filtering SMT such as the one above can be used to drop tombstone records in such case.\n But also the exact opposite\u0026#8201;\u0026#8212;\u0026#8201;producing tombstone records\u0026#8201;\u0026#8212;\u0026#8201;can be useful: some sink connectors use tombstone records as the indicator to delete corresponding rows from a downstream datastore. Now when using a CDC connector like Debezium to capture changes from a database where \"soft deletes\" are used (i.e. records are not physically deleted, but a logically deleted flag is set to true when deleting a record), those change events will be exported as update events (which they technically are). A bespoke SMT can be used to translate these update events into tombstone records, triggering the deletion of corresponding records in downstream datastores.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎\u0026nbsp;\u0026nbsp; SMTs work well to discard tombstones or convert soft delete events into tombstones. What\u0026#8217;s not possible though is to keep the original event and produce an additional tombstone record at the same time\n   Externalizing Large Payloads Even some advanced enterprise application patterns can be implemented with the help of SMTs, one example being the claim check pattern. This pattern comes in handy in situations like this:\n  A message may contain a set of data items that may be needed later in the message flow, but that are not necessary for all intermediate processing steps. We may not want to carry all this information through each processing step because it may cause performance degradation and makes debugging harder because we carry so much extra data.\n \u0026#8201;\u0026#8212;\u0026#8201;Gregor Hohpe, Bobby Woolf; Enterprise Application Patterns\n   A specific example could again be a CDC connector that captures changes from a database table Users, with a BLOB column that contains the user\u0026#8217;s profile picture (surely not a best practice, still not that uncommon in reality\u0026#8230;\u0026#8203;).\n     Apache Kafka and Large Messages Apache Kafka isn\u0026#8217;t meant for large messages. The maximum message size is 1 MB by default, and while this can be increased, benchmarks are showing best throughput for much smaller messages. Strategies like chunking and externalizing large payloads can thus be vital in order to ensure a satisfying performance.\n     When propagating change data events from that table to Apache Kafka, adding the picture data to each event poses a significant overhead. In particular, if the picture BLOB hasn\u0026#8217;t changed between two events at all.\n Using an SMT, the BLOB data could be externalized to some other storage. On the source side, the SMT could extract the image data from the original record and e.g. write it to a network file system or an Amazon S3 bucket. The corresponding field in the record would be updated so it just contains the unique address of the externalised payload, such as the S3 bucket name and file path:\n   As an optimization, it could be avoided to re-upload unchanged file contents another time by comparing earlier and current hash of the externalized file.\n A corresponding SMT instance applied to sink connectors would retrieve the identifier of the externalized files from the incoming record, obtain the contents from the external storage and put it back into the record before passing it on to the connector.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎\u0026nbsp;\u0026nbsp; SMTs can help to externalize payloads, avoiding large Kafka records. Relying on another service increases overall complexity, though\n   Limitations As we\u0026#8217;ve seen, single message transformations can help to address quite a few requirements that commonly come up for users of Kafka Connect. But there are limitations, too; Like MacGyver, who sometimes has to reach for some other tool than his beloved Swiss Army knife, you shouldn\u0026#8217;t think of SMTs as the perfect solution all the time.\n The biggest shortcoming is already hinted at in their name: SMTs only can be used to process single records, one at a time. E.g. you cannot split up a record into multiple ones using an SMT, as they only can return (at most) one record. Also any kind of stateful processing, like aggregating data from multiple records, or correlating records from several topics is off limits for SMTs. For such use cases, you should be looking at stream processing technologies like Kafka Streams and Apache Flink; also integration technologies like Apache Camel can be of great use here.\n One thing to be aware of when working with SMTs is configuration complexity; when using generic, highly configurable SMTs, you might end up with lengthy configuration that\u0026#8217;s hard to grasp and debug. You might be better off implementing a bespoke SMT which is focussing on one particular task, leveraging the full capabilities of the Java programming language.\n     SMT Testing Whether you use ready-made SMTs by means of configuration, or you implement custom SMTs in Java, testing your work is essential.\n While unit tests are a viable option for basic testing of bespoke SMT implementations, integration tests running against Kafka Connect connectors are recommended for testing SMT configurations. That way you\u0026#8217;ll be sure that the SMT can process actual messages and it has been configured the way you intended to.\n Testcontainers and the Debezium support for Testcontainers are a great foundation for setting up all the required components such as Apache Kafka, Kafka Connect, connectors and the SMTs to test.\n     A specific feature I wished for every now and then is the ability to apply SMTs only to a specific sub-set of the topics created or consumed by a connector. In particular if connectors create different kinds of topics (like an actual data topic and another one with with metadata), it can be desirable to apply SMTs only to the topics of one group but not the other. This requirement is captured in KIP-585 (\"Filter and Conditional SMTs\"), please join the discussion on that one if you got requirements or feedback related to that.\n   Learning More There are several great presentations and blog posts out there which describe in depth what SMTs are, how you can implement your own one, how they are configured etc.\n Here are a few resources I found particularly helpful:\n   KIP-66: The original KIP (Kafka Improvement Proposal) that introduced SMTs\n  Singe Message Transforms are not the Transformations You\u0026#8217;re Looking For: A great overview on SMTs, their capabilities as well as limitations, by Ewen Cheslack-Postava\n  A hands-on experience with Kafka Connect SMTs: In-depth blog post on SMT use cases, things to be aware of and more, by Gian D\u0026#8217;Uia\n   Now, considering this wide range of use cases for SMTs, would MacGyver like and use them for implementing various tasks around Kafka Connect? I would certainly think so. But as always, the right tool for the job must be chosen: sometimes an SMT may be a great fit, another time a more flexible (and complex) stream processing solution might be preferable.\n Just as MacGyver, you got to make a call when to use your Swiss Army knife, duct tape or a paper clip.\n Many thanks to Hans-Peter Grahsl for his feedback while writing this blog post!\n  ","id":7,"publicationdate":"May 14, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eDo you remember Angus \"Mac\" MacGyver?\nThe always creative protagonist of the popular 80ies/90ies TV show, who could solve about any problem with nothing more than a Swiss Army knife, duct tape, shoe strings and a paper clip?\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe single message transformations (SMTs) of Kafka Connect are almost as versatile as MacGyver\u0026#8217;s Swiss Army knife:\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"ulist\"\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eHow to change the timezone or format of date/time message fields?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow to change the topic a specific message gets sent to?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow to filter out specific records?\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eSMTs can be the answer to these and many other questions that come up in the context of Kafka Connect.\nApplied to source or sink connectors,\nSMTs allow to modify Kafka records before they are sent to Kafka, or after they are consumed from a topic, respectively.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Single Message Transformations - The Swiss Army Knife of Kafka Connect","uri":"https://www.morling.dev/blog/single-message-transforms-swiss-army-knife-of-kafka-connect/"},{"content":"For libraries and frameworks it\u0026#8217;s a common requirement to make specific aspects customizeable via service provider interfaces (SPIs): contracts to be implemented by the application developer, which then are invoked by framework code, adding new or replacing existing functionality.\n Often times, the method implementations of such an SPI need to return value(s) to the framework. An alternative to return values are \"emitter parameters\": passed by the framework to the SPI method, they offer an API for receiving value(s) via method calls. Certainly not revolutionary or even a new idea, I find myself using emitter parameters more and more in libraries and frameworks I work on. Hence I\u0026#8217;d like to discuss some advantages I perceive about the emitter parameter pattern.\n An Example As an example, let\u0026#8217;s consider a blogging platform which provides an SPI for extracting categories and tags from given blog posts. Application developers can plug in custom implementations of that SPI, e.g. based on the latest and greatest algorithms in information retrieval and machine learning. Here\u0026#8217;s how a basic SPI contract for this use case could look like, using regular method return values:\n 1 2 3 4 5 public interface BlogPostDataExtractor { Set\u0026lt;String\u0026gt; extractCategories(String contents); Set\u0026lt;String\u0026gt; extractTags(String contents); }    This probably would get the job done, but there are a few problems: any implementation will have to do two passes on the given blog post contents, once in each method\u0026#8201;\u0026#8212;\u0026#8201;not ideal. Also let\u0026#8217;s assume that most blog posts only belong to exactly one category. Implementations still would have to allocate a set for the single returned category.\n While there\u0026#8217;s not much we can do about the second issue with a return value based design, the former problem could be addressed by combining the two methods:\n 1 2 3 4 public interface BlogPostDataExtractor { CategoriesAndTags extractCategoriesAndTags(String contents); }    Now an implementation can retrieve both categories and tags at once. But it\u0026#8217;s worth thinking about how an SPI implementation would instantiate the return type.\n Exposing a concrete class to be instantiated by implementors poses a challenge for future evolution of the SPI: following the best practice and making the return object type immutable, all its properties must be passed to its constructor. Now if an additional attribute should be extracted from blog posts, such as a teaser, the existing constructor cannot be modified, so to not break existing user code. Instead, we\u0026#8217;d have to introduce new constructors whenever adding further attributes. Dealing with all these constructors could become quite inconvenient, in particular if a specific SPI implementation is only interested in producing some of the attributes.\n All in all, for SPIs it\u0026#8217;s often a good idea to only expose interfaces, but no concrete classes. So we could make the return type an interface and leave it to SPI implementors to create an implementation class, but that\u0026#8217;d be rather tedious.\n   The Emitter Parameter Pattern Or, we could provide some sort of builder object which can be used to construct CategoriesAndTags objects. But then why even return an object at all, instead of simply mutating the state of a builder that is provided through a method parameter? And that\u0026#8217;s essentially what the emitter parameter pattern is about: passing in an object which can be used to emit the values which should be \"returned\" by the method.\n     I\u0026#8217;m not aware of any specific name for this pattern, so I came up with \"emitter parameter pattern\" (the notion of callback parameters is related, yet different). And hey, perhaps I\u0026#8217;ll become famous for coining a design pattern name ;) Please let me know in the comments below if you know this pattern under a different name.\n     Here\u0026#8217;s how the extractor SPI could look like when designed with an emitter parameter:\n 1 2 3 4 5 6 7 8 9 10 public interface BlogPostDataExtractor { void extractData(String contents, BlogPostDataReceiver data); (1) interface BlogPostDataReceiver { (2) void addCategory(String category); void addTag(String tag); } }      1 SPI method with input parameter and emitter parameter   2 Emitter parameter type    An implementation would emit the retrieved information by invoking the methods on the data parameter:\n 1 2 3 4 5 6 7 8 9 10 public class MyBlogPostDataExtractor implements BlogPostDataExtractor { public void extractData(String contents, BlogPostDataReceiver data) { String category = ...; Stream\u0026lt;String\u0026gt; tags = ...; data.addCatgory(category); tags.forEach(data::addTag); } }    This approach nicely avoids all the issues with the return value based design:\n   Single and multiple value case handled uniformly: an implementation can call addCategory() just once, or multiple times; either way, it doesn\u0026#8217;t have to deal with the creation of a set, list, or other container for the produced value(s)\n  Flexible evolution of the SPI contract: new methods such as addTeaser(), or addTags(String\u0026#8230;\u0026#8203; tags) can be added to the emitter parameter type, avoiding the creation of more and more return type constructors; as the passed BlogPostDataReceiver instance is controlled by the framework itself, we also could add methods which provide more context required for the task at hand\n  No need for exposing concrete types on the SPI surface: as no return value needs to be instantiated by SPI implementations, the solution works solely with interfaces on the SPI surface; this provides more control to the framework, e.g. the emitter object could be re-used etc.\n  Flexible implementation choices: by not requiring SPI implementations to allocate any return objects, the platform gains a lot of flexibility for how it\u0026#8217;s processing the emitted values: while it could collect the values in a set or list, it also has the option to not allocate any intermediary collections, but process and pass on values one-by-one in a streaming-based way, without any of this impacting SPI implementors\n   Now, are there some downsides to this approach, too? I can see two: if a method only ever should yield a single value, the emitter API might be misleading. We could raise an exception though if an emitter method is called more than once. Also an implementation might hold on to the emitter object and invoke its methods after the call flow has returned from the SPI method, which typically isn\u0026#8217;t desirable. Again that\u0026#8217;s something that can be prevented by invalidating the emitter object after the SPI method returned, raising an exception in case of further method invocations.\n Overall, I think the emitter parameter pattern is a valuable tool in the box of library and framework authors; it provides flexibility for implementation choices and future evolution when designing SPIs. Real-world examples include the ValueExtractor SPI in Bean Validation 2.0 (where it was chosen to provide a uniform value of extracting single and multiple values from container objects) and the ChangeRecordEmitter contract in Debezium\u0026#8217;s SPI.\n Many thanks to Hans-Peter Grahsl and Nils Hartmann for reviewing an early version of this blog post.\n  ","id":8,"publicationdate":"May 4, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eFor libraries and frameworks it\u0026#8217;s a common requirement to make specific aspects customizeable via \u003ca href=\"https://en.wikipedia.org/wiki/Service_provider_interface\"\u003eservice provider interfaces\u003c/a\u003e (SPIs):\ncontracts to be implemented by the application developer, which then are invoked by framework code,\nadding new or replacing existing functionality.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eOften times, the method implementations of such an SPI need to return value(s) to the framework.\nAn alternative to return values are \"emitter parameters\":\npassed by the framework to the SPI method, they offer an \u003cem\u003eAPI\u003c/em\u003e for receiving value(s) via method calls.\nCertainly not revolutionary or even a new idea,\nI find myself using emitter parameters more and more in libraries and frameworks I work on.\nHence I\u0026#8217;d like to discuss some advantages I perceive about the emitter parameter pattern.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"The Emitter Parameter Pattern for Flexible SPI Contracts","uri":"https://www.morling.dev/blog/emitter-parameter-pattern-for-flexible-spis/"},{"content":"Making applications extensible with some form of plug-ins is a very common pattern in software design: based on well-defined APIs provided by the application core, plug-ins can customize an application\u0026#8217;s behavior and provide new functionality. Examples include desktop applications like IDEs or web browsers, build tools such as Apache Maven or Gradle, as well as server-side applications such as Apache Kafka Connect, a runtime for Kafka connectors plug-ins.\n In this post I\u0026#8217;m going to explore how the Java Platform Module System's notion of module layers can be leveraged for implementing plug-in architectures on the JVM. We\u0026#8217;ll also discuss how Layrry, a launcher and runtime for layered Java applications, can help with this task.\n A key requirement for any plug-in architecture is strong isolation between different plug-ins: their state, classes and dependencies should be encapsulated and independent of each other. E.g. package declarations in two plug-ins should not collide, also they should be able to use different versions of another 3rd party dependency. This is why the default module path of Java (specified using the --module-path option) is not enough for this purpose: it doesn\u0026#8217;t support more than one version of a given module.\n The module system\u0026#8217;s answer are module layers: by organizing an application and its plug-ins into multiple layers, the required isolation between plug-ins can be achieved.\n     With the module system, each Java application always contains at least one layer, the boot layer. It contains the platform modules and the modules provided on the module path.\n     An Example: The Greeter CLI App To make things more tangible, let\u0026#8217;s consider a specific example; The \"Greeter\" app is a little CLI utility, that can produce greetings in different languages.\n In order to not limit the number of supported languages, it provides a plug-in API, which allows to add additional greeting implementations, without the need to rebuild the core application. Here is the Greeter contract, which is to be implemented by each language plug-in:\n 1 2 3 4 5 package com.example.greeter.api; public interface Greeter { String greet(String name); }    Greeters are instantiated via accompanying implementations of GreeterFactory:\n 1 2 3 4 5 public interface GreeterFactory { String getLanguage(); (1) String getFlag(); Greeter getGreeter(); (2) }      1 The getLanguage() and getFlag() methods are used to show a description of all available greeters in the CLI application   2 The getGreeter() method returns a new instance of the corresponding Greeter type    Here\u0026#8217;s the overall architecture of the Greeter application, with three different language implementations:\n   The application is made up of five different layers:\n   greeter-platform: contains the Greeter and GreeterFactory contracts\n  greeter-en, greeter-de and greeter-fr: greeter implementations for different languages; note how each one is depending on a different version of some greeter-date module. As they are isolated in different layers, they can co-exist within the application\n  greeter-app: the \"shell\" of the application which loads all the greeter implementations and makes them accessible as a simple CLI application\n   Now let\u0026#8217;s see how this application structure can be assembled using Layrry.\n   Application Plug-ins With Layrry In a previous blog post we\u0026#8217;ve explored how applications can be cut into layers, described in Layrry\u0026#8217;s layers.yml configuration file. A simple static layer definition would defeat the purpose of a plug-in architecture, though: not all possible plug-ins are known when assembling the application.\n Layrry addresses this requirement by allowing to source different layers from directories on the file system:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 layers: platform: (1) modules: - \"com.example.greeter:greeter-api:1.0.0\" plugins: (2) parents: - \"api\" directory: path/to/plugins app: (3) parents: - \"plugins\" modules: - \"com.example.greeter:greeter-app:1.0.0\" main: module: com.example.greeter.app class: com.example.greeter.app.App      1 The platform layer with the API module   2 The plug-in layer(s)   3 The application layer with the \"application shell\"    Whereas the platform and app layers are statically defined, using the Maven GAV coordinates of the modules to include, the plugins part of the configuration describes an open-ended set of layers. Each sub-directory of the given directory represents its own layer. All modules within this sub-directory will be added to the layer, and the API layer will be the parent of each of the plug-in layers. The app layer has all the plug-in layers as its ancestors, allowing it to retrieve plug-in implementations from these layers.\n More greeter plug-ins can be added to the application by simply creating a sub-directory with the required module(s).\n   Finding Plug-in Implementations With the Java Service Loader Structuring the application into different layers isn\u0026#8217;t all we need for building a plug-in architecture; we also need a way for detecting and loading the actual plug-in implementations. The service loader mechanism of the Java platform comes in handy for that. If you have never worked with the service loader API, it\u0026#8217;s definitely recommended to study its extensive JavaDoc description:\n  A service is a well-known interface or class for which zero, one, or many service providers exist. A service provider (or just provider) is a class that implements or subclasses the well-known interface or class. A ServiceLoader is an object that locates and loads service providers deployed in the run time environment at a time of an application\u0026#8217;s choosing.   Having been a supported feature of Java since version 6, the service loader API has been been reworked and refined to work within modular environments when the Java Module System was introduced in JDK 9.\n In order to retrieve service implementations via the service loader, a consuming module must declare the use of the service in its module descriptor. For our purposes, the GreeterFactory contract is a perfect examplification of the service idea. Here\u0026#8217;s the descriptor of the Greeter application\u0026#8217;s app module, declaring its usage of this service:\n 1 2 3 4 5 module com.example.greeter.app { exports com.example.greeter.app; requires com.example.greeter.api; uses com.example.greeter.api.GreeterFactory; }    The module descriptor of each greeter plug-in must declare the service implementation(s) which it provides. E.g. here is the module descriptor of the English greeter implementation:\n 1 2 3 4 5 6 module com.example.greeter.en { requires com.example.greeter.api; requires com.example.greeter.dateutil; provides com.example.greeter.api.GreeterFactory with com.example.greeter.en.EnglishGreeterFactory; }    From within the app module, the service implementations can be retrieved via the java.util.ServiceLoader class.\n When using the service loader in layered applications, there\u0026#8217;s one potential pitfall though, which mostly will affect existing applications which are migrated: in order to access service implementations located in a different layer (specifically, in an ancestor layer of the loading layer), the method load(ModuleLayer, Class\u0026lt;?\u0026gt;) must be used. When using other overloaded variants of load(), e.g. the commonly used load(Class\u0026lt;?\u0026gt;), those implementations won\u0026#8217;t be found.\n Hence the code for loading the greeter implementations from within the app layer could look like this:\n 1 2 3 4 5 6 7 8 9 10 private static List\u0026lt;GreeterFactory\u0026gt; getGreeterFactories() { ModuleLayer appLayer = App.class.getModule().getLayer(); return ServiceLoader.load(appLayer, GreeterFactory.class) .stream() .map(p -\u0026gt; p.get()) .sorted((gf1, gf2) -\u0026gt; gf1.getLanguage().compareTo( gf2.getLanguage())) .collect(Collectors.toList()); }    Having loaded the list of greeter factories, it doesn\u0026#8217;t take too much code to display a list with all available implementations, expect a choice by the user and invoke the greeter for the chosen language. This code which isn\u0026#8217;t too interesting is omitted here for the sake of brevity and can be found in the accompanying example source code repo.\n     JDK 9 brought some more nice improvements for the service loader API. E.g. the type of service implementations can be examined without actually instantiating them. This allows for interesting alternatives for providing service meta-data and choosing an implementation based on some criteria. For instance, greeter metadata like the language name and flag could be given using an annotation:\n 1 2 3 4 @GreeterDefinition(lang=\"English\", flag=\"🇬🇧\") public class EnglishGreeterFactory implements GreeterFactory { Greeter getGreeter(); }    Then the method ServiceLoader.Provider#type() can be used to obtain the annotation and return a greeter factory for a given language:\n 1 2 3 4 5 6 7 8 9 10 11 private Optional\u0026lt;GreeterFactory\u0026gt; getGreeterFactoryForLanguage( String language) { ModuleLayer layer = App.class.getModule().getLayer(); return ServiceLoader.load(layer, GreeterFactory.class) .stream() .filter(gf -\u0026gt; gf.type().getAnnotation( GreeterDefinition.class).lang().equals(language)) .map(gf -\u0026gt; gf.get()) .findFirst(); }          Seeing it in Action Lastly, let\u0026#8217;s take a look at the complete Greeter application in action. Here it is, initially with two, and then with three greeter implementations:\n   The layers configuration file is adjusted to load greeter plug-ins from the plugins directory; initially, two greeters for English and French exist. Then the German greeter implementation gets picked up by the application after adding it to the plug-in directory, without requiring any changes to the application tiself.\n The complete source code, including the logic for displaying all the available greeters and prompting for input, is available in the Layrry repository on GitHub.\n And there you have it, a basic plug-in architecture using Layrry and the Java Module System. Going forward, this might evolve in a few ways. E.g. it might be desirable to detect additional plug-ins without having to restart the application, e.g. when thinking of desktop application use cases. While loading additional plug-ins in new layers should be comparatively easy, unloading already loaded layers, e.g. when updating a plug-in to a newer version, could potentially be quite tricky. In particular, there\u0026#8217;s no way to actively unload layers, so we\u0026#8217;d have to rely on the garbage collector to clean up unused layers, making sure no references to any of their classes are kept in other, active layers.\n One also could think of an event bus, allowing different plug-ins to communicate in a safe, yet loosely coupled way. What requirements would you have for plug-in centered applications running on the Java Module System? Let\u0026#8217;s exchange in the comments below!\n  ","id":9,"publicationdate":"Apr 21, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eMaking applications extensible with some form of plug-ins is a very common pattern in software design:\nbased on well-defined APIs provided by the application core, plug-ins can customize an application\u0026#8217;s behavior and provide new functionality.\nExamples include desktop applications like IDEs or web browsers, build tools such as Apache Maven or Gradle, as well as server-side applications such as Apache Kafka Connect,\na runtime for Kafka connectors plug-ins.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIn this post I\u0026#8217;m going to explore how the \u003ca href=\"https://www.jcp.org/en/jsr/detail?id=376\"\u003eJava Platform Module System\u003c/a\u003e's notion of module layers can be leveraged for implementing plug-in architectures on the JVM.\nWe\u0026#8217;ll also discuss how \u003ca href=\"https://github.com/moditect/layrry\"\u003eLayrry\u003c/a\u003e, a launcher and runtime for layered Java applications, can help with this task.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Plug-in Architectures With Layrry and the Java Module System","uri":"https://www.morling.dev/blog/plugin-architectures-with-layrry-and-the-java-module-system/"},{"content":"One of the biggest changes in recent Java versions has been the introduction of the module system in Java 9. It allows to organize Java applications and their dependencies in strongly encapsulated modules, utilizing explicit and well-defined module APIs and relationships.\n In this post I\u0026#8217;m going to introduce the Layrry open-source project, a launcher and Java API for executing modularized Java applications. Layrry helps Java developers to assemble modularized applications from dependencies using their Maven coordinates and execute them using module layers. Layers go beyond the capabilities of the \"flat\" module path specified via the --module-path parameter of the java command, e.g. allowing to use multiple versions of one module within one and the same application.\n Why Layrry? The Java Module System doesn\u0026#8217;t define any means of mapping between modules (e.g. com.acme.crm) and JARs providing such module (e.g. acme-crm-1.0.0.Final.jar), or retrieving modules from remote repositories using unique identifiers (e.g. com.acme:acme-crm:1.0.0.Final). Instead, it\u0026#8217;s the responsibility of the user to obtain all required JARs of a modularized application and provide them via the --module-path parameter.\n Furthermore, the module system doesn\u0026#8217;t define any means of module versioning; i.e. it\u0026#8217;s the responsibility of the user to obtain all modules in the right version. Using the --module-path option, it\u0026#8217;s not possible, though, to assemble an application that uses multiple versions of one and the same module. This may be desirable for transitive dependencies of an application, which might be required in different versions by two separate direct dependencies.\n This is where Layrry comes in (pronounced \"Larry\"): it provides a declarative approach as well as an API for assembling modularized applications. The (modular) JARs to be included are described using Maven GAV (group id, artifact id, version) coordinates, solving the issue of retrieving all required JARs from a remote repository, in the right version.\n With Layrry, applications are organized in module layers, which allows to use different versions of one and the same module in different layers of an application (as long as they are not exposed in a conflicting way on module API boundaries).\n   An Example As an example, let\u0026#8217;s consider an application made up of the following modules:\n   The application\u0026#8217;s main module, com.example:app, depends on two others, com.example:foo and com.example:bar. They in turn depend on the Log4j API and another module, com.example:greeter. The latter is used in two different versions, though.\n Let\u0026#8217;s take a closer look at the Greeter class in these modules. Here is the version in com.example:greeter@1.0.0, as used by com.example:foo:\n 1 2 3 4 5 6 public class Greeter { public String greet(String name, String from) { return \"Hello, \" + name + \" from \" + from + \" (Greeter 1.0.0)\"; } }    And this is how it looks in com.example:greeter@2.0.0, as used by com.example:bar:\n 1 2 3 4 5 6 7 8 9 10 11 public class Greeter { public String hello(String name, String from) { return \"Hello, \" + name + \" from \" + from + \" (Greeter 2.0.0)\"; } public String goodBye(String name, String from) { return \"Good bye, \" + name + \" from \" + from + \" (Greeter 2.0.0)\"; } }    The Greeter API has evolved in a backwards-incompatible way, i.e. it\u0026#8217;s not possible for the foo and bar modules to use the same version.\n With a \"flat\" module path (or classpath), there\u0026#8217;s no way for dealing with this situation. You\u0026#8217;d inevitably end up with a NoSuchMethodError, as either foo or bar would be linked at runtime against a version of the class different from the version it has been compiled against.\n The lack of support for using multiple module versions when working with the --module-path option might be surprising at first, but it\u0026#8217;s an explicit non-requirement of the module system to support multiple module versions or even deal with selecting matching module versions at all.\n This means that the module descriptors of both foo and bar require the greeter module without any version information:\n 1 2 3 4 5 module com.example.foo { exports com.example.foo; requires org.apache.logging.log4j; requires com.example.greeter; }    1 2 3 4 5 module com.example.bar { exports com.example.bar; requires org.apache.logging.log4j; requires com.example.greeter; }      Module Layers to the Rescue While only one version of a given module is supported when running applications via java --module-path=\u0026#8230;\u0026#8203;, there\u0026#8217;s a lesser known feature of the module system which provides a way out: module layers.\n A module layer \"is created from a graph of modules in a Configuration and a function that maps each module to a ClassLoader.\" Using the module layer API, multiple versions of a module can be loaded in different layers, thus using different classloaders.\n Note the layers API doesn\u0026#8217;t concern itself with obtaining JARs or modules from remote locations such as the Maven Central repository; instead, any modules must be provided as Path objects. Here is how a layer with the foo and greeter:1.0.0 modules could be assembled:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ModuleLayer boot = ModuleLayer.boot(); ClassLoader scl = ClassLoader.getSystemClassLoader(); Path foo = Paths.get(\"path/to/foo-1.0.0.jar\"); (1) Path greeter10 = Paths.get(\"path/to/greeter-1.0.0.jar\"); (2) ModuleFinder fooFinder = ModuleFinder.of(foo, greeter10); Configuration fooConfig = boot.configuration() (3) .resolve( fooFinder, ModuleFinder.of(), Set.of(\"com.example.foo\", \"com.example.greeter\") ); ModuleLayer fooLayer = boot.defineModulesWithOneLoader( fooConfig, scl); (4)      1 obtain foo-1.0.0.jar   2 obtain greeter-1.0.0.jar   3 Create a configuration derived from the \"boot\" module of the JVM, providing a ModuleFinder for the two JARs obtained before, and resolving the two modules   4 Create a module layer using the configuration, loading all contained modules with a single classloader    Similarly, you could create a layer for bar and greeter:2.0.0, as well as layers for log4j and the main application module. The layers API is very flexible, e.g. you could load each module in its own classloader and more. But all this flexibility can make using the API direcly a daunting task.\n Also using an API might not be what you want in the first place: wouldn\u0026#8217;t it be nice if there was a CLI tool, akin to using java --module-path=\u0026#8230;\u0026#8203;, but with the additional powers of module layers?\n   The Layrry Launcher This is where Layrry comes in: it is a CLI tool which takes a configuration of a layered application (defined in a YAML file) and executes it. The layer descriptor for the example above looks like so:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 layers: log: (1) modules: (2) - \"org.apache.logging.log4j:log4j-api:jar:2.13.1\" - \"org.apache.logging.log4j:log4j-core:jar:2.13.1\" - \"com.example:logconfig:1.0.0\" foo: parents: (3) - \"log\" modules: - \"com.example:greeter:1.0.0\" - \"com.example:foo:1.0.0\" bar: parents: - \"log\" modules: - \"com.example:greeter:2.0.0\" - \"com.example:bar:1.0.0\" app: parents: - \"foo\" - \"bar\" modules: - \"com.example:app:1.0.0\" main: (4) module: com.example.app class: com.example.app.App      1 Each layer has a unique name   2 The modules element lists all the modules contained in the layer, using Maven coordinates (group id, artifact id, version), unambigously referencing a (modular) JAR in a specific version   3 A layer can have one or more parent layers, whose modules it can access; if no parent is given, the JVM\u0026#8217;s \"boot\" layer is the implicit parent of a layer   4 The given main module and class is the one that will be executed by Layrry    The configuration above describes four layers, log, foo, bar and app, with the modules they contain and the parent/child relationships between these layers. Note how the versions 1.0.0 and 2.0.0 of the greeter module are used in foo and bar. The file also specifies the main class to execute when running this application.\n Using Layrry, a modular application is executed like this:\n 1 2 3 4 5 6 7 java -jar layrry-1.0-SNAPSHOT-jar-with-dependencies.jar \\ --layers-config layers.yml \\ Alice 20:58:01.451 [main] INFO com.example.foo.Foo - Hello, Alice from Foo (Greeter 1.0.0) 20:58:01.472 [main] INFO com.example.bar.Bar - Hello, Alice from Bar (Greeter 2.0.0) 20:58:01.473 [main] INFO com.example.bar.Bar - Good bye, Alice from Bar (Greeter 2.0.0)    The log messages show how the two versions of greeter are used by foo and bar, respectively. Layrry will download all referenced JARs using the Maven resolver API, i.e. you don\u0026#8217;t have to deal with manually obtaining all the JARs and providing them to the java runtime.\n   Using the Layrry API In addition to the YAML-based launcher, Layrry provides also a Java API for assembling and running layered applications. This can be used in cases where the structure of layers is only known at runtime, or for implementing plug-in architectures.\n In order to use Layrry programmatically, add the following dependency to your pom.xml:\n 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.moditect.layrry\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;layrry\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;    Then, the Layrry Java API can be used like this (showing the same example as above):\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Layers layers = Layers.layer(\"log\") .withModule(\"org.apache.logging.log4j:log4j-api:jar:2.13.1\") .withModule(\"org.apache.logging.log4j:log4j-core:jar:2.13.1\") .withModule(\"com.example:logconfig:1.0.0\") .layer(\"foo\") .withParent(\"log\") .withModule(\"com.example:greeter:1.0.0\") .withModule(\"com.example:foo:1.0.0\") .layer(\"bar\") .withParent(\"log\") .withModule(\"com.example:greeter:2.0.0\") .withModule(\"com.example:bar:1.0.0\") .layer(\"app\") .withParent(\"foo\") .withParent(\"bar\") .withModule(\"com.example:app:1.0.0\") .build(); layers.run(\"com.example.app/com.example.app.App\", \"Alice\");      Next Steps The Layrry project is still in its infancy. Nevertheless it can be a useful tool for application developers wishing to leverage the Java Module System. Obtaining modular JARs via Maven coordinates and providing an easy-to-use mechanism for organizing modules in layers enables usages which cannot be addressed using the plain java --module-path \u0026#8230;\u0026#8203; approach.\n Layrry is open-source (under the Apache License version 2.0). The source code is hosted on GitHub, and your contributions are very welcomed.\n Please let me know about your ideas and requirements in the comments below or by opening up issues on GitHub. Planned enhancements include support for creating modular runtime images (jlink) based on the modules referenced in a layers.yml file, and visualization of module layers and their modules via GraphViz.\n  ","id":10,"publicationdate":"Mar 29, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eOne of the biggest changes in recent Java versions has been the introduction of the \u003ca href=\"http://openjdk.java.net/projects/jigsaw/spec/\"\u003emodule system\u003c/a\u003e in Java 9.\nIt allows to organize Java applications and their dependencies in strongly encapsulated modules, utilizing explicit and well-defined module APIs and relationships.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIn this post I\u0026#8217;m going to introduce the \u003ca href=\"https://github.com/moditect/layrry\"\u003eLayrry\u003c/a\u003e open-source project, a launcher and Java API for executing modularized Java applications.\nLayrry helps Java developers to assemble modularized applications from dependencies using their Maven coordinates and execute them using module layers.\nLayers go beyond the capabilities of the \"flat\" module path specified via the \u003cem\u003e--module-path\u003c/em\u003e parameter of the \u003cem\u003ejava\u003c/em\u003e command,\ne.g. allowing to use multiple versions of one module within one and the same application.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Introducing Layrry: A Launcher and API for Modularized Java Applications","uri":"https://www.morling.dev/blog/introducing-layrry-runner-and-api-for-modularized-java-applications/"},{"content":"Within Debezium, the project I\u0026#8217;m working on at Red Hat, we recently encountered an \"interesting\" situation where we had to resolve a rather difficult merge conflict. As others where interested in how we addressed the issue, and also for our own future reference, I\u0026#8217;m going to give a quick run down of the problem we encountered and how we solved it.\n The Problem Ideally, we\u0026#8217;d only ever work on a single branch and would never have to deal with porting changes between the master and other branches. Oftentimes we cannot get around this, though: specific versions of a software may have to be maintained for some time, requiring to backport bugfixes from the current development branch to the branch corresponding to the maintained version.\n In our specific case we had to deal with backporting changes to our project documentation. To complicate things, this documentation (written in AsciiDoc) has been largely re-organized between master and the targeted older branch, 1.0. What used to be one large AsciiDoc file for each of the Debezium connectors, got split up into multiple smaller files on master now. This split was meant to be applied to 1.0 too, but due to some miscommunication in the team (these things happen, right) this wasn\u0026#8217;t done, whereas an asorted set of documentation changes had been backported already to the larger, monolithic AsciiDoc files.\n So the situation we faced was this:\n   large, monolithic AsciiDoc files on the 1.0 branch\n  smaller, modularized AsciiDoc files on master\n  Documentation updates applied on master, of which only a subset is relevant for 1.0 (new features shouldn\u0026#8217;t be added to the Debezium 1.0 documentation)\n  Some of the documentation updates relevant for the 1.0 branch already had been backported from master, while others had not\n   All in all, a rather convoluted situation; the full diff of the documentation sub-directory between the two branches was about 13K lines.\n So what should we do? Cherry-picking individual commits from master was not really an option, as there were a few hundred commits on master since 1.0 had been forked off. Also many commits would contain documentation and code changes. The latter had already been backported successfully before.\n Realizing that resolving that merge conflict was next to impossible, the next idea was to essentially start from scratch and re-apply all relevant documentation changes to the 1.0 branch. Our initial idea was to create a patch with the difference of the documentation directory between the two branches. But editing that patch file with 13K lines turned out to be not manageable, either.\n   The Solution This is when we were reminded of the possibilities of git filter-branch: using this command it should be possible to isolate all the documentation changes done on master since Debezium 1.0 and apply the required sub-set of these changes to the 1.0 branch.\n To start with a clean slate, we created a new temporary branch based on 1.0:\n git checkout -b docs_backport 1.0   We then reset the contents of the documentation directory to its state as of the 1.0.0.Final release, as that\u0026#8217;s where the 1.0 and master branches diverged.\n rm -rf documentation git add documentation git checkout v1.0.0.Final documentation git commit -m \"Resetting documentation dir to v1.0.0.Final\" # This should yield no differences git diff v1.0.0.Final..docs_backport documentation   The next step was to filter all commits on master so to only keep any changes to the documentation directory. This was done on a new branch, docs_filtered. The --subdirectory-filter option comes in handy for that:\n git checkout -b docs_filtered master git filter-branch -f --prune-empty \\ --subdirectory-filter documentation \\ v1.0.0.Final..docs_filtered   This leaves us with a branch docs_filtered which only contains the commits since the v1.0.0.Final tag that modified the documentation directory.\n The --subdirectory-filter option also moves the contents of the given directory to the root of the repo, though. That\u0026#8217;s not exactly what we need. But another option, --tree-filter, lets us restore the original directory layout. It allows to run a set of commands against each of the filtered commits. We can use this to move the contents of documentation back to that directory:\n git filter-branch -f \\ --tree-filter 'mkdir -p documentation; \\ mv antora.yml documentation 1\u0026gt;/dev/null 2\u0026gt;/dev/null; \\ mv modules documentation 1\u0026gt;/dev/null 2\u0026gt;/dev/null;' \\ v1.0.0.Final..docs_filtered   Examining the history now, we can see that the commits on the docs_filtered apply the changes to the documentation directory, as expected.\n One problem still remains, though: by means of the --subdirectory-filter option, the very first commit removes all contents besides the documentation directory. This can be fixed by doing an interactive rebase of the current branch, beginning at the v1.0.0.Final tag:\n git rebase -i v1.0.0.Final   We need to edit the very first commit; all changes besides those to the documentation directory need to be reverted from that commit. There might be a better way of doing so, I simply ran git checkout for all the other resources:\n git checkout v1.0.0.Final debezium-connector-mongodb git checkout v1.0.0.Final debezium-connector-mysql ...   At this point the filtered branch still is based off of the v1.0.0.Final tag, whereas it should be based off of the docs_backport branch. git rebase --onto to the rescue:\n git rebase --onto docs_backport v1.0.0.Final docs_filtered   This rebases all the commits from the docs_filtered branch onto the docs_backport branch. Now we have a state where where all the documention changes have been cleanly applied to the 1.0 code base, i.e. the following should yield no differences:\n git diff docs_filtered..master documentation   The last and missing step is to do another rebase of all the documentation commits, discarding those that apply to any features that didn\u0026#8217;t get backported to 1.0.\n Thankfully, my partner-in-crime Jiri Pechanec stepped in here: as he had done the original feature backport, it didn\u0026#8217;t take him too long to go through the list of documentation commits and identify those which were relevant for the 1.0 code base. After one more interactive rebase for applying those we finally were in a state, where all the required documentation changes had been backported.\n Looking at the 1.0 history, you\u0026#8217;d still see some partial documentation changes up to the point, where we decided to start all over and revert these. Theoretically we could do another git filter run to exclude those, but we decided against that, as we already had done releases off of the 1.0 branch and didn\u0026#8217;t want to alter the commit history of a released branch after the fact.\n  ","id":11,"publicationdate":"Mar 16, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eWithin \u003ca href=\"https://debezium.io/\"\u003eDebezium\u003c/a\u003e, the project I\u0026#8217;m working on at Red Hat, we recently encountered an \"interesting\" situation where we had to resolve a rather difficult merge conflict.\nAs others where interested in how we addressed the issue, and also for our own future reference,\nI\u0026#8217;m going to give a quick run down of the problem we encountered and how we solved it.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Reworking Git Branches with git filter-branch","uri":"https://www.morling.dev/blog/reworking-git-branches-with-git-filter-branch/"},{"content":"","id":12,"publicationdate":"Jan 29, 2020","section":"tags","summary":"","tags":null,"title":"jakartaee","uri":"https://www.morling.dev/tags/jakartaee/"},{"content":"","id":13,"publicationdate":"Jan 29, 2020","section":"tags","summary":"","tags":null,"title":"java","uri":"https://www.morling.dev/tags/java/"},{"content":"","id":14,"publicationdate":"Jan 29, 2020","section":"tags","summary":"","tags":null,"title":"microprofile","uri":"https://www.morling.dev/tags/microprofile/"},{"content":"","id":15,"publicationdate":"Jan 29, 2020","section":"tags","summary":"","tags":null,"title":"monitoring","uri":"https://www.morling.dev/tags/monitoring/"},{"content":"The JDK Flight Recorder (JFR) is an invaluable tool for gaining deep insights into the performance characteristics of Java applications. Open-sourced in JDK 11, JFR provides a low-overhead framework for collecting events from Java applications, the JVM and the operating system.\n In this blog post we\u0026#8217;re going to explore how custom, application-specific JFR events can be used to monitor a REST API, allowing to track request counts, identify long-running requests and more. We\u0026#8217;ll also discuss how the JFR Event Streaming API new in Java 14 can be used to export live events, making them available for monitoring and alerting via tools such as Prometheus and Grafana.\n JFR and its companion tool JDK Mission Control (JMC) for analyzing JFR recordings have come a long way; originally developed at BEA and part of the JRockit VM, they were later on commercial features of the Oracle JDK. As of Java 11, JFR got open-sourced and is part of OpenJDK distributions. JMC is also open-source, but it\u0026#8217;s an independent tool under the OpenJDK umbrella, which must be downloaded separately.\n Using the combination of JFR and JMC, you can get all kinds of information about your Java application, such as events on garbage collection, compilation, classloading, memory allocation, file and socket IO, method profiling data, and much more. To learn more about Flight Recorder and Mission Control in general, have a look at the Code One 2019 presentation Introduction to JDK Mission Control \u0026amp; JDK Flight Recorder by Marcus Hirt and Klara Ward. You can find some more links to related useful resources towards the end of this post.\n Custom Flight Recorder Events One thing that\u0026#8217;s really great about JFR and JMC is that you\u0026#8217;re not limited to the events and data baked into the JVM and platform libraries: JFR also provides an API for implementing custom events. That way you can use the low-overhead event recording infrastructure (its goal is to add at most 1% performance overhead) for your own event types. This allows you to record and analyze higher-level events, using the language of your application-specific domain.\n Taking my day job project Debezium as an example (an open-source platform for change data capture for a variety of databases), we could for instance produce events such as \"Snapshot started\", \"Snapshotting of table 'Customers' completed\", \"Captured change event for transaction log offset 123\" etc. Users could send us recordings with these events and we could dive into them, in order to identify bugs or performance issues.\n In the following let\u0026#8217;s consider a less complex and hence better approachable example, though. We\u0026#8217;ll implement an event for measuring the duration of REST API calls. The Todo service from my recent blog post on Quarkus Qute will serve as our guinea pig. It is based on the Quarkus stack and provides a simple REST API based on JAX-RS. As always, you can find the complete source code for this blog post on GitHub.\n Event types are implemented by extending the jdk.jfr.Event class; It already provides us with some common attributes such as a timestamp and a duration. In sub-classes you can add application-specific payload attributes, as well as some metadata such as a name and category which will be used for organizing and displaying events when looking at them in JMC.\n Which attributes to add depends on your specific requirements; you should aim for the right balance between capturing all the relevant information that will be useful for analysis purposes later on, while not going overboard and adding too much, as that could cause record files to become too large, in particular for events that are emitted with a high frequency. Also retrieval of the attributes should be an efficient operation, so to avoid any unneccessary overhead.\n Here\u0026#8217;s a basic event class for monitoring our REST API calls:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 @Name(JaxRsInvocationEvent.NAME) (1) @Label(\"JAX-RS Invocation\") @Category(\"JAX-RS\") @Description(\"Invocation of a JAX-RS resource method\") @StackTrace(false) (2) public class JaxRsInvocationEvent extends Event { static final String NAME = \"dev.morling.jfr.JaxRsInvocation\"; @Label(\"Resource Method\") (3) public String method; @Label(\"Media Type\") public String mediaType; @Label(\"Java Method\") public String javaMethod; @Label(\"Path\") public String path; @Label(\"Query Parameters\") public String queryParameters; @Label(\"Headers\") public String headers; @Label(\"Length\") @DataAmount (4) public int length; @Label(\"Response Headers\") public String responseHeaders; @Label(\"Response Length\") public int responseLength; @Label(\"Response Status\") public int status; }      1 The @Name, @Category, @Description and @Label annotations define some meta-data, e.g. used for controlling the appearance of these events in the JMC UI   2 JAX-RS invocation events shouldn\u0026#8217;t contain a stacktrace by default, as that\u0026#8217;d only increase the size of Flight Recordings without adding much value   3 One payload attribute is defined for each relevant property such as HTTP method, media type, the invoked path etc.   4 @DataAmount tags this attribute as a data amount (by default in bytes) and will be displayed accordingly in JMC; there are many other similar annotations in the jdk.jfr package, such as @MemoryAddress, @Timestamp and more    Having defined the event class itself, we must find a way for emitting event instances at the right point in time. In the simplest case, e.g. suitable for events related to your application logic, this might happen right in the application code itself. For more \"technical\" events it\u0026#8217;s a good idea though to keep the creation of Flight Recorder events separate from your business logic, e.g. by using mechanisms such as servlet filters, interceptors and similar, which allow to inject cross-cutting logic into the call flow of your application.\n You also might employ byte code instrumentation at build or runtime for this purpose. The JMC Agent project aims at providing a configurable Java agent that allows to dynamically inject code for emitting JFR events into running programs. Via the EventFactory class, the JFR API also provides a way for defining event types dynamically, should their payload attributes only be known at runtime.\n For monitoring a JAX-RS based REST API, the ContainerRequestFilter and ContainerResponseFilter contracts come in handy, as they allow to hook into the request handling logic before and after a REST request gets processed:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @Provider (1) public class FlightRecorderFilter implements ContainerRequestFilter, ContainerResponseFilter { @Override (2) public void filter(ContainerRequestContext requestContext) throws IOException { JaxRsInvocationEvent event = new JaxRsInvocationEvent(); if (!event.isEnabled()) { (3) return; } event.begin(); (4) requestContext.setProperty(JaxRsInvocationEvent.NAME, event); (5) } @Override (6) public void filter(ContainerRequestContext requestContext, ContainerResponseContext responseContext) throws IOException { JaxRsInvocationEvent event = (JaxRsInvocationEvent) requestContext .getProperty(JaxRsInvocationEvent.NAME); if (event == null || !event.isEnabled()) { return; } event.end(); (7) event.path = String.valueOf(requestContext.getUriInfo().getPath()); if (event.shouldCommit()) { (8) event.method = requestContext.getMethod(); event.mediaType = String.valueOf(requestContext.getMediaType()); event.length = requestContext.getLength(); event.queryParameters = requestContext.getUriInfo() .getQueryParameters().toString(); event.headers = requestContext.getHeaders().toString(); event.javaMethod = getJavaMethod(requestContext); event.responseLength = responseContext.getLength(); event.responseHeaders = responseContext.getHeaders().toString(); event.status = responseContext.getStatus(); event.commit(); (9) } } private String getJavaMethod(ContainerRequestContext requestContext) { String propName = \"org.jboss.resteasy.core.ResourceMethodInvoker\"; ResourceMethodInvoker invoker = (ResourceMethodInvoker)requestContext.getProperty(propName); return invoker.getMethod().toString(); } }      1 Allows the filter to be picked up automatically by the JAX-RS implementation   2 Will be invoked before the request is processed   3 Nothing to do if the event type is not enabled for recordings currently   4 Begin the timing of the event   5 Store the event in the request context, so it can be obtained again later on   6 Will be invoked after the request has been processed   7 End the timing of the event   8 The event should be committed if it is enabled and its duration is within the threshold configured for it; in that case, populate all the payload attributes of the event based on the values from the request and response contexts   9 Commit the event with Flight Recorder    With that, our event class is pretty much ready to be used. There\u0026#8217;s only one more thing to do, and that is registering the new type with the Flight Recorder system. A Quarkus application start-up lifecycle method comes in handy for that:\n 1 2 3 4 5 6 7 @ApplicationScoped public class Metrics { public void registerEvent(@Observes StartupEvent se) { FlightRecorder.register(JaxRsInvocationEvent.class); } }    Note this step isn\u0026#8217;t strictly needed, the event type can also be used without explicit registration. But doing so will later on allow to apply specific settings for the event in Mission Control (see below), also if no event of this type has been emitted yet.\n   Creating JFR Recordings Now let\u0026#8217;s capture some JAX-RS API events using Flight Recorder and inspect them in Mission Control.\n To do so, make sure to have Mission Control installed. Just as with OpenJDK, there are different builds for Mission Control to choose from. If you\u0026#8217;re in the Fedora/RHEL universe, there\u0026#8217;s a repository package which you can install, e.g. like this for the Fedora JMC package:\n 1 sudo dnf module install jmc:7/default    Alternatively, you can download builds for different platforms from Oracle; some more info about these builds can be found in this blog post by Marcus Hirt. There\u0026#8217;s also the Liberica Mission Control build by BellSoft and Zulu Mission Control by Azul. The AdoptOpenJDK provides snapshot builds of JMC 8 as well as an Eclipse update site for installing JMC into an existing Eclipse instance.\n If you\u0026#8217;d like to follow along and run these steps yourself, check out the source code from GitHub and then perform the following commands:\n 1 2 cd example-service \u0026amp;\u0026amp; mvn clean package \u0026amp;\u0026amp; cd .. docker-compose up --build    This builds the project using Maven and spins up the following services using Docker Compose:\n   example-service: The Todo example application\n  todo-db: The Postgres database used by the Todo service\n  prometheus and grafana: For monitoring live events later on\n   Then go to http://localhost:8080/todo, where you should see the Todo web application:\n   Now fire up Mission Control. The example service run via Docker Compose is configured so you can connect to it on localhost. In the JVM Browser, create a new connection with host \"localhost\" and port \"1898\". Hit \"Test connection\", which should yield \"OK\", then click \"Finish\".\n   Create a new recording by expanding the localhost:1898 node in the JVM Explorer, right-clicking on \"Flight Recorder\" and choosing \"Start Flight Recording\u0026#8230;\u0026#8203;\". Confirm the default settings, which will create a recording with a duration of one minute. Go back to the Todo web application and perform a few tasks like creating some new todos, editing and deleting them, or filtering the todo list.\n Either wait for the recording to complete or stop it by right-clicking on the recording name and selecting \"Stop\". Once the recording is done, it will be opened automatically. Now you could dive into all the logged events for the OS, the JVM etc, but as we\u0026#8217;re interested in our custom JAX-RS events, Choose \"Event Browser\" in the outline view and expand the \"JAX-RS\" category. You will see the events for all your REST API invocations, including information such as duration of the request, the HTTP method, the resource path and much more:\n   In a real-world use case, you could now use this information for instance to identify long-running requests and correlate these events with other data points in the Flight Recording, such as method profiling and memory allocation data, or sub-optimal SQL statements in your database.\n     If your application is running in production, it might not be feasible to connect to it via Mission Control from your local workstation. The jcmd utility comes in handy in that case; part of the JDK, you can use it to issue diagnostic commands against a running JVM.\n Amongst many other things, it allows you to start and stop Flight Recordings. On the environment with your running application, first run jcmd -l, which will show you the PIDs of all running Java processes. Having identified the PID of the process you\u0026#8217;d like to examine, you can initiate a recording like so:\n 1 2 jcmd \u0026lt;PID\u0026gt; JFR.start delay=5s duration=30s \\ name=MyRecording filename=my-recording.jfr    This will start a recording of 30 seconds, beginning in 5 seconds from now. Once the recording is done, you could copy the file to your local machine and load it into Mission Control for further analysis. To learn more about creating Flight Recordings via jcmd, refer to this great cheat sheet.\n     Another useful tool in the belt is the jfr command, which was introduced in JDK 12. It allows you to filter and examine the binary Flight Recording files. You also can use it to extract parts of a recording and convert them to JSON, allowing them to be processed with other tools. E.g. you could convert all the JAX-RS events to JSON like so:\n 1 jfr print --json --categories JAX-RS my-recording.jfr      Event Settings Sometimes it\u0026#8217;s desirable to configure detailed behaviors of a given event type. For the JAX-RS invocation event it might for instance make sense to only log invocations of particular paths in a specific recording, allowing for a smaller recording size and keeping the focus on a particular subset of all invocations. JFR supports this by the notion of event settings. Such settings can be specified when creating a recording; based on the active settings, particular events will be included or excluded in the recording.\n Inspired by the JavaDoc of @SettingDefinition let\u0026#8217;s see what\u0026#8217;s needed to enhance JaxRsInvocationEvent with that capability. The first step is to define a subclass of jdk.jfr.SettingControl, which serves as the value holder for our setting:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public class PathFilterControl extends SettingControl { private Pattern pattern = Pattern.compile(\".*\"); (1) @Override (2) public void setValue(String value) { this.pattern = Pattern.compile(value); } @Override (3) public String combine(Set\u0026lt;String\u0026gt; values) { return String.join(\"|\", values); } @Override (4) public String getValue() { return pattern.toString(); } (5) public boolean matches(String s) { return pattern.matcher(s).matches(); } }      1 A regular expression pattern that\u0026#8217;ll be matched against the path of incoming events; by default all paths are included (.*)   2 Invoked by the JFR runtime to set the value for this setting   3 Invoked when multiple recordings are running at the same time, combining the settings values   4 Invoked by the runtime for instance when getting the default value of the setting   5 Matches the configured setting value against a particular path    On the event class itself a method with the following characteristics must be declared which will receive the setting by the JFR runtime:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 class JaxRsInvocationEvent extends Event { @Label(\"Path\") public String path; // other members... @Label(\"Path Filter\") @SettingDefinition (1) protected boolean pathFilter(PathFilterControl pathFilter) { (2) return pathFilter.matches(path); } }      1 Tags this as a setting   2 The method must be public, take a SettingControl type as its single parameter and return boolean    This method will be invoked by the JFR runtime during the shouldCommit() call. It passes in the setting value of the current recording so it can applied to the path value of the given event. In case the filter returns true, the event will be added to the recording, otherwise it will be ignored.\n We also could use such setting to control the inclusion or exclusion of specific event attributes. For that, the setting definition method would always have to return true, but depending on the actual setting it might set particular attributes of the event class to null. For instance this might come in handy if we wanted to log the entire request/response body of our REST API. Doing this all the time might be prohibitive in terms of recording size, but it might be enabled for a particlar short-term recording for analyzing some bug.\n Now let\u0026#8217;s see how the path filter can be applied when creating a new recording in Mission Control. The option is a bit hidden, but here\u0026#8217;s how you can enable it. First, create a new Flight Recording, then choose \"Template Manager\" in the dialogue:\n   Duplicate the \"Continuous\" template and edit it:\n   Click \"Advanced\":\n   Expand \"JAX-RS\" \u0026#8594; \"JAX-RS Invocation\" and put .*(new|edit).* into the Path Filter control:\n   Now close the last two dialogues. In the \"Start Flight Recording\" dialogue make sure to select your new template under \"Event Settings\"; although you\u0026#8217;ve edited it before, it won\u0026#8217;t be selected automatically. I lost an hour or so wondering why my settings were not applied\u0026#8230;\u0026#8203; .\n Lastly, click \"Finish\" to begin the recording:\n   Perform some tasks in the Todo web app and stop the recording. You should see only the REST API calls for the new and edit operations, whereas no events should be shown for the list and delete operations of the API.\n     In order to apply specific settings when creating a recording on the CLI using jcmd, edit the settings as described above. Then go to the Template Manager and export the profile you\u0026#8217;d like to use. When starting the recording via jcmd, specify the settings file via the settings=/path/to/settings.jfc parameter.\n       JFR Event Streaming Flight Recorder files are great for analyzing performance characteristics in an \"offline\" approach: you can take recordings in your production environment and ship them to your work station or a remote support team, without requiring live access to the running application. This is also an interesting mode for open-source projects, where maintainers typically don\u0026#8217;t have access to running applications of their users. Exchanging Flight Recordings (limited to a sensible subset of information, so to avoid exposure of confidential internals) might allow open source developers to gain insight into characteristics of their libraries when deployed to production at their users.\n But there\u0026#8217;s another category of use cases for event data sourced from applications, the JVM and the operating system, where the recording file approach doesn\u0026#8217;t quite fit: live monitoring and alerting of running applications. E.g. operations teams might want to set up dashboards showing the most relevant application metrics in \"real-time\", without having to create any recording files first. A related requirement is alerting, so to be notified when metrics reach a certain threshold. For instance it might be desirable to be alterted if the request duration of our JAX-RS API goes beyond a defined value such as 100 ms.\n This is where JEP 349 (\"JFR Event Streaming\") comes in. It\u0026#8217;ll be part of Java 14 and its stated goal is to \"provide an API for the continuous consumption of JFR data on disk, both for in-process and out-of-process applications\". That\u0026#8217;s exactly what we need for our monitoring/dashboarding use case. Using the Streaming API, Flight Recorder events of the running application can be exposed to external consumers, without having to explicitly load any recording files.\n Now it may be prohibitively expensive to stream each and every event with all its detailed information to remote clients. But that\u0026#8217;s not needed for monitoring purposes anyways. Instead, we can expose metrics based on our events, such as the total number and frequency of REST API invocations, or the average and 99th percentile duration of the calls.\n   MicroProfile Metrics The following shows a basic implementation of exposing these metrics for the JAX-RS API events to Prometheus/Grafana, where they can be visualized using a dashboard. Being based on Quarkus, the Todo web application can leverage all the MicroProfile APIs. On of them is the MicroProfile Metrics API, which defines a \"unified way for Microprofile servers to export Monitoring data (\"Telemetry\") to management agents\".\n While the MicroProfile Metrics API is used in an annotation-driven fashion often-times, it also provides a programmatic API for registering metrics. This can be leveraged to expose metrics based on the JAX-RS Flight Recorder events:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 @ApplicationScoped public class Metrics { @Inject (1) MetricRegistry metricsRegistry; private RecordingStream recordingStream; (2) public void onStartup(@Observes StartupEvent se) { recordingStream = new RecordingStream(); (3) recordingStream.enable(JaxRsInvocationEvent.NAME); recordingStream.onEvent(JaxRsInvocationEvent.NAME, event -\u0026gt; { (4) String path = event.getString(\"path\") .replaceAll(\"(\\\\/)([0-9]+)(\\\\/?)\", \"$1{param}$3\"); (5) String method = event.getString(\"method\"); String name = path + \"-\" + method; Metadata metadata = metricsRegistry.getMetadata().get(name); if (metadata == null) { metricsRegistry.timer(Metadata.builder() (6) .withName(name) .withType(MetricType.TIMER) .withDescription(\"Metrics for \" + path + \" (\" + method + \")\") .build()).update(event.getDuration().toNanos(), TimeUnit.NANOSECONDS); } else { (7) metricsRegistry.timer(name).update(event.getDuration() .toNanos(), TimeUnit.NANOSECONDS); } }); recordingStream.startAsync(); (8) } public void stop(@Observes ShutdownEvent se) { recordingStream.close(); (9) try { recordingStream.awaitTermination(); } catch (InterruptedException e) { throw new RuntimeException(e); } } }      1 Inject the MicroProfile Metrics registry   2 A stream providing push access to JFR events   3 Initialize the stream upon application start-up, so it includes the JAX-RS invocation events   4 For each JaxRsInvocationEvent this callback will be invoked   5 To register a corresponding metric, any path parameters are replaced with a constant placeholder, so that e.g. all invocations of the todo/{id}/edit path are exposed via one single metric instead of having separate ones for Todo 1, Todo 2 etc.   6 If the metric for the specific path hasn\u0026#8217;t been registered yet, then do so; it\u0026#8217;s a metric of type TIMER, allowing metric consumers to track the duration of calls of that particular path   7 If the metric for the path has been registered before, update its value with the duration of the incoming event   8 Start the stream asynchronously, not blocking the onStartup() method   9 Close the JFR event stream upon application shutdown    When connecting to the running application using JMC now, you\u0026#8217;ll see a continuous recording, which serves as the basis for the event stream. It only contains events of the JaxRsInvocationEvent type.\n MicroProfile Metrics exposes any application-provided metrics in the Prometheus format under the /metrics/application endpoint; for each operation of the REST API, e.g. POST to /todo/{id}/edit, the following metrics are provided:\n   request rate per second, minute, five minutes and 15 minutes\n  min, mean and max duration as well as standard deviation\n  total invocation count\n  duration of 75th, 95th, 99th etc. percentiles\n     Once the endpoint is provided, it\u0026#8217;s not difficult to set up a scraping process for ingesting the metrics into the Prometheus time-series database. You can find the required Prometheus configuration in the accompanying source code repository.\n While Prometheus provides some visualization capabilities itself, it is often used together with Grafana, which allows to build nicely looking dashboards via a rather intuitive UI. Here\u0026#8217;s an example dashboard showing the duration and invocation numbers for the different methods in the Todo REST API:\n   Again you can find the complete configuration for Grafana including the definition of that dashboard in the example repo. It will automatically be loaded when using the Docker Compose set-up shown above. Based on that you could easily expand the dashboard for other metrics and set up alerts, too.\n Combining the monitoring of live key metrics with the deep insights possible via detailed JFR recordings enable a very powerful workflow for analysing performance issues in production:\n   When setting up the continuous recording that serves as the basis for the metrics, have it contain all the event types you\u0026#8217;d need to gain insight into GC or memory issues etc.; specify a maximum size via RecordingStream#setMaxSize(), so to avoid an indefinitely growing recording; you\u0026#8217;ll probably need to experiment a bit to find the right trade-off between number of enabled events, duration that\u0026#8217;ll be covered by the recording and the required disk space\n  Only expose a relevant subset of the events as metrics to Prometheus/Grafana, such as the JAX-RS API invocation events in our example\n  Set up an alert in Grafana on the key metrics, e.g. mean duration of the REST calls, or 99th percentile thereof\n  If the alert triggers, take a dump of the last N minutes of the continuous recording via JMC or jcmd (using the JFR.dump command), and analyze that detailed recording to understand what was happening in the time leading to the alert\n     Summary and Related Work Flight Recorder and Mission Control are excellent tools providing deep insight into the performance characteristics of Java applications. While there\u0026#8217;s a large amount of data and highly valuable information provided out the box, JFR and JMC also allow for the recording of custom, application-specific events. With its low overhead, JFR can be enabled on a permanent basis in production environments. Combined with the Event Streaming API introduced in Java 14, this opens up an attractive, very performant alternative to other means of capturing analysis information at application runtime, such as logging libraries. Providing live key metrics derived from JFR events to tools such as Prometheus and Grafana enables monitoring and alerting in \"real-time\".\n For many enterprises that are still on Java 11 or even 8, it\u0026#8217;ll still be far out into the future until they might adopt the streaming API. But with more and more companies joining the OpenJDK efforts, it might be a possiblity that this useful feature gets backported to earlier LTS releases, just as the open-sourced version of Flight Recorder itself got backported to Java 8.\n There are quite a few posts and presentations about JFR and JMC available online, but many of them refer to older versions of those tools, before they got open-sourced. Here are some up-to-date resources which I found very helpful:\n   Continuous Monitoring with JDK Flight Recorder: a talk from QCon SF 2019 by Mikael Vidstedt\n  Flight Recorder \u0026amp; Mission Control at Code One 2019: a compilation of several great sessions on these two tools at last year\u0026#8217;s Code One, put together by Marcus Hirt\n  Digging Into Sockets With Java Flight Recorder: blog post by Petr Bouda on identifying performance bottlenecks with JFR in a Netty-based web application\n   Lastly, the Red Hat OpenJDK team is working on some very interesting projects around JFR and JMC, too. E.g. they\u0026#8217;ve built a datasource for Grafana which lets you examine the events of a JFR file. They also work on tooling to simplify the usage of JFR in container-based environments such as Kubernetes and OpenShift, including a K8s Operator for controlling Flight Recordings and a web-based UI for managing JFR in remote JVMs. Should you happen to be at the FOSDEM conference in Brussels on the next weekend, be sure to not miss the JMC \u0026amp; JFR - 2020 Vision session by Red Hat engineer Jie Kang.\n If you\u0026#8217;d like to experiment with JDK Flight Recorder and JDK Mission Control based on the Todo web application yourself, you can find the complete source code for this post on GitHub.\n Many thanks to Mario Torre and Jie Kang for reviewing an early draft of this post.\n  ","id":16,"publicationdate":"Jan 29, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe \u003ca href=\"https://openjdk.java.net/jeps/328\"\u003eJDK Flight Recorder\u003c/a\u003e (JFR) is an invaluable tool for gaining deep insights into the performance characteristics of Java applications.\nOpen-sourced in JDK 11, JFR provides a low-overhead framework for collecting events from Java applications, the JVM and the operating system.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIn this blog post we\u0026#8217;re going to explore how custom, application-specific JFR events can be used to monitor a REST API, allowing to track request counts, identify long-running requests and more.\nWe\u0026#8217;ll also discuss how the JFR \u003ca href=\"https://openjdk.java.net/jeps/349\"\u003eEvent Streaming API\u003c/a\u003e new in Java 14 can be used to export live events,\nmaking them available for monitoring and alerting via tools such as Prometheus and Grafana.\u003c/p\u003e\n\u003c/div\u003e","tags":["java","monitoring","microprofile","jakartaee","quarkus"],"title":"Monitoring REST APIs with Custom JDK Flight Recorder Events","uri":"https://www.morling.dev/blog/rest-api-monitoring-with-custom-jdk-flight-recorder-events/"},{"content":"","id":17,"publicationdate":"Jan 29, 2020","section":"tags","summary":"","tags":null,"title":"quarkus","uri":"https://www.morling.dev/tags/quarkus/"},{"content":"","id":18,"publicationdate":"Jan 29, 2020","section":"tags","summary":"","tags":null,"title":"Tags","uri":"https://www.morling.dev/tags/"},{"content":"","id":19,"publicationdate":"Jan 20, 2020","section":"tags","summary":"","tags":null,"title":"bean-validation","uri":"https://www.morling.dev/tags/bean-validation/"},{"content":"Record types are one of the most awaited features in Java 14; they promise to \"provide a compact syntax for declaring classes which are transparent holders for shallowly immutable data\". One example where records should be beneficial are data transfer objects (DTOs), as e.g. found in the remoting layer of enterprise applications. Typically, certain rules should be applied to the attributes of such DTO, e.g. in terms of allowed values. The goal of this blog post is to explore how such invariants can be enforced on record types, using annotation-based constraints as provided by the Bean Validation API.\n Record Invariants and Bean Validation Records (a preview feature as of Java 14) help to cut down the ceremony when defining plain data holder objects. In a nutshell, you solely need to declare the attributes that should make up the state of the record type (\"components\" in terms of JEP 359), and quite a few things you\u0026#8217;d otherwise have to implement by hand will be created for you automatically:\n   a private final field and a corresponding read accessor for each component\n  a constructor for passing in all component values\n  toString(), equals() and hashCode() methods.\n   As an example, here\u0026#8217;s a record Car with three components:\n 1 2 3 public record Car(String manufacturer, String licensePlate, int seatCount) { }    Now let\u0026#8217;s assume a few class invariants should be applied to this record (inspired by an example from the Hibernate Validator reference guide):\n   manufacturer is a non-blank string\n  license plate is never null and has a length of 2 to 14 characters\n  seatCount is at least 2\n   Class invariants like these are specific conditions or rules applying to the state of a class (as manifesting in its fields), which always are guaranteed to be satisfied for the lifetime of an instance of the class.\n The Bean Validation API defines a way for expressing and validating constraints using Java annotations. By putting constraint annotations to the components of a record type, it\u0026#8217;s a perfect means of describing the invariants from above:\n 1 2 3 4 5 public record Car( @NotBlank String manufacturer, @NotNull @Size(min = 2, max = 14) String licensePlate, @Min(2) int seatCount) { }    Of course declaring constraints using annotations by itself won\u0026#8217;t magically enforce these invariants. In order to do so, the javax.validation.Validator API must be invoked at suitable points in the object lifecycle, so to avoid any of the invariants to be violated. As records are immutable, it is sufficient to validate the constraints once when creating a new Car instance. If no constraints are violated, the created instance is guaranteed to always satisfy its invariants.\n   Implementation The key question now is how to validate the invariants while constructing new Car instances. This is where Bean Validation\u0026#8217;s API for method validation comes in: it allows to validate pre- and post-conditions that should be satisfied when a Java method or constructor gets invoked. Pre-conditions are expressed by applying constraints to method and constructor parameters, whereas post-conditions are expressed by putting constraints to a method or constructor itself.\n This can be leveraged for enforcing record invariants: as it turns out, any annotations on the components of a record type are also copied to the corresponding parameters of the generated constructor. I.e. the Car record implicitly has a constructor which looks like this:\n 1 2 3 4 5 6 7 8 9 public Car( @NotBlank String manufacturer, @NotNull @Size(min = 2, max = 14) String licensePlate, @Min(2) int seatCount) { this.manufacturer = manufacturer; this.licensePlate = licensePlate; this.seatCount = seatCount; }    That\u0026#8217;s exactly what we need: by validating these parameter constraints upon instantiation of the Car class, we can make sure that only valid objects can ever be created, ensuring that the record type\u0026#8217;s invariants are always guaranteed.\n What\u0026#8217;s missing is a way for automatically validating them upon constructor invocation. The idea for that is to enhance the byte code of the implicit Car constructor so that it passes the incoming parameter values to Bean Validation\u0026#8217;s ExecutableValidator#validateConstructorParameters() method and raises a constraint violation exception in case of any invalid parameter values.\n We\u0026#8217;re going to use the excellent ByteBuddy library for this job. Here\u0026#8217;s a slightly simplified implementation for invoking the executable validator (you can find the complete source code of this example in this GitHub repository):\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 public class ValidationInterceptor { private static final Validator validator = Validation (1) .buildDefaultValidatorFactory() .getValidator(); public static \u0026lt;T\u0026gt; void validate(@Origin Constructor\u0026lt;T\u0026gt; constructor, @AllArguments Object[] args) { (2) Set\u0026lt;ConstraintViolation\u0026lt;T\u0026gt;\u0026gt; violations = validator (3) .forExecutables() .validateConstructorParameters(constructor, args); if (!violations.isEmpty()) { String message = violations.stream() (4) .sorted(ValidationInterceptor::compare) .map(cv -\u0026gt; getParameterName(cv) + \" - \" + cv.getMessage()) .collect(Collectors.joining(System.lineSeparator())); throw new ConstraintViolationException( (5) \"Invalid instantiation of record type \" + constructor.getDeclaringClass().getSimpleName() + System.lineSeparator() + message, violations); } } private static int compare(ConstraintViolation\u0026lt;?\u0026gt; o1, ConstraintViolation\u0026lt;?\u0026gt; o2) { return Integer.compare(getParameterIndex(o1), getParameterIndex(o2)); } private static String getParameterName(ConstraintViolation\u0026lt;?\u0026gt; cv) { // traverse property path to extract parameter name } private static int getParameterIndex(ConstraintViolation\u0026lt;?\u0026gt; cv) { // traverse property path to extract parameter index } }      1 Obtain a Bean Validation Validator instance   2 The @Origin and @AllArguments annotations are the hint to ByteBuddy that the invoked constructor and parameter values should be passed to this method from within the enhanced constructor   3 Validate the passed constructor arguments using Bean Validation   4 If there\u0026#8217;s at least one violated constraint, create a message comprising all constraint violation messages, ordered by parameter index   5 Raise a ConstraintViolationException, containing the message created before as well as all the constraint violations    Having implemented the validation interceptor, the code of the record constructor must be enhanced by ByteBuddy, so that it invokes the inceptor. ByteBuddy provides different ways for doing so, e.g. at application start-up using a Java agent. For this example, we\u0026#8217;re going to employ build-time enhancement via the ByteBuddy Maven plug-in. The enhancement logic itself is implemented in a custom net.bytebuddy.build.Plugin:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 public class ValidationWeavingPlugin implements Plugin { @Override public boolean matches(TypeDescription target) { (1) return target.getDeclaredMethods() .stream() .anyMatch(m -\u0026gt; m.isConstructor() \u0026amp;\u0026amp; hasConstrainedParameter(m)); } @Override public Builder\u0026lt;?\u0026gt; apply(Builder\u0026lt;?\u0026gt; builder, TypeDescription typeDescription, ClassFileLocator classFileLocator) { return builder.constructor(this::hasConstrainedParameter) (2) .intercept(SuperMethodCall.INSTANCE.andThen( MethodDelegation.to(ValidationInterceptor.class))); } private boolean hasConstrainedParameter(MethodDescription method) { return method.getParameters() (3) .asDefined() .stream() .anyMatch(p -\u0026gt; isConstrained(p)); } private boolean isConstrained( ParameterDescription.InDefinedShape parameter) { (4) return !parameter.getDeclaredAnnotations() .asTypeList() .filter(hasAnnotation(annotationType(Constraint.class))) .isEmpty(); } @Override public void close() throws IOException { } }      1 Determines whether a type should be enhanced or not; this is the case if there\u0026#8217;s at least one constructor that has one more more constrained parameters   2 Applies the actual enhancement: into each constrained constructor the call to ValidationInterceptor gets injected   3 Determines whether a method or constructor has at least one constrained parameter   4 Determines whether a parameter has at least one constraint annotation (an annotation meta-annotated with @Constraint; for the sake of simplicity the case of constraint inheritance is ignored here)    The next step is to configure the ByteBuddy Maven plug-in in the pom.xml of the project:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;net.bytebuddy\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;byte-buddy-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${version.bytebuddy}\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;transform\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;transformations\u0026gt; \u0026lt;transformation\u0026gt; \u0026lt;plugin\u0026gt; dev.morling.demos.recordvalidation.implementation.ValidationWeavingPlugin \u0026lt;/plugin\u0026gt; \u0026lt;/transformation\u0026gt; \u0026lt;/transformations\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt;    This plug-in runs in the process-classes phase by default, so it can access and enhance the class files generated during compilation. If you were to build the project now, you could use the javap tool to examine the byte code of the Car class,and you\u0026#8217;d see that the implicit constructor of that class contains an invocation of the ValidationInterceptor#validate() method.\n As an example, let\u0026#8217;s consider the following attempt to instantiate a Car object, which violates the invariants of that record type:\n 1 Car invalid = new Car(\"\", \"HH-AB-123\", 1);    A constraint violation like this will be thrown immediately:\n 1 2 3 4 5 javax.validation.ConstraintViolationException: Invalid instantiation of record type Car manufacturer - must not be blank seatCount - must be greater than or equal to 2 at dev.morling.demos.recordvalidation.RecordValidationTest.canValidate(RecordValidationTest.java:20)    If all constraints are satisfied, no exception will be thrown and the caller obtains the new Car instance, whose invariants are guaranteed to be met for the remainder of the object\u0026#8217;s lifetime.\n   Advantages Having shown how Bean Validation can be leveraged to enforce the invariants of Java record types, it is time to reflect: is this this approach worth the additional complexity incurred by adding a library such as Bean Validation and hooking it up using byte code enhancement? After all, you could also validate incoming parameter values using methods such as Objects#requireNonNull().\n As so often, you need to make such decision based on your specific requirements and needs. Here are some advantages I can see about the Bean Validation approach:\n   Invariants become part of the API: Constraint annotations on public API members such as the implicit record constructor are easily discoverable by users of such type; they are listed in generated JavaDoc, you can see them when hovering over an invocation in your IDE (once records are supported); when used on the DTOs of a REST layer, the invariants could also be added to automatically generated API documentation. All this makes it easy for users of the type to understand the invariants and also avoids potential inconsistencies between a manual validation implementation and corresponding hand-written documentation\n  Providing constraint metadata: The Bean Validation constraint meta-data API can be used to obtain information about the constraints of Java types; for instance this can be used to implement client-side validation of constraints in a web application\n  Less code: Putting constraint annotations directly to the record components themselves avoids the need for implementing these checks manually in an explicit canonical constructor\n  I18N support: Bean Validation provides means of internationalizing constraint violation messages; if your record types are instantiated based on user input (e.g. when using them as data types in a REST API), this allows for localized error messages in the UI\n  Returning all constraints at once: For UIs it\u0026#8217;s typically beneficial to return all the constraint violations at once instead of showing them one by one; while doable in a hand-written implementation, it requires a bit of effort, whereas you get this \"for free\" when using Bean Validation which always returns a set of all the violations\n  Lots of ready-made constraints: Bean Validation comes with a range of constraints out of the box; in addition libraries such as Hibernate Validator and others provide many more ready-to-use constraints, coming in handy for instance when implementing domain-specific value types with complex validation rules:\n1 2 3 public record EmailAddress( @Email @NotNull @Size(min=1, max=250) String value) { }      Support for validation groups: Bean Validation\u0026#8217;s concept of validation groups allows you to validate only sub-sets of constraints in specific contexts; e.g. based on location and applying legal requirements\n  Dynamic constraint definition: Using Hibernate Validator, constraints can also be declared dynamically using a fluent API. This can be very useful when your validation requirements vary at runtime, e.g. if you need to apply different constraint configurations for different tenants.\n     Limitations One area where this current proof-of-concept implementation falls a bit short is the validation of invariants that apply to multiple components. For instance consider a record type representing an interval with a begin and an end attribute, where you\u0026#8217;d like to enforce the invariant that end is larger than begin.\n Bean Validation addresses this sort of requirement via class-level constraints and, for method and constructor validation, cross-parameter constraints. Class-level constraints are not really suitable for our purposes, because we want to validate the invariants before an object instance is created.\n Cross-parameter constraints on the other hand are exactly what we\u0026#8217;d need. As they must be given on a constructor or method, the canonical constructor of a record must be explicitly declared in this case. Using Hibernate Validator\u0026#8217;s @ParameterScriptAssert constraint, the invariant from above could be expressed like so:\n 1 2 3 4 5 6 public record Interval(int begin, int end) { @ParameterScriptAssert(lang=\"javascript\", script=\"end \u0026gt; begin\") public Interval { } }    This works as expected, but there\u0026#8217;s one caveat: any annotations from the record components are not propagated to the corresponding parameters of the canoncial constructor in this case. This means that any constraints given on the individual components would be lost. Right now it\u0026#8217;s not quite clear to me whether that\u0026#8217;s an intended behavior or rather a bug in the current record implementation.\n If indeed it is intentional, than there\u0026#8217;d be no way other than specifying the constraints explicitly on the parameters of a fully manually implemented constructor:\n 1 2 3 4 5 6 7 8 public record Interval(int begin, int end) { @ParameterScriptAssert(lang=\"javascript\", script=\"end \u0026gt; begin\") public Interval(@Positive int begin, @Positive int end) { this.begin = begin; this.end = end; } }    This works, but of course we\u0026#8217;re losing a bit of the conciseness promised by records.\n Update, Jan 20, 2020, 20:57: Turns out, the current behavior indeed is not intended (see JDK-8236597) and in a future Java version the shorter version of the code shown above should work.\n   Wrap-Up In this blog post we\u0026#8217;ve explored how invariants on Java 14 record types can be enforced using the Bean Validation API. With just a bit of byte code magic the task gets manageable: by validating invariants expressed by constraint annotations on record components right at instantiation time, only valid record instances will ever be exposed to callers. Key for that is the fact that any annotations from record components are automatically propagated to the corresponding parameters of the canonical record constructor. That way they can be validated using Bean Validation\u0026#8217;s method validation API. It remains to be seen, whether invariants based on multiple record components also can be enforced as easily.\n From the perspective of the Bean Validation specification, it\u0026#8217;ll surely make sense to explore support for record types. While not as powerful as enforcing invariants at construction time via byte code enhancement, it might also be useful to support the validation of component values via their read accessors. For that, the notion of \"properties\" would have to be relaxed, as the read accessors of records don\u0026#8217;t have the JavaBeans get prefix currently expected by Bean Validation. It also should be considered to expand the Bean Validation metadata API accordingly.\n I would also be very happy to learn about your thoughts around this topic. While Bean Validation 3.0 (as part of Jakarta EE 9) in all likelyhood won\u0026#8217;t bring any changes besides the transition to the jakarta.* package namespace, this may be an area where we could evolve the specification for Jakarta EE 10.\n If you\u0026#8217;d like to experiment with the validation of record types yourself, you can find the complete source code on GitHub.\n   ","id":20,"publicationdate":"Jan 20, 2020","section":"blog","summary":"Record types are one of the most awaited features in Java 14; they promise to \"provide a compact syntax for declaring classes which are transparent holders for shallowly immutable data\". One example where records should be beneficial are data transfer objects (DTOs), as e.g. found in the remoting layer of enterprise applications. Typically, certain rules should be applied to the attributes of such DTO, e.g. in terms of allowed values. The goal of this blog post is to explore how such invariants can be enforced on record types, using annotation-based constraints as provided by the Bean Validation API.","tags":["bean-validation","jakartaee"],"title":"Enforcing Java Record Invariants With Bean Validation","uri":"https://www.morling.dev/blog/enforcing-java-record-invariants-with-bean-validation/"},{"content":"When Java 9 was introduced in 2017, it was the last major version published under the old release scheme. Since then, a six month release cadence has been adopted. This means developers don\u0026#8217;t have to wait years for new APIs and language features, but they can get their hands onto the latest additions twice a year. In this post I\u0026#8217;d like to describe how you can try out new language features such as Java 13 text blocks in the test code of your project, while keeping your main code still compatible with older Java versions.\n One goal of the increased release cadence is to shorten the feedback loop for the OpenJDK team: have developers in the field try out new functionality early on, collect feedback based on that, adjust as needed. To aid with that process, the JDK has two means of publishing preliminary work before new APIs and language features are cast in stone:\n   Incubator JDK modules\n  Preview language and VM features\n   An example for the former is the new HTTP client API, which was an incubator module in JDK 9 and 10, before it got standardized as a regular API in JDK 11. Examples for preview language features are switch expressions (added as a preview feature in Java 12) and text blocks (added in Java 13).\n Now especially text blocks are a feature which many developers have missed in Java for a long time. They are really useful when embedding other languages, or just any kind of longer text into your Java program, e.g. multi-line SQL statements, JSON documents and others. So you might want to go and use them as quickly as possible, but depending on your specific situation and requirements, you may no be able to move to Java 13 just yet.\n In particular when working on libraries, compatibility with older Java versions is a high priority in order to not cut off a large number of potential users. E.g. in the JetBrains Developer Ecosystem Survey from early 2019, 83% of participants said that Java 8 is a version they regularly use. This matches with what I\u0026#8217;ve observed myself during conversations e.g. at conferences. Now this share may have reduced a bit since then (I couldn\u0026#8217;t find any newer numbers), but at this point in time it still seems save to say that libraries should support Java 8 to not limit their audience in a signficant way.\n So while building on Java 13 is fine, requiring it at runtime for libraries isn\u0026#8217;t. Does this mean as a library author you cannot use text blocks then for many years to come? For your main code (i.e. the one shipped to users) it indeed does mean that, but things look different when it comes to test code.\n An Example One case where text blocks come in extremely handy is testing of REST APIs, where JSON requests need to created and responses may have to be compared to a JSON string with the expected value. Here\u0026#8217;s an example of using text blocks in a test of a Quarkus-based REST service, implemented using RESTAssured and JSONAssert:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @QuarkusTest public class TodoResourceTest { @Test public void canPostNewTodoAndReceiveId() throws Exception { given() .when() .body(\"\"\" (1) { \"title\" : \"Learn Java\", \"completed\" : false } \"\"\" ) .contentType(ContentType.JSON) .post(\"/hello\") .then() .statusCode(201) .body(matchesJson(\"\"\" (2) { \"id\" : 1, \"title\" : \"Learn Java\", \"completed\" : false } \"\"\") ); } }      1 Text block with the JSON request to send   2 Text block with the expected JSON response    Indeed that\u0026#8217;s much nicer to read, e.g. when comparing the request JSON to the code you\u0026#8217;d typically write without text blocks. Concatenating multiple lines, escaping quotes and explicitly specifying line breaks make this quite cumbersome:\n 1 2 3 4 5 6 .body( \"{\\n\" + \" \\\"title\\\" : \\\"Learn Java 13\\\",\\n\" + \" \\\"completed\\\" : false\\n\" + \"}\" )    Now let\u0026#8217;s see what\u0026#8217;s needed in terms of configuration to enable usage of Java 13 text blocks for tests, while keeping the main code of a project compatible with Java 8.\n   Configuration Two options of the Java compiler javac come into play here:\n   --release: specifies the Java version to compile for\n  --enable-preview: allows to use language features currently in \"preview\" status such as text blocks as of Java 13/14\n       The --release option was introduced in Java 9 and should be preferred over the more widely known pair of --source and --target. The reason being that --release will prevent any accidental usage of APIs only introduced in later versions.\n E.g. say you were to write code such as List.of(\"Foo\", \"Bar\"); the of() methods on java.util.List were only introduced in Java 9, so compiling with --release 8 will raise a compilation error in this case. When using the older options, this situation wouldn\u0026#8217;t be detected at compile time, making the problem only apparent when actually running the application on the older Java version.\n     Build tools typically allow to use different configurations for the compilation of main and test code. E.g. here is what you\u0026#8217;d use for Maven (you can find the complete source code of the example in this GitHub repo):\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 ... \u0026lt;properties\u0026gt; ... \u0026lt;maven.compiler.release\u0026gt;8\u0026lt;/maven.compiler.release\u0026gt; (1) ... \u0026lt;/properties\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; ... \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.8.1\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;default-testCompile\u0026lt;/id\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;release\u0026gt;13\u0026lt;/release\u0026gt; (2) \u0026lt;compilerArgs\u0026gt;--enable-preview\u0026lt;/compilerArgs\u0026gt; (3) \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; ... \u0026lt;/plugins\u0026gt; ... \u0026lt;/build\u0026gt; ...      1 Compile for release 8 by default, i.e. the main code   2 Compile test code for release 13   3 Also pass the --enable-preview option when compiling the test code    Also at runtime preview features must be explicitly enabled. Therefore the java command must be accordingly configured when executing the tests, e.g. like so when using the Maven Surefire plug-in:\n 1 2 3 4 5 6 7 8 9 ... \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-surefire-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.22.1\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;argLine\u0026gt;--enable-preview\u0026lt;/argLine\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; ...    With this configuration in place, text blocks can now be used in tests as the one above, but not in the main code of the program. Doing so would result in a compilation error.\n Note your IDE might still let you do this kind of mistake. At least Eclipse chose for me the maximum of main (8) and test code (13) release levels when importing the project. But running the build on the command line via Maven or on your CI server will detect this situation.\n As Java 13 now is required to build this code base, it\u0026#8217;s a good idea to make this prerequisite explicit in the build process itself. The Maven enforcer plug-in comes in handy for that, allowing to express this requirement using its Java version rule:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ... \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-enforcer-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0-M3\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;enforce-java\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;enforce\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;rules\u0026gt; \u0026lt;requireJavaVersion\u0026gt; \u0026lt;version\u0026gt;[13,)\u0026lt;/version\u0026gt; \u0026lt;/requireJavaVersion\u0026gt; \u0026lt;/rules\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; ...    The plug-in will fail the build when being run on a version before Java 13.\n   Should You Do This? Having seen how you can use preview features in test code, the question is: should you actually do this? A few things should be kept in mind for answering that. First of all, preview features are really that, a preview. This means that details may change in future Java revisions. Or, albeit unlikely, such feature may even be dropped altogether, should the JDK team arrive at the conclusion that it is fundamentally flawed.\n Another important factor is the minimum Java language version supported by the JDK compiler. As of Java 13, the oldest supported release is 7; i.e. using JDK 13, you can produce byte code that can be run with Java versions as old as Java 7. In order to keep the Java compiler maintainable, support for older versions is dropped every now and then. Right now, there\u0026#8217;s no formal process in place which would describe when support for a specific version is going to be removed (defining such policy is the goal of JEP 182).\n As per JDK developer Joe Darcy, \"there are no plans to remove support for --release 7 in JDK 15\". Conversely, this means that support for release 7 theoretically could be removed in JDK 16 and support for release 8 could be removed in JDK 17. In that case you\u0026#8217;d be caught between a rock and a hard place: Once you\u0026#8217;re on a non-LTS (\"long-term support\") release like JDK 13, you\u0026#8217;ll need to upgrade to JDK 14, 15 etc. as soon as they are out, in order to not be cut off from bug fixes and security patches. Now while doing so, you\u0026#8217;d be forced to increase the release level of your main code, once support for release 8 gets dropped, which may not desirable. Or you\u0026#8217;d have to apply some nice awk/sed magic to replace all those shiny text blocks with traditional concatenated and escaped strings, so you can go back to the current LTS release, Java 11. Not nice, but surely doable.\n That being said, this all doesn\u0026#8217;t seem like a likely scenario to me. JEP 182 expresses a desire \"that source code 10 or more years old should still be able to be compiled\"; hence I think it\u0026#8217;s save to assume that JDK 17 (the next release planned to receive long-term support) will still support release 8, which will be seven years old when 17 gets released as planned in September 2021. In that case you\u0026#8217;d be on the safe side, receiving update releases and being able to keep your main code Java 8 compatible for quite a few years to come.\n Needless to say, it\u0026#8217;s a call that you need to make, deciding for yourself wether the benefits of using new language features such as text blocks is worth it in your specific situation or not.\n  ","id":21,"publicationdate":"Jan 13, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eWhen Java 9 was introduced in 2017,\nit was the last major version published under the old release scheme.\nSince then, a \u003ca href=\"https://www.infoq.com/news/2017/09/Java6Month/\"\u003esix month release cadence\u003c/a\u003e has been adopted.\nThis means developers don\u0026#8217;t have to wait years for new APIs and language features,\nbut they can get their hands onto the latest additions twice a year.\nIn this post I\u0026#8217;d like to describe how you can try out new language features such as \u003ca href=\"http://openjdk.java.net/jeps/355\"\u003eJava 13 text blocks\u003c/a\u003e in the test code of your project,\nwhile keeping your main code still compatible with older Java versions.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Using Java 13 Text Blocks (Only) for Your Tests","uri":"https://www.morling.dev/blog/using-java-13-text-blocks-for-tests/"},{"content":"One of the long-awaited features in Quarkus was support for server-side templating: until recently, Quarkus supported only client-side web frameworks which obtain there data by calling a REST API on the backend. This has changed with Quarkus 1.1: it comes with a brand-new template engine named Qute, which allows to build web applications using server-side templates.\n When looking at frameworks for building web applications, there\u0026#8217;s two large categories:\n   client-side solutions based on JavaScript such as React, vue.js or Angular\n  server-side frameworks such as Spring Web MVC, JSF or MVC 1.0 (in the Java world)\n   Both have their indivdual strengths and weaknesses and it\u0026#8217;d be not very wise to always prefer one over the other. Instead, the choice should be based on specific requirements (e.g. what kind of interactivity is needed) and prerequisites (e.g. the skillset of the team building the application).\n Being mostly experienced with Java, server-side solutions are appealing to me, as they allow me to use the language I know and tooling (build tools, IDEs) I\u0026#8217;m familiar and most productive with. So when Qute was announced, it instantly caught my attention and I had to give it a test ride. In this post I want to share some of the experiences I made.\n Note this isn\u0026#8217;t a comprehensive tutorial for building web apps with Qute, instead, I\u0026#8217;d like to discuss a few things that stuck out to me. You can find a complete working example here on GitHub. It implements a basic CRUD application for managing personal todos, persisted in a Postgres database. Here\u0026#8217;s a video that shows the demo in action:\n    The Basics The Qute engine is based on RESTEasy/JAX-RS. As such, Qute web applications are implemented by defining resource types with methods answering to specific HTTP verbs and accept headers. The only difference being, that HTML pages are returned instead of JSON as in your typical REST-ful data API. The individual pages are created by processing template files. Here\u0026#8217;s a basic example for returning all the Todo records in our application:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 @Path(\"/todo\") public class TodoResource { @Inject Template todos; @GET (1) @Consumes(MediaType.TEXT_HTML) (2) @Produces(MediaType.TEXT_HTML) public TemplateInstance listTodos() { return todos.data(\"todos\", Todo.findAll().list()); (3) } }      1 Processes HTTP GET requests for /todo   2 This method consumes and produces the text/html media type   3 Obtain all todos from the database and feed them to the todos template    The Todo class is as JPA entity implemented via Hibernate Panache:\n 1 2 3 4 5 6 7 @Entity public class Todo extends PanacheEntity { public String title; public int priority; public boolean completed; }    Panache is a perfect fit for this kind of CRUD applications. It helps with common tasks such as id mapping, and by means of the active record pattern you get query methods like findAll() \"for free\".\n To produce an HTML page for displaying the result list, the todos template is used. Templates are located under src/main/resources/templates. As you would expect it, changes to template files are immediatly picked up when running Quarkus in Dev Mode. By default, the template name is derived from the field name of the injected Template instance, i.e. in this case the src/main/resources/templates/todos.html template will be used. It could look like this:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\"en\"\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\"utf-8\"\u0026gt; \u0026lt;!-- CSS ... --\u0026gt; \u0026lt;link rel=\"stylesheet\" href=\"...\"\u0026gt; \u0026lt;title\u0026gt;My Todos\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\"container\"\u0026gt; \u0026lt;h1\u0026gt;My Todos\u0026lt;/h1\u0026gt; \u0026lt;table class=\"table table-striped table-bordered\"\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th scope=\"col\" class=\"fit\"\u0026gt;Id\u0026lt;/th\u0026gt; \u0026lt;th scope=\"col\" \u0026gt;Title\u0026lt;/th\u0026gt; \u0026lt;th scope=\"col\" class=\"fit\"\u0026gt;Priority\u0026lt;/th\u0026gt; \u0026lt;th scope=\"col\" class=\"fit\"\u0026gt;Completed\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; {#if todos.size == 0} (1) \u0026lt;tr\u0026gt; \u0026lt;td colspan=\"4\"\u0026gt;No data found.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; {#else} {#for todo in todos} (2) \u0026lt;tr\u0026gt; \u0026lt;th scope=\"row\"\u0026gt;#{todo.id}\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt; {todo.title} (3) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; {todo.priority} (4) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; (5) \u0026lt;div class=\"custom-control custom-checkbox\"\u0026gt; \u0026lt;input type=\"checkbox\" class=\"custom-control-input\" disabled id=\"completed-{todo.id}\" {#if todo.completed}checked{/if}\u0026gt; \u0026lt;label class=\"custom-control-label\" for=\"completed-{todo.id}\"\u0026gt;\u0026lt;/label\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; {/for} {/if} \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;      1 If the injected todos list is empty, display a placeholder row   2 Otherwise, iterate over the todos list and add a table row for each one   3 Table cell for title   4 Table cell for priority   5 Table cell for completion status, rendered as a checkbox    If you\u0026#8217;ve worked with other templating engine before, this will look very familiar to you. You can refer to injected objects and their properties to display their values, have conditional logic, iterate over collections etc. A very nice aspect about Qute templates is that they are processed at build time, following the Quarkus notion of \"compile-time boot\". This means if there is an error in a template such as unbalanced control keywords, you\u0026#8217;ll find out about this at build time instead of only at runtime.\n The reference documentation describes the syntax and all options in depth. Note that things are still in flux here, e.g. I couldn\u0026#8217;t work with boolean operators in conditions.\n   Combining HTML and Data APIs Thanks to HTTP content negotiation, you can easily combine resource methods for returning HTML and JSON for API-style consumers in a single endpoint. Just add another resource method for handling the required media type, e.g. \"application/json\":\n 1 2 3 4 5 6 @GET @Produces(MediaType.APPLICATION_JSON) @Consumes(MediaType.APPLICATION_JSON) public List\u0026lt;Todo\u0026gt; listTodosJson() { return Todo.findAll().list(); }    A standard HTTP request issued by a web browser would now be answered with the HTML page, whereas an AJAX request with the \"application/json\" accept header (or a manual invocation via curl) would yield the JSON representation. I really like that idea of considering HTML and JSON-based representations as two different \"views\" of the same API essentially.\n   Template Organization If a web application has multiple pages or \"views\", chances are there are many similarities between those. E.g. there might be a common header and footer for all pages, or one and the same form is used on multiple pages.\n To avoid duplication in the templates in such cases, Qute supports the notion of includes. E.g. let\u0026#8217;s say there\u0026#8217;s a common form for creating new and editing existing todos. This can be put into its own template:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 (1) \u0026lt;form action=\"/todo/{#if update}{todo.id}/edit{#else}new{/if}\" method=\"POST\" name=\"todoForm\" enctype=\"multipart/form-data\"\u0026gt; \u0026lt;div class=\"form-row align-items-center\"\u0026gt; \u0026lt;div class=\"col-sm-3 my-1\"\u0026gt; \u0026lt;label class=\"sr-only\" for=\"title\"\u0026gt;Title\u0026lt;/label\u0026gt; (2) \u0026lt;input type=\"text\" name=\"title\" class=\"form-control\" id=\"title\" placeholder=\"Title\" required autofocus {#if update}value=\"{todo.title}\"{/if}\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"col-auto my-1\"\u0026gt; \u0026lt;select class=\"custom-select\" name=\"priority\"\u0026gt; \u0026lt;option disabled value=\"\"\u0026gt;Priority\u0026lt;/option\u0026gt; {#for prio in priorities} \u0026lt;option value=\"{prio}\" {#if todo.priority == prio}selected{/if}\u0026gt;{prio}\u0026lt;/option\u0026gt; {/for} \u0026lt;/select\u0026gt; \u0026lt;/div\u0026gt; (3) {#if update} \u0026lt;div class=\"col-auto my-1\"\u0026gt; \u0026lt;div class=\"form-check\"\u0026gt; \u0026lt;input type=\"checkbox\" name=\"completed\" class=\"form-check-input\" id=\"completed\" {#if todo.completed}checked{/if}\u0026gt; \u0026lt;label class=\"form-check-label\" for=\"completed\"\u0026gt;Completed\u0026lt;/label\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {/if} (4) \u0026lt;button type=\"submit\" class=\"btn btn-primary\"\u0026gt;{#if update}Update{#else}Create{/if}\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt;      1 Post to different path for update and create   2 Display existing title and priority in case of an update   3 Show checkbox for completion status in case of an update   4 Choose button caption depending on use case    In order to display this form right under the table with all todos, the template can simply be included like so:\n 1 2 \u0026lt;h2\u0026gt;New Todo\u0026lt;/h2\u0026gt; {#include todo-form.html}{/include}    It\u0026#8217;s also possible to extract the outer shell of multiple pages into a shared template (\"template inheritance\"). This allows to extract common headers and footers into one single template with placeholders for the inner parts.\n For that, create a template with the common outer structure:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\"en\"\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\"utf-8\"\u0026gt; \u0026lt;!-- CSS ... --\u0026gt; \u0026lt;link rel=\"stylesheet\" href=\"...\"\u0026gt; \u0026lt;title\u0026gt;{#insert title}Default Title{/}\u0026lt;/title\u0026gt; (1) \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\"container\"\u0026gt; \u0026lt;h1\u0026gt;{#insert title}Default Title{/}\u0026lt;/h1\u0026gt; (1) {#insert contents}No contents!{/} (2) \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;      1 Derived templates define a section title which will be inserted here   2 Derived templates define a section contents which will be inserted here    Other templates can then extend the base one, e.g. like so for the \"Edit Todo\" page:\n 1 2 3 4 5 6 {#include base.html} (1) {#title}Edit Todo #{todo.id}{/title} (2) {#contents} (3) {#include todo-form.html}{/include} (4) {/contents} {/include}      1 Include the base template   2 Define the title section   3 Define the contents section   4 Include the template for displaying the todo form    As so often, a balance needs to be found between extracting common parts and still being able to comprehend the overall structure without having to pursue a large number of template references. But in any case with includes and inserts Qute puts the neccessary tools into your hands.\n   Error Handling For a great user experience robust error handling is a must. E.g. might happen that a user loads the \"Edit Todo\" dialog and while they\u0026#8217;re in the process of editing, that record gets deleted by someone else. When saving, a proper error message should be displayed to the first user. Here\u0026#8217;s the resource method implementation for that:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @POST @Consumes(MediaType.MULTIPART_FORM_DATA) @Transactional @Path(\"/{id}/edit\") public Object updateTodo( @PathParam(\"id\") long id, @MultipartForm TodoForm todoForm) { Todo loaded = Todo.findById(id); (1) if (loaded == null) { (2) return error.data(\"error\", \"Todo with id \" + id + \" has been deleted after loading this form.\"); } loaded = todoForm.updateTodo(loaded); (3) return Response.status(301) (4) .location(URI.create(\"/todo\")) .build(); }      1 Load the todo record to be updated   2 If it doesn\u0026#8217;t exist, render the \"error\" template   3 Otherwise, update the record; as loaded is an attached entity, no call to persist is needed   4 redirect the user to the main page, avoiding issues with reloading etc. (post-redirect-get pattern)    Note that TemplateInstance as returned from the Template#data() method doesn\u0026#8217;t extend the JAX-RS Response class. Therefore the return type of the method must be declared as Object in this case.\n   Search Thanks to Hibernate Panache it\u0026#8217;s quite simple to refine the todo list and only return those whose title matches a given search term. Also ordering the list in some meaningful way would be nice. All we need is an optional query parameter for specifying the search term and a custom query method:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @GET @Consumes(MediaType.TEXT_HTML) @Produces(MediaType.TEXT_HTML) public TemplateInstance listTodos(@QueryParam(\"filter\") String filter) { return todos.data(\"todos\", find(filter)); } @GET @Produces(MediaType.APPLICATION_JSON) @Consumes(MediaType.APPLICATION_JSON) public List\u0026lt;Todo\u0026gt; listTodosJson(@QueryParam(\"filter\") String filter) { return find(filter); } private List\u0026lt;Todo\u0026gt; find(String filter) { Sort sort = Sort.ascending(\"completed\") (1) .and(\"priority\", Direction.Descending) .and(\"title\", Direction.Ascending); if (filter != null \u0026amp;\u0026amp; !filter.isEmpty()) { (2) return Todo.find(\"LOWER(title) LIKE LOWER(?1)\", sort, \"%\" + filter + \"%\").list(); } else { return Todo.findAll(sort).list(); (3) } }      1 First sort by completion status, then priority, then by title   2 If a filter is given, apply the search term lower-cased and with wildcards, i.e. using a WHERE clause such as where lower(todo0_.title) like lower(%searchterm%)   3 Otherwise, return all todos    To enter the search term, a form is added next to the table of todos:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 (1) \u0026lt;form action=\"/todo\" method=\"GET\" name=\"search\"\u0026gt; \u0026lt;div class=\"form-row align-items-center\"\u0026gt; \u0026lt;div class=\"col-sm-3 my-1\"\u0026gt; \u0026lt;label class=\"sr-only\" for=\"filter\"\u0026gt;Search\u0026lt;/label\u0026gt; (2) \u0026lt;input type=\"text\" name=\"filter\" class=\"form-control\" id=\"filter\" placeholder=\"Search By Title\" required {#if filtered}value=\"{filter}\"{/if}\u0026gt; \u0026lt;/div\u0026gt; (3) \u0026lt;input class=\"btn btn-primary\" value=\"Search\" type=\"submit\"\u0026gt;\u0026amp;nbsp; \u0026lt;a class=\"btn btn-secondary {#if !filtered}disabled{/if}\" href=\"/todo\" role=\"button\"\u0026gt;Clear Filter\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt;      1 Invoke this page with the entered search as query parameter   2 Input for the search term; show the previously entered term, if any   3 A button for clearing the result list if a search term has been entered; otherwise the button will be disabled      Smoother User Experience via Unpoly The last thing I wanted to explore is how the usability and performance of the application can be improved by means of some client-side enhancements. By default, a web app rendered on the server-side like ours requires full page loads when going from one page to the other. This is where single page applications (SPAs) implemented with client-side frameworks shine: just parts of the document object model tree in the browser will be replaced e.g. when loading a result list via AJAX, resulting in a much smoother and faster user experience.\n Does this mean we have to give up on server-side rendering altogether if we\u0026#8217;re after this kind of UX? Luckily not, as small helper libraries such as Unpoly, Intercooler or Turbolinks can be leveraged to replace just page fragments instead of requiring full page loads. This results in a smooth SPA-like user experience without having to opt into the full client-side programming model. For the Todo example I\u0026#8217;ve obtained great results using Unpoly. After importing its JavaScript file, all that\u0026#8217;s needed is to add the up-target attribute to links or forms.\n E.g. here\u0026#8217;s the form for entering the search term with that modification:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 (1) \u0026lt;form action=\"/todo\" method=\"GET\" name=\"search\" up-target=\".container\"\u0026gt; \u0026lt;div class=\"form-row align-items-center\"\u0026gt; \u0026lt;div class=\"col-sm-3 my-1\"\u0026gt; \u0026lt;label class=\"sr-only\" for=\"filter\"\u0026gt;Search\u0026lt;/label\u0026gt; \u0026lt;input type=\"text\" name=\"filter\" class=\"form-control\" id=\"filter\" placeholder=\"Search By Title\" required {#if filtered}value=\"{filter}\"{/if}\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;input class=\"btn btn-primary\" value=\"Search\" type=\"submit\"\u0026gt;\u0026amp;nbsp; (2) \u0026lt;a class=\"btn btn-secondary {#if !filtered}disabled{/if}\" href=\"/todo\" role=\"button\" up-target=\".container\"\u0026gt;Clear Filter\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt;      1 When receiving the result of the form submission, replace the \u0026lt;div\u0026gt; with CSS class container of the current page with the one from the response   2 Do the same when following the \"Clear Filter\" link    The magic trick of Unpoly is that links and forms with the up-target attribute are intercepted by Unpoly and executed via AJAX calls. The specified fragments from the result page are then used to replace parts of the already loaded page, instead of having the browser load the full response page. The result is the fast user experience shown in the video above.\n Unpoly also allows to show page fragments in modal dialogs, allowing to remain on the same page also when showing forms such as the one for editing a todo:\n   Note that if JavaScript is disabled, the application gracefully falls back to full page loads. I.e. it will still be fully functional, just with a slightly degraded user experience. The same would happen when accessing the edit dialog directly via its URL or when opening the \"Edit\" link in a new tab or window:\n     Bonus: Using WebJars In a thread on Twitter James Ward brought up the idea of pulling in required resources such as Bootstrap via WebJars instead of getting them from a CDN. WebJars is a useful utility for obtaining all sorts of client-side libraries with Java build tools such as Maven or Gradle.\n For Bootstrap, the following dependency must be added to the Maven pom.xml file:\n 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.webjars\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;bootstrap\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.4.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;    The Bootstrap CSS can then be included within the base.html template like so:\n 1 2 3 4 5 6 7 ... \u0026lt;head\u0026gt; ... \u0026lt;link rel=\"stylesheet\" href=\"/webjars/bootstrap/4.4.1/css/bootstrap.min.css\"\u0026gt; ... \u0026lt;/head\u0026gt; ...    This is all that\u0026#8217;s needed in order to use Bootstrap via WebJars. Note this will work on the JVM and also with a native binary via GraalVM: WebJars resources are located under META-INF/resources, and Quarkus automatically adds all resources from there when building a native image.\n   Wrap Up This concludes my quick tour through server-side web applications with Quarkus and its new Qute extension. Where only web applications based on REST APIs called by client-side web applications were supported before, Qute is a great addition to the list of Quarkus extensions, allowing to choose different architecture styles based on your needs and preferences.\n Note that Qute currently is in \"Experimental\" state, i.e. it\u0026#8217;s a great time to give it a try and share your feedback, but be prepared for possible immaturities and potential changes down the road. E.g. I noticed that complex boolean expressions in template conditions aren\u0026#8217;t support yet. Also it would be great to get build-time feedback upon invalid variable references in templates.\n To learn more, refer to the Qute guide and its reference documentation. You can find the complete source code of the Todo example including instructions for building and running in this GitHub repo.\n  ","id":22,"publicationdate":"Jan 3, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eOne of the long-awaited features in Quarkus was support for server-side templating:\nuntil recently, Quarkus supported only client-side web frameworks which obtain there data by calling a REST API on the backend.\nThis has changed with \u003ca href=\"https://quarkus.io/blog/quarkus-1-1-0-final-released/\"\u003eQuarkus 1.1\u003c/a\u003e: it comes with a brand-new template engine named \u003ca href=\"https://quarkus.io/guides/qute\"\u003eQute\u003c/a\u003e,\nwhich allows to build web applications using server-side templates.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Quarkus Qute – A Test Ride","uri":"https://www.morling.dev/blog/quarkus-qute-test-ride/"},{"content":"As a software engineer, I like to automate tedious tasks as much as possible. The deployment of this website is no exception: it is built using the Hugo static site generator and hosted on GitHub Pages; so wouldn\u0026#8217;t it be nice if the rendered website would automatically be published whenever an update is pushed to its source code repository?\n With the advent of GitHub Actions, tasks like this can easily be implemented without having to rely on any external CI service. Instead, many ready-made actions can be obtained from the GitHub marketplace and easily be configured as per our needs. E.g. triggered by a push to a specified branch in a GitHub repository, they can execute tasks like project builds, tests and many others, running in virtual machines based on Linux, Windows and even macOS. So let\u0026#8217;s see what\u0026#8217;s needed for building a Hugo website and deploying it to GitHub Pages.\n GitHub Actions To the Rescue Using my favourite search engine, I came across two GitHub actions which do everything we need:\n   GitHub Actions for Hugo\n  GitHub Actions for GitHub Pages\n   There are multiple alternatives for GitHub Pages deployment. I chose this one basically because it seems to be the most popular one (as per number of GitHub stars), and because it\u0026#8217;s by the same author as the Hugo one, so they should nicely play together.\n   Registering a Deploy Key In order for the GitHub action to deploy the website, a GitHub deploy key must be registered.\n To do so, create a new SSH key pair on your machine like so:\n ssh-keygen -t rsa -b 4096 -C \"$(git config user.email)\" -f gh-pages -N \"\"   This will create two files, the public key (gh-pages.pub) and the private key (gh-pages). Go to https://github.com/\u0026lt;your-user-or-organisation\u0026gt;/\u0026lt;your-repo\u0026gt;/settings/keys and click \"Add deploy key\". Paste in the public part of your key pair and check the \"Allow write access\" box.\n No go to https://github.com/\u0026lt;your-user-or-organisation\u0026gt;/\u0026lt;your-repo\u0026gt;/settings/secrets and click \"Add new secret\". Choose ACTIONS_DEPLOY_KEY as the name and paste the private part of your key pair into the \"Value\" field.\n The key will be stored in an encrypted way as per GitHub\u0026#8217;s documentation Nevertheless I\u0026#8217;d recommend to use a specific key pair just for this purpose, instead of re-using any other key pair. That way, impact will be reduced to this particular usage, should the private key get leaked somehow.\n   Defining the Workflow With the key in place, it\u0026#8217;s time to set up the actual GitHub Actions workflow. This is simply done by creating the file .github/workflows/gh-pages-deployment.yml in your repository with the following contents. GitHub Actions workflows are YAML files, because YOLO ;)\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 name: GitHub Pages on: (1) push: branches: - master jobs: build-deploy: runs-on: ubuntu-18.04 steps: - uses: actions/checkout@v1 (2) with: submodules: true - name: Install Ruby Dev (3) run: sudo apt-get install ruby-dev - name: Install AsciiDoctor and Rouge run: sudo gem install asciidoctor rouge - name: Setup Hugo (4) uses: peaceiris/actions-hugo@v2 with: hugo-version: '0.62.0' - name: Build (5) run: hugo - name: Deploy (6) uses: peaceiris/actions-gh-pages@v2 env: ACTIONS_DEPLOY_KEY: ${{ secrets.ACTIONS_DEPLOY_KEY }} PUBLISH_BRANCH: gh-pages PUBLISH_DIR: ./public      1 Run this action whenever changes are pushed to the master branch   2 The first step in the job: check out the source code   3 Install AsciiDoctor (in case you use Hugo with AsciiDoc files, like I do) and Rouge, a Ruby gem for syntax highlighting; I\u0026#8217;m installing the gems instead of Ubuntu packages in order to get current versions   4 Set up Hugo via the aforementioned GitHub Actions for Hugo   5 Run the hugo command; here you could add parameters such as -F for also building future posts   6 Deploy the website to GitHub pages; the contents of Hugo\u0026#8217;s build directory public will be pushed to the gh-pages branch of the upstream repository, using the deploy key configured before    And that\u0026#8217;s all we need; once the file is committed and pushed to the upstream repository, the deployment workflow will be executed upon each push to the master branch.\n You can find the complete workflow definition used for publishing this website here. Also check out the documentation of GitHub Actions for Hugo and GitHub Actions for GitHub Pages to learn more about their capabilities and the options they offer.\n  ","id":23,"publicationdate":"Dec 26, 2019","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eAs a software engineer, I like to automate tedious tasks as much as possible.\nThe deployment of this website is no exception:\nit is built using the \u003ca href=\"https://gohugo.io/\"\u003eHugo\u003c/a\u003e static site generator and hosted on \u003ca href=\"https://pages.github.com/\"\u003eGitHub Pages\u003c/a\u003e;\nso wouldn\u0026#8217;t it be nice if the rendered website would automatically be published whenever an update is pushed to its source code repository?\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Automatically Deploying a Hugo Website via GitHub Actions","uri":"https://www.morling.dev/blog/automatically-deploying-hugo-website-via-github-actions/"},{"content":"This page gives an overview over some talks I have done over the last years. I have spoken at large conferences such as QCon San Francisco, Devoxx and JavaOne, local meet-ups as well as company-internal events, covering topics such as Debezium and Change Data Capture, Bean Validation, NoSQL and more.\n If you\u0026#8217;d like me to have as a speaker at your conference or meet-up, please get in touch.\n 2020   JokerConf (online): Change data capture pipelines with Debezium and Kafka Streams\n  QConPlus (online): Serverless Search for My Blog With Java, Quarkus, \u0026amp; AWS Lambda\n  JFall (joint presentation with Andres Almiray; online): Plug-in Architectures for Java With Layrry and the Java Module System\n  Java Day Istanbul (online): Change Data Streaming Use Cases With Apache Kafka \u0026amp; Debezium\n  Great International Developer Summit (online): Change Data Capture Pipelines with Debezium and Kafka Streams\n  Kafka Summit (online): Change Data Capture Pipelines with Debezium and Kafka Streams\n  Red Hat Summit Virtual Experience: Data integration patterns for microservices with Debezium and Apache Kafka\n     2019   Nordic Coding, Kiel: Quarkus - Supersonic Subatomic Java\n  Java User Group Paderborn: Change Data Streaming Use Cases mit Debezium und Apache Kafka\n  QCon San Francisco: Practical Change Data Streaming Use Cases With Apache Kafka \u0026amp; Debezium\n  JokerConf, St. Petersburg: Practical change data streaming use cases with Apache Kafka and Debezium\n  JavaZone, Oslo: Change Data Streaming For Microservices With Apache Kafka and Debezium\n  MicroXchg, Berlin: Change Data Streaming Patterns For Microservices With Debezium\n  JavaLand, Brühl\n  Change Data Streaming für Microservices mit Debezium\n  Das Annotation Processing API - Use Cases und Best Practices\n     RivieraDev, Sophia Antipolis: Practical Change Data Streaming Use Cases With Apache Kafka and Debezium\n  Kafka Summit London: Change Data Streaming Patterns For Microservices With Debezium\n  Red Hat Summit, Boston\n  Bridging microservice boundaries with Apache Kafka and Debezium (hands-on lab)\n  Change data streaming patterns for microservices with Debezium\n     Red Hat Modern Integration and Application Development Day, Milano: Data Strategies for Microservices: Change Data Capture with Debezium\n     2018   Devoxx Morocco, Marrakesh\n  Change Data Streaming Patterns for Microservices With Debezium\n  Map me if you can! Painless bean mappings with MapStruct\n     Kafka Summit San Francisco: Change Data Streaming Patterns for Microservices With Debezium\n  VoxxedDays Microservices Paris: Data Streaming for Microservices using Debezium\n  JUG Saxony Day, Dresden: Streaming von Datenbankänderungen mit Debezium\n  Java User Group Darmstadt: Streaming von Datenbankänderungen mit Debezium\n  JavaLand, Brühl: Hibernate - State of the Union; Migrating to Java 9 Modules with ModiTect\n  RivieraDev, Sophia Antipolis: Data Streaming for Microservices using Debezium\n  Red Hat Summit, San Francisco: Running data-streaming applications with Kafka on OpenShift (hands-on lab)\n  Java User Group Münster, Streaming von Datenbankänderungen mit Debezium\n     2017   JavaZone, Oslo: Keeping Your Data Sane with Bean Validation 2.0\n  JavaOne, San Francisco\n  Keeping Your Data Sane with Bean Validation 2.0\n  NoSQL? Have it Your Way!\n     Devoxx Belgium, Antwerp\n  Streaming Database Changes with Debezium\n  Short talks on Bean Validation 2.0 and MapStruct\n     jdk.io, Copenhagen: Keeping Your Data Sane with Bean Validation 2.0\n  RivieraDev, Sophia Antipolis: Keeping Your Data Sane with Bean Validation 2.0\n  JavaLand, Brühl\n  Bean Validation 2.0\n  Hibernate Search and Elasticsearch\n        2016   JavaZone, Oslo: From Hibernate to Elasticsearch in no time\n     ","id":24,"publicationdate":"Dec 26, 2019","section":"","summary":"This page gives an overview over some talks I have done over the last years. I have spoken at large conferences such as QCon San Francisco, Devoxx and JavaOne, local meet-ups as well as company-internal events, covering topics such as Debezium and Change Data Capture, Bean Validation, NoSQL and more.\n If you\u0026#8217;d like me to have as a speaker at your conference or meet-up, please get in touch.\n 2020   JokerConf (online): Change data capture pipelines with Debezium and Kafka Streams","tags":null,"title":"Conferences","uri":"https://www.morling.dev/conferences/"},{"content":"I had the pleasure to do a few podcasts and interviews, mostly around Debezium and change data capture.\n   The InfoQ Podcast, with Wes Reisz: Gunnar Morling on Change Data Capture and Debezium\n  Data Engineering Podcast by Tobias Macey: Episode 114\u0026#8201;\u0026#8212;\u0026#8201;Change Data Capture For All Of Your Databases With Debezium; together with Randall Hauch\n  Adam Bien\u0026#8217;s airhacks.fm podcast: Episode 39\u0026#8201;\u0026#8212;\u0026#8201;Use the Most Productive Stack You Can Get\n  Adam Bien\u0026#8217;s airhacks.fm podcast: Episode 57\u0026#8201;\u0026#8212;\u0026#8201;CDC, Debezium, streaming and Apache Kafka\n  Streaming Audio: a Confluent podcast about Apache Kafka: Change Data Capture with Debezium ft. Gunnar Morling\n  Interview with Thorben Janssen for heise.de (German): Im Gespräch: Gunnar Morling über Debezium und CDC\n  Thoughts On Java: Interview with Gunnar Morling\n   ","id":25,"publicationdate":"Dec 26, 2019","section":"","summary":"I had the pleasure to do a few podcasts and interviews, mostly around Debezium and change data capture.\n   The InfoQ Podcast, with Wes Reisz: Gunnar Morling on Change Data Capture and Debezium\n  Data Engineering Podcast by Tobias Macey: Episode 114\u0026#8201;\u0026#8212;\u0026#8201;Change Data Capture For All Of Your Databases With Debezium; together with Randall Hauch\n  Adam Bien\u0026#8217;s airhacks.fm podcast: Episode 39\u0026#8201;\u0026#8212;\u0026#8201;Use the Most Productive Stack You Can Get","tags":null,"title":"Podcasts and Interviews","uri":"https://www.morling.dev/podcasts/"},{"content":"I have contributed to a wide range of open-source projects over the last years. Here\u0026#8217;s a selection of projects I have been involved with.\n  Debezium  Debezium is a platform for change data capture; it lets you stream changes out of different databases such as Postgres, MySQL, MongoDB and SQL Server into Apache Kafka. I am the current lead of the Debezium project.\n Quarkus  Quarkus is a \"Kubernetes Native Java stack tailored for OpenJDK HotSpot and GraalVM, crafted from the best of breed Java libraries and standards\". My contributions to Quarkus are centered around its extension for Kafka Streams, which I initially created.\n Bean Validation and Hibernate Validator  Bean Validation is a Java specification which lets you express constraints on object models via annotations. Originally developed at the JCP, it\u0026#8217;s now part of the Jakarta EE umbrella at the Eclipse foundation. I have been the spec lead for Bean Validation 2.0 (JSR 380) and the lead of the reference implementation Hibernate Validator.\n Other Hibernate Projects  As part of the Hibernate team, I\u0026#8217;ve contributed to different projects such as Hibernate OGM (an effort to access NoSQL stores with JPA), Hibernate Search (full-text search for domain models based on Apache Lucene and Elasticsearch) and Hibernate ORM.\n MapStruct  MapStruct is a compile-time code generator for bean-to-bean mappings. Based on annotated Java interfaces, MapStruct generates mapping code that it is fully type-safe and very efficient by avoiding any usage of reflection. I was the creator and initial project lead of MapStruct.\n Deptective and ModiTect  ModiTect is a family of Maven and Gradle plug-ins around the Java Module System, e.g. for creating module descriptors and building modular runtime images via jlink. Deptective is a plug-in for the Java compiler (javac) for enforcing package dependencies within Java projects based on a declarative architecture definition.\n   ","id":26,"publicationdate":"Dec 26, 2019","section":"","summary":"I have contributed to a wide range of open-source projects over the last years. Here\u0026#8217;s a selection of projects I have been involved with.\n  Debezium  Debezium is a platform for change data capture; it lets you stream changes out of different databases such as Postgres, MySQL, MongoDB and SQL Server into Apache Kafka. I am the current lead of the Debezium project.\n Quarkus  Quarkus is a \"","tags":null,"title":"Projects","uri":"https://www.morling.dev/projects/"},{"content":"It has been quite a while since the last post on my old personal blog; since then, I\u0026#8217;ve mostly focused on writing about my day-work on the Debezium blog as well as some posts about more general technical topics on the Hibernate team blog.\n Now recently I had some ideas for things I wanted to write about, which didn\u0026#8217;t feel like a good fit for neither of those two. So it was time to re-boot a personal blog. The previous Blogger based one really, really feels outdated by now. Plus, I also wanted to have more control over how things work, and also be able to publish a list of projects I work on, conference talks I gave etc. So I decided to build the site using Hugo, a static site generator, and also use a nice new shiny dev domain. And here we are, welcome to morling.dev!\n Stay tuned for more posts every now and then about anything related to open source, the projects I work on and software engineering in general. Onwards!\n","id":27,"publicationdate":"Dec 26, 2019","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIt has been quite a while since the last post on my old \u003ca href=\"http://musingsofaprogrammingaddict.blogspot.com/\"\u003epersonal blog\u003c/a\u003e;\nsince then, I\u0026#8217;ve mostly focused on writing about my day-work on the \u003ca href=\"https://debezium.io/blog/\"\u003eDebezium blog\u003c/a\u003e as well as \u003ca href=\"https://in.relation.to/gunnar-morling/\"\u003esome posts\u003c/a\u003e about more general technical topics on the Hibernate team blog.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Time for a New Blog","uri":"https://www.morling.dev/blog/time-for-new-blog/"},{"content":"I am an open-source software engineer, currently working for Red Hat, where I am leading the Debezium project, a platform for change data capture.\n I have been a long-time committer to multiple open-source projects, including Hibernate, MapStruct and Deptective; I also serve as the spec lead for Bean Validation 2.0 (first at the JCP, now under the Jakarta EE umbrella at the Eclipse Foundation).\n Named a Java Champion, I\u0026#8217;m regularly speaking at conferences such as QCon, JavaOne, Red Hat Summit, JavaZone, JavaLand and Kafka Summit.\n Occasionally, I blog about topics related to software engineering.\n ","id":28,"publicationdate":"Dec 25, 2019","section":"","summary":"I am an open-source software engineer, currently working for Red Hat, where I am leading the Debezium project, a platform for change data capture.\n I have been a long-time committer to multiple open-source projects, including Hibernate, MapStruct and Deptective; I also serve as the spec lead for Bean Validation 2.0 (first at the JCP, now under the Jakarta EE umbrella at the Eclipse Foundation).\n Named a Java Champion, I\u0026#8217;m regularly speaking at conferences such as QCon, JavaOne, Red Hat Summit, JavaZone, JavaLand and Kafka Summit.","tags":null,"title":"About Me","uri":"https://www.morling.dev/about/"},{"content":"","id":29,"publicationdate":"Jan 1, 0001","section":"categories","summary":"","tags":null,"title":"Categories","uri":"https://www.morling.dev/categories/"}]