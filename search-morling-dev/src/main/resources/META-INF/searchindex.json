[{"content":"","id":0,"publicationdate":"May 28, 2021","section":"blog","summary":"","tags":null,"title":"Blogs","uri":"https://www.morling.dev/blog/"},{"content":"","id":1,"publicationdate":"May 28, 2021","section":"","summary":"","tags":null,"title":"Gunnar Morling","uri":"https://www.morling.dev/"},{"content":"Over the course of the last few months, I\u0026#8217;ve had the pleasure to serve on the Kafka Summit program committee and review several hundred session abstracts for the three Summits happening this year (Europe, APAC, Americas). That\u0026#8217;s not only a big honour, but also a unique opportunity to learn what excites people currently in the Kafka eco-system (and yes, it\u0026#8217;s a fair amount of work, too ;).\n While voting on the proposals, and also generally aspiring to stay informed of what\u0026#8217;s going on in the Kafka community at large, I noticed a few repeating themes and topics which I thought would be interesting to share (without touching on any specific talks of course). At first I meant to put this out via a Twitter thread, but then it became a bit too long for that, so I decided to write this quick blog post instead. Here it goes!\n Cambrian Explosion of Connectors Apache Kafka is a great commit log and streaming platform, but of course you also need to get data into and out of it. Kafka Connect is vital for doing just that, linking data sources and sinks to the Kafka backbone. Be it integration of legacy apps and databases, external systems (e.g. IoT), data lakes, or DWHs, different CDC options (including Debezium, of course)\u0026#8201;\u0026#8212;\u0026#8201;There\u0026#8217;s connectors for everything.\n The ever-increasing number of connectors is accompanied by growing operational maturity (large-scale deployments, KC on K8s, etc.) and upcoming improvements like KIP-618 (exactly-once source connectors) or KIP-731 (rate limiting). There\u0026#8217;s so much activity within the Kafka connector eco-system, and it really sets Kafka apart from alternatives.\n   Democratization of Data Pipelines Another exciting trend is a move to self-service Kafka environments, with portals and infrastructure aimed at reducing the friction for standing up new deployments of Kafka, Connect, and related components like schema registries, while keeping track of and running everything in a safe way, e.g. when it comes to things like access control, role and schema management, (topic) naming conventions, managing data lineage and quality, ensuring compliance, privacy and operational best-practices, or observability.\n A healthy combination of in-house as well as open-source developments is happening here, and I\u0026#8217;m sure it\u0026#8217;s a field where we\u0026#8217;ll see more tools and solutions appearing in the next months and years.\n   Stream Processing for Everyone Not exactly a new trend, but definitely a growing one: more and more users appreciate the benefits of stream processing for working with their data in Kafka, filtering, transforming, enriching and aggregating it either programmatically using libraries such as Kafka Streams or Apache Flink, or in a declarative fashion, e.g. via ksqlDB or Flink SQL. Either way, small, focused stream processing apps are a true manifestation of the microservices idea\u0026#8201;\u0026#8212;\u0026#8201;have cohesive, independent application units, each focusing on one particular task and loosely coupled to each other, via Apache Kafka in this case.\n It\u0026#8217;s great to see the uptake here, including approaches for dynamic scaling based on end-to-end lag, and innovative new solutions for efficient incremental view materialization.\n   Honorable Mentions Besides these bigger trends, there\u0026#8217;s also a few more specific topics which I saw several times and which I found very interesting:\n   Tools and best practices for testing of Kafka-based applications (e.g. for creating test data or mock producers/consumers)\n  Feeding ML/AI models is becoming a popular Kafka use case; it\u0026#8217;s not my field of experience at all, but it seems like a very logical choice to run ML algorithms on data ingested via Kafka, allowing to gain new insight into business data with a low latency\n  Pushing data to consumers via GraphQL; (still?) even more niche probably, but I love the idea of push updates to browsers based on data from Kafka; this should allow for some interesting use cases\n   Of course there\u0026#8217;s also things like geo-replicated Kafka, the ongoing move towards managed Kafka service offerings (which raises interesting questions around connectivity to on-prem systems and data), architectural trends like data meshes, and so much more.\n If you want to learn more about these and many other facets of Apache Kafka, its use cases, best practices, and latest developments, make sure to register for Kafka Summit (it\u0026#8217;s free and online). The sessions from the Europe run can already be watched, while the APAC (July 27 - 28) and Americas (September 14 - 15) editions are still to come.\n  ","id":2,"publicationdate":"May 28, 2021","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eOver the course of the last few months, I\u0026#8217;ve had the pleasure to serve on the \u003ca href=\"https://www.kafka-summit.org/\"\u003eKafka Summit\u003c/a\u003e program committee and review several hundred session abstracts for the three Summits happening this year (Europe, APAC, Americas).\nThat\u0026#8217;s not only a big honour, but also a unique opportunity to learn what excites people currently in the Kafka eco-system\n(and yes, it\u0026#8217;s a fair amount of work, too ;).\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eWhile voting on the proposals, and also generally aspiring to stay informed of what\u0026#8217;s going on in the Kafka community at large, I noticed a few repeating themes and topics which I thought would be interesting to share\n(without touching on any specific talks of course).\nAt first I meant to put this out via a Twitter thread, but then it became a bit too long for that, so I decided to write this quick blog post instead.\nHere it goes!\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Three Plus Some Lovely Kafka Trends","uri":"https://www.morling.dev/blog/three-plus-some-lovely-kafka-trends/"},{"content":"Sometimes, less is more. One case where that\u0026#8217;s certainly true is dependencies. And so it shouldn\u0026#8217;t come at a surprise that the Apache Kafka community is eagerly awaiting the removal of the dependency to the ZooKeeper service, which currently is used for storing Kafka metadata (e.g. about topics and partitions) as well as for the purposes of leader election in the cluster.\n The Kafka improvement proposal KIP-500 (\"Replace ZooKeeper with a Self-Managed Metadata Quorum\") promises to make life better for users in many regards:\n   Better getting started and operational experience by requiring to run only one system, Kafka, instead of two\n  Removing potential for discrepancies of metadata state between ZooKeeper and the Kafka controller\n  Simplifying configuration, for instance when it comes to security\n  Better scalability, e.g. in terms of number of partitions; faster execution of operations like topic creation\n   With KIP-500, Kafka itself will store all the required metadata in an internal Kafka topic, and controller election will be done amongst (a subset of) the Kafka cluster nodes themselves, based on a variant of the Raft protocol for distributed consensus. Removing the ZooKeeper dependency is great not only for running Kafka clusters in production, also for local development and testing being able to start up a Kafka node with a single process comes in very handy.\n Having been in the works for multiple years, ZK-less Kafka, also known as KRaft (\"Kafka Raft metadata mode\"), was recently published as an early access feature with Kafka 2.8. I.e. the perfect time to get my hands on this and get a first feeling for ZK-less Kafka myself. Note this post isn\u0026#8217;t meant to be a thorough evaluation or systematic testing of the new Kafka deployment mode, rather take it as a description of how to get started with playing with ZK-less Kafka and of a few observations I made while doing so.\n In the world of ZK-less Kafka, there\u0026#8217;s two node roles for nodes: controller and broker. Each node in the cluster can have either one or both roles (\"combined nodes\"). All controller nodes elect the active controller, which is in charge of coordinating the whole cluster, with other controller nodes acting as hot stand-by replicas. In the KRaft KIPs, the active controller sometimes also is simply referred to as leader. This may appear confusing at first, if you are familiar with the existing concept of partition leaders. It started to make sense to me once I realized that the active controller is the leader of the sole partition of the metadata topic. All broker nodes are handling client requests, just as before with ZooKeeper.\n While for smaller clusters it is expected that the majority of, or even all cluster nodes act as controllers, you may have dedicated controller-only nodes in larger clusters, e.g. 3 controller nodes and 7 broker nodes in a cluster of 10 nodes overall. As per the KRaft README, having dedicated controller nodes should increase overall stability, as for instance an out-of-memory error on a broker wouldn\u0026#8217;t impact controllers, or potentially even cause a leader re-election.\n Trying ZK-less Kafka Yourself As a foundation, I\u0026#8217;ve created a variant of the Debezium 1.6 container image, which updates Kafka from 2.7 to Kafka 2.8, and also does the required changes to the entrypoint script for using the KRaft mode. Note this change hasn\u0026#8217;t been merged yet to the upstream Debezium repository, so if you\u0026#8217;d like to try out things by yourself, you\u0026#8217;ll have to clone my repo, and then build the container image yourself like this:\n $ git clone git@github.com:gunnarmorling/docker-images.git $ cd docker-images/kafka/1.6 $ docker build -t debezium/zkless-kafka:1.6 --build-arg DEBEZIUM_VERSION=1.6.0 .   In order to start the image with Kafka in KRaft mode, the CLUSTER_ID environment variable must be set. A value can be obtained using the new bin/kafka-storage.sh script; going forward, we\u0026#8217;ll likely add an option to the Debezium Kafka container image for doing so. If that variable is set, the entrypoint script of the image does the following things:\n   Use config/kraft/server.properties instead of config/server.properties as the Kafka configuration file; this one comes with the Kafka distribution and is meant for nodes which should have both the controller and broker roles; i.e. the container image currently only supports combined nodes\n  Format the node\u0026#8217;s storage directory, if not the case yet\n  Set up a listener for controller communication\n   Based on that, here is what\u0026#8217;s needed in a Docker Compose file for spinning up a Kafka cluster with three nodes:\n version: '2' services: kafka-1: image: debezium/zkless-kafka:1.6 ports: - 19092:9092 - 19093:9093 environment: - CLUSTER_ID=oh-sxaDRTcyAr6pFRbXyzA (1) - BROKER_ID=1 (2) - KAFKA_CONTROLLER_QUORUM_VOTERS=1@kafka-1:9093,2@kafka-2:9093,3@kafka-3:9093 (3) kafka-2: image: debezium/zkless-kafka:1.6 ports: - 29092:9092 - 29093:9093 environment: - CLUSTER_ID=oh-sxaDRTcyAr6pFRbXyzA (1) - BROKER_ID=2 (2) - KAFKA_CONTROLLER_QUORUM_VOTERS=1@kafka-1:9093,2@kafka-2:9093,3@kafka-3:9093 (3) kafka-3: image: debezium/zkless-kafka:1.6 ports: - 39092:9092 - 39093:9093 environment: - CLUSTER_ID=oh-sxaDRTcyAr6pFRbXyzA (1) - BROKER_ID=3 (2) - KAFKA_CONTROLLER_QUORUM_VOTERS=1@kafka-1:9093,2@kafka-2:9093,3@kafka-3:9093 (3)     1 Cluster id; must be the same for all the nodes   2 Broker id; must be unique for each node   3 Addresses of all the controller nodes in the format id1@host1:port1,id2@host2:port2\u0026#8230;\u0026#8203;    No ZooKeeper nodes, yeah :)\n Working on Debezium, and being a Kafka Connect aficionado allaround, I\u0026#8217;m also going to add Connect and a Postgres database for testing purposes (you can find the complete Compose file here):\n version: '2' services: # ... connect: image: debezium/connect:1.6 ports: - 8083:8083 links: - kafka-1 - kafka-2 - kafka-3 - postgres environment: - BOOTSTRAP_SERVERS=kafka-1:9092 - GROUP_ID=1 - CONFIG_STORAGE_TOPIC=my_connect_configs - OFFSET_STORAGE_TOPIC=my_connect_offsets - STATUS_STORAGE_TOPIC=my_connect_statuses postgres: image: debezium/example-postgres:1.6 ports: - 5432:5432 environment: - POSTGRES_USER=postgres - POSTGRES_PASSWORD=postgres   Now let\u0026#8217;s start everything:\n $ docker-compose -f docker-compose-zkless-kafka.yaml up   Let\u0026#8217;s also register an instance of the Debezium Postgres connector, which will connect to the PG database and take an initial snapshot, so we got some topics with a few messages to play with:\n $ curl -0 -v -X POST http://localhost:8083/connectors \\ -H \"Expect:\" \\ -H 'Content-Type: application/json; charset=utf-8' \\ --data-binary @- \u0026lt;\u0026lt; EOF{ \"name\": \"inventory-connector\", \"config\": { \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\", \"tasks.max\": \"1\", \"database.hostname\": \"postgres\", \"database.port\": \"5432\", \"database.user\": \"postgres\", \"database.password\": \"postgres\", \"database.dbname\" : \"postgres\", \"database.server.name\": \"dbserver1\", \"schema.include\": \"inventory\", \"topic.creation.default.replication.factor\": 2, \"topic.creation.default.partitions\": 10 } } EOF   Note how this is using a replication factor of 2 for all the topics created via Kafka Connect, which will come in handy for some experimenting later on.\n The nosy person I am, I first wanted to take a look into that new internal metadata topic, where all the cluster metadata is stored. As per the release announcement, it should be named @metadata. But no such topic shows up when listing the available topics; only the __consumer_offsets topic, the change data topics created by Debezium, and some Kafka Connect specific topics are shown:\n # Get a shell on one of the broker containers $ docker-compose -f docker-compose-zkless-kafka.yaml exec kafka-1 bash # In that shell $ /kafka/bin/kafka-topics.sh --bootstrap-server kafka-3:9092 --list __consumer_offsets dbserver1.inventory.customers dbserver1.inventory.geom dbserver1.inventory.orders dbserver1.inventory.products dbserver1.inventory.products_on_hand dbserver1.inventory.spatial_ref_sys my_connect_configs my_connect_offsets my_connect_statuses   Seems that this topic is truly meant to be internal; also trying to consume messages from the topic with kafka-console-consumer.sh or kafkacat fails due to the invalid topic name. Let\u0026#8217;s see whether things are going to change here, since KIP-595 (\"A Raft Protocol for the Metadata Quorum\") explicitly mentions the ability for consumers to \"read the contents of the metadata log for debugging purposes\".\n In the meantime, we can take a look at the contents of the metadata topic using the kafka-dump-log.sh utility, e.g. filtering out all RegisterBroker records:\n $ /kafka/bin/kafka-dump-log.sh --cluster-metadata-decoder \\ --skip-record-metadata \\ --files /kafka/data//\\@metadata-0/*.log | grep REGISTER_BROKER payload: {\"type\":\"REGISTER_BROKER_RECORD\",\"version\":0,\"data\":{\"brokerId\":3,\"incarnationId\":\"O_PiUrjNTsqVEQv61gB2Vg\",\"brokerEpoch\":0,\"endPoints\":[{\"name\":\"PLAINTEXT\",\"host\":\"172.18.0.2\",\"port\":9092,\"securityProtocol\":0}],\"features\":[],\"rack\":null}} payload: {\"type\":\"REGISTER_BROKER_RECORD\",\"version\":0,\"data\":{\"brokerId\":1,\"incarnationId\":\"FbOZdz9rSZqTyuSKr12JWg\",\"brokerEpoch\":2,\"endPoints\":[{\"name\":\"PLAINTEXT\",\"host\":\"172.18.0.3\",\"port\":9092,\"securityProtocol\":0}],\"features\":[],\"rack\":null}} payload: {\"type\":\"REGISTER_BROKER_RECORD\",\"version\":0,\"data\":{\"brokerId\":2,\"incarnationId\":\"ZF_WQqk_T5q3l1vhiWT_FA\",\"brokerEpoch\":4,\"endPoints\":[{\"name\":\"PLAINTEXT\",\"host\":\"172.18.0.4\",\"port\":9092,\"securityProtocol\":0}],\"features\":[],\"rack\":null}} ...   The individual record formats are described in KIP-631 (\"The Quorum-based Kafka Controller\").\n Another approach would be to use a brand-new tool, kafka-metadata-shell.sh. Also defined in KIP-631, this utility script allows to browse a cluster\u0026#8217;s metadata, similarly to zookeeper-shell.sh known from earlier releases. For instance, you can list all brokers and get the metadata of the registration of node 1 like this:\n $ /kafka/bin/kafka-metadata-shell.sh --snapshot /kafka/data/@metadata-0/00000000000000000000.log Loading... Starting... [ Kafka Metadata Shell ] \u0026gt;\u0026gt; ls brokers configs local metadataQuorum topicIds topics \u0026gt;\u0026gt; ls brokers 1 2 3 \u0026gt;\u0026gt; cd brokers/1 \u0026gt;\u0026gt; cat registration RegisterBrokerRecord(brokerId=1, incarnationId=TmM_u-_cQ2ChbUy9NZ9wuA, brokerEpoch=265, endPoints=[BrokerEndpoint(name='PLAINTEXT', host='172.18.0.3', port=9092, securityProtocol=0)], features=[], rack=null) \u0026gt;\u0026gt;   Or to display the current leader:\n \u0026gt;\u0026gt; cat /metadataQuorum/leader MetaLogLeader(nodeId=1, epoch=12)   Or to show the metadata of a specific topic partition:\n \u0026gt;\u0026gt; cat /topics/dbserver1.inventory.customers/0/data { \"partitionId\" : 0, \"topicId\" : \"8xjqykVRT_WpkqbXHwbeCA\", \"replicas\" : [ 2, 3 ], \"isr\" : [ 2, 3 ], \"removingReplicas\" : null, \"addingReplicas\" : null, \"leader\" : 2, \"leaderEpoch\" : 0, \"partitionEpoch\" : 0 } \u0026gt;\u0026gt;   Those are just a few of the things you can do with kafka-metadata-shell.sh, and it surely will be an invaluable tool in the box of administrators in the ZK-less era. Another new tool is kafka-cluster.sh, which currently can do two things: displaying the unique id of a cluster, and unregistering a broker. While the former worked for me:\n $ /kafka/bin/kafka-cluster.sh cluster-id --bootstrap-server kafka-1:9092 Cluster ID: oh-sxaDRTcyAr6pFRbXyzA   The latter always failed with a NotControllerException, no matter on which node I invoked the command:\n $ /kafka/bin/kafka-cluster.sh unregister --bootstrap-server kafka-1:9092 --id 3 [2021-05-15 20:52:54,626] ERROR [AdminClient clientId=adminclient-1] Unregister broker request for broker ID 3 failed: This is not the correct controller for this cluster.   It\u0026#8217;s not quite clear to me whether I did something wrong, or whether this functionality simply should not be expected to be supported just yet.\n The Raft-based metadata quorum also comes with a set of new metrics (described in KIP-595), allowing to retrieve information like the current active controller, role of the node at hand, and more. Here\u0026#8217;s a screenshot of the metrics invoked on a non-leader node:\n     Taking Brokers Down An essential aspect to any distributed system like Kafka is the fact that invidual nodes of a cluster can disappear at any time, be it due to failures (node crashes, network splits, etc.), or due to controlled shut downs, e.g. for a version upgrade. So I was curious how Kafka in KRaft mode would deal with the situation where nodes in the cluster are stopped and then restarted. Note I\u0026#8217;m stopping nodes gracefully via docker-compose stop, instead of randomly crashing them, Jepsen-style ;)\n The sequence of events I was testing was the following:\n   Stop the current active controller, so two nodes from the original three-node cluster remain\n  Stop the then new active controller node, at which point the majority of cluster nodes isn\u0026#8217;t available any longer\n  Start both nodes again\n   Here\u0026#8217;s a few noteworthy things I observed. As you\u0026#8217;d expect, when stopping the active controller, a new leader was elected (as per the result of cat /metadataQuorum/leader in the Kafka metadata shell), and also all partitions which had the previous active controller as partition leader, got re-assigned (in this case node 1 was the active controller and got stopped):\n $ /kafka/bin/kafka-topics.sh --bootstrap-server kafka-2:9092 --describe --topic dbserver1.inventory.customers Topic: dbserver1.inventory.customers\tTopicId: a6qzjnQwQ2eLNSXL5svW8g\tPartitionCount: 10\tReplicationFactor: 2\tConfigs: segment.bytes=1073741824 Topic: dbserver1.inventory.customers\tPartition: 0\tLeader: 1\tReplicas: 1,3\tIsr: 1,3 Topic: dbserver1.inventory.customers\tPartition: 1\tLeader: 1\tReplicas: 3,1\tIsr: 1,3 Topic: dbserver1.inventory.customers\tPartition: 2\tLeader: 1\tReplicas: 1,2\tIsr: 1,2 Topic: dbserver1.inventory.customers\tPartition: 3\tLeader: 1\tReplicas: 2,1\tIsr: 1,2 Topic: dbserver1.inventory.customers\tPartition: 4\tLeader: 1\tReplicas: 2,1\tIsr: 1,2 Topic: dbserver1.inventory.customers\tPartition: 5\tLeader: 2\tReplicas: 3,2\tIsr: 2,3 Topic: dbserver1.inventory.customers\tPartition: 6\tLeader: 2\tReplicas: 3,2\tIsr: 2,3 Topic: dbserver1.inventory.customers\tPartition: 7\tLeader: 2\tReplicas: 2,3\tIsr: 2,3 Topic: dbserver1.inventory.customers\tPartition: 8\tLeader: 1\tReplicas: 2,1\tIsr: 1,2 Topic: dbserver1.inventory.customers\tPartition: 9\tLeader: 2\tReplicas: 3,2\tIsr: 2,3 # After stopping node 1 $ /kafka/bin/kafka-topics.sh --bootstrap-server kafka-2:9092 --describe --topic dbserver1.inventory.customers Topic: dbserver1.inventory.customers\tTopicId: a6qzjnQwQ2eLNSXL5svW8g\tPartitionCount: 10\tReplicationFactor: 2\tConfigs: segment.bytes=1073741824 Topic: dbserver1.inventory.customers\tPartition: 0\tLeader: 3\tReplicas: 1,3\tIsr: 3 Topic: dbserver1.inventory.customers\tPartition: 1\tLeader: 3\tReplicas: 3,1\tIsr: 3 Topic: dbserver1.inventory.customers\tPartition: 2\tLeader: 2\tReplicas: 1,2\tIsr: 2 Topic: dbserver1.inventory.customers\tPartition: 3\tLeader: 2\tReplicas: 2,1\tIsr: 2 Topic: dbserver1.inventory.customers\tPartition: 4\tLeader: 2\tReplicas: 2,1\tIsr: 2 Topic: dbserver1.inventory.customers\tPartition: 5\tLeader: 2\tReplicas: 3,2\tIsr: 2,3 Topic: dbserver1.inventory.customers\tPartition: 6\tLeader: 2\tReplicas: 3,2\tIsr: 2,3 Topic: dbserver1.inventory.customers\tPartition: 7\tLeader: 2\tReplicas: 2,3\tIsr: 2,3 Topic: dbserver1.inventory.customers\tPartition: 8\tLeader: 2\tReplicas: 2,1\tIsr: 2 Topic: dbserver1.inventory.customers\tPartition: 9\tLeader: 2\tReplicas: 3,2\tIsr: 2,3   Things got interesting though when also stopping the newly elected leader subsequently. At this point, the cluster isn\u0026#8217;t in a healthy state any longer, as no majority of nodes of the cluster is available for leader election. Logs of the remaining node are flooded with an UnknownHostException in this situation:\n kafka-3_1 | 2021-05-16 10:16:45,282 - WARN [kafka-raft-outbound-request-thread:NetworkClient@992] - [RaftManager nodeId=3] Error connecting to node kafka-2:9093 (id: 2 rack: null) kafka-3_1 | java.net.UnknownHostException: kafka-2 kafka-3_1 | at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797) kafka-3_1 | at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1505) kafka-3_1 | at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1364) kafka-3_1 | at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1298) kafka-3_1 | at org.apache.kafka.clients.DefaultHostResolver.resolve(DefaultHostResolver.java:27) kafka-3_1 | at org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:111) kafka-3_1 | at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:512) kafka-3_1 | at org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:466) kafka-3_1 | at org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:172) kafka-3_1 | at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:985) kafka-3_1 | at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:311) kafka-3_1 | at kafka.common.InterBrokerSendThread.$anonfun$sendRequests$1(InterBrokerSendThread.scala:103) kafka-3_1 | at kafka.common.InterBrokerSendThread.$anonfun$sendRequests$1$adapted(InterBrokerSendThread.scala:99) kafka-3_1 | at scala.collection.Iterator.foreach(Iterator.scala:943) kafka-3_1 | at scala.collection.Iterator.foreach$(Iterator.scala:943) kafka-3_1 | at scala.collection.AbstractIterator.foreach(Iterator.scala:1431) kafka-3_1 | at scala.collection.IterableLike.foreach(IterableLike.scala:74) kafka-3_1 | at scala.collection.IterableLike.foreach$(IterableLike.scala:73) kafka-3_1 | at scala.collection.AbstractIterable.foreach(Iterable.scala:56) kafka-3_1 | at kafka.common.InterBrokerSendThread.sendRequests(InterBrokerSendThread.scala:99) kafka-3_1 | at kafka.common.InterBrokerSendThread.pollOnce(InterBrokerSendThread.scala:73) kafka-3_1 | at kafka.common.InterBrokerSendThread.doWork(InterBrokerSendThread.scala:94) kafka-3_1 | at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)   Here I think it\u0026#8217;d be great to get a more explicit indication in the logs of what\u0026#8217;s going on, clearly indicating the unhealthy status of the cluster at large.\n What\u0026#8217;s also interesting is that the remaining node claims to be a leader as per its exposed metrics and value of /metadataQuorum/leader in the metadata shell. This seems a bit dubious, as no leader election can happen without the majority of nodes available. Consequently, creation of a topic in this state also times out, so I suspect this is more an artifact of displaying the cluster state rather than of what\u0026#8217;s actually going on.\n Things get a bit more troublesome when restarting the two stopped nodes; Very often I\u0026#8217;d then see a very high CPU consumption on the Kafka nodes as well as the Connect node:\n $ docker stats CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS 642eb697fed6 tutorial_connect_1 122.04% 668.3MiB / 7.775GiB 8.39% 99.7MB / 46.9MB 131kB / 106kB 47 5d9806526f92 tutorial_kafka-1_1 9.24% 386.4MiB / 7.775GiB 4.85% 105kB / 104kB 0B / 877kB 93 767e6c0f6cd3 tutorial_kafka-3_1 176.40% 739.2MiB / 7.775GiB 9.28% 14.5MB / 40.6MB 0B / 1.52MB 120 a0ce8438557f tutorial_kafka-2_1 87.51% 567.8MiB / 7.775GiB 7.13% 6.52MB / 24.9MB 0B / 881kB 95 df978d220132 tutorial_postgres_1 0.00% 36.39MiB / 7.775GiB 0.46% 243kB / 5.49MB 0B / 79.4MB 9   In some cases stopping and restarting the Kafka nodes would help, other times only a restart of the Connect node would mitigate the situation. I didn\u0026#8217;t further explore this issue by taking a thread dump, but I suppose threads are stuck in some kind of busy spin loop at this point. The early access state of KRaft mode seems to be somewhat showing here. After bringing up the issue on the Kafka mailing list, I\u0026#8217;ve logged KAFKA-12801 for this problem, as it seems not to have been tracked before.\n On the bright side, once all brokers were up and running again, the cluster and the Debezium connector would happily continue their work.\n   Wrap-Up Not many features have been awaited by the Kafka community as eagerly as the removal of the ZooKeeper dependency. Rightly so: Kafka-based metadata storage and leader election will greatly simplify the operational burden for running Kafka and also allow for better scalability. Lifting the requirement for running separate ZooKeeper processes or even machines should also help to make things more cost-effective, so you should benefit from this change no matter whether you\u0026#8217;re running Kafka yourself or are using a managed service offering.\n The early access release of ZK-less Kafka in version 2.8 gives a first impression of what will hopefully be the standard way of running Kafka in the not too distant future. As very clearly stated in the KRaft README, you should not use this in production yet; this matches with the observerations made above: while running Kafka without ZooKeeper definitely feels great, there\u0026#8217;s still some rough edges to be sorted out. Also check out the README for a list of currently missing features, such as support of transactions, adding partitions to existing topics, partition reassignment, and more. Lastly, any distributed system should only be fully trusted after going through the grinder of the Jepsen test suite, which I\u0026#8217;m sure will only be a question of time.\n Despite the early state, I would very much recommend to get started testing ZK-less Kafka at this point, so to get a feeling for it and of course to report back any findings and insights. To do so, either download the upstream Kafka distribution, or build the Debezium 1.6 container image for Kafka with preliminary support for KRaft mode, which lets you set up a ZK-less Kafka cluster in no time.\n In order to learn more about ZK-less Kafka, besides diving into the relevant KIPs (which all are linked from the umbrella KIP-500), also check out the QCon talk \"Kafka Needs No Keeper\" by Colin McCabe, one of the main engineers driving this effort.\n  ","id":3,"publicationdate":"May 17, 2021","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eSometimes, less is more.\nOne case where that\u0026#8217;s certainly true is dependencies.\nAnd so it shouldn\u0026#8217;t come at a surprise that the \u003ca href=\"https://kafka.apache.org/\"\u003eApache Kafka\u003c/a\u003e community is eagerly awaiting the removal of the dependency to the \u003ca href=\"https://zookeeper.apache.org/\"\u003eZooKeeper\u003c/a\u003e service,\nwhich currently is used for storing Kafka metadata (e.g. about topics and partitions) as well as for the purposes of leader election in the cluster.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe Kafka improvement proposal \u003ca href=\"https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum\"\u003eKIP-500\u003c/a\u003e\n(\"Replace ZooKeeper with a Self-Managed Metadata Quorum\")\npromises to make life better for users in many regards:\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"ulist\"\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eBetter getting started and operational experience by requiring to run only one system, Kafka, instead of two\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRemoving potential for discrepancies of metadata state between ZooKeeper and the Kafka controller\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSimplifying configuration, for instance when it comes to security\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBetter scalability, e.g. in terms of number of partitions; faster execution of operations like topic creation\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e","tags":null,"title":"Exploring ZooKeeper-less Kafka","uri":"https://www.morling.dev/blog/exploring-zookeeper-less-kafka/"},{"content":"One of the ultimate strengths of Java is its strong notion of backwards compatibility: Java applications and libraries built many years ago oftentimes run without problems on current JVMs, and the compiler of current JDKs can produce byte code, that is executable with earlier Java versions.\n For instance, JDK 16 supports byte code levels going back as far as to Java 1.7; But: hic sunt dracones. The emitted byte code level is just one part of the story. It\u0026#8217;s equally important to consider which APIs of the JDK are used by the compiled code, and whether they are available in the targeted Java runtime version. As an example, let\u0026#8217;s consider this simple \"Hello World\" program:\n package com.example; import java.util.List; public class HelloWorld { public static void main(String... args) { System.out.println(List.of(\"Hello\", \"World!\")); } }   Let\u0026#8217;s assume we\u0026#8217;re using Java 16 for compiling this code, aiming for compatibility with Java 1.8. Historically, the Java compiler has provided the --source and --target options for this purpose, which are well known to most Java developers:\n $ javac --source 1.8 --target 1.8 -d classes HelloWorld.java warning: [options] bootstrap class path not set in conjunction with -source 8 1 warning   This compiles successfully (we\u0026#8217;ll come back on that warning in a bit). But if you actually try to run that class on Java 8, you\u0026#8217;re in for a bad suprise:\n $ java -classpath classes com.example.HelloWorld Exception in thread \"main\" java.lang.NoSuchMethodError: ↩ java.util.List.of(Ljava/lang/Object;Ljava/lang/Object;)Ljava/util/List; at com.example.HelloWorld.main(HelloWorld.java:7)   This makes sense: the List.of() methods were only introduced in Java 9, so they are not present in the Java 8 API. Shouldn\u0026#8217;t the compiler have let us know us about this? Absolutely, and that\u0026#8217;s where this warning about the bootstrap class path is coming in: the compiler recognized our potentially dangerous endavour and essentially suggested to compile against the class library matching the targeted Java version instead of that one of the JDK used for compilation. This is done using the -Xbootclasspath option:\n $ javac --source 1.8 --target 1.8 \\ -d classes \\ -Xbootclasspath:${JAVA_8_HOME}/jre/lib/rt.jar \\ (1) HelloWorld.java HelloWorld.java:7: error: cannot find symbol System.out.println(List.of(\"Hello\", \"World!\")); ^ symbol: method of(String,String) location: interface List 1 error     1 Path to the rt.jar of Java 8    That\u0026#8217;s much better: now the invocation of the List.of() method causes compilation to fail, instead of finding out about this problem only during testing, or worse, in production.\n While this approach works, it\u0026#8217;s not without issues: requiring the target Java version\u0026#8217;s class library complicates things quite a bit; multiple Java versions need to be installed, and the targeted JDK\u0026#8217;s location must be known, which for instance tends to make build processes not portable between different machines and platforms.\n Luckily, Java 9 improved things significantly here; by means of the new --release option, code can be compiled for older Java versions in a fully safe and portable way. Let\u0026#8217;s give this a try:\n $ javac --release 8 -d classes HelloWorld.java HelloWorld.java:7: error: cannot find symbol System.out.println(List.of(\"Hello\", \"World!\")); ^ symbol: method of(String,String) location: interface List 1 error   Very nice, the same compilation error as before, but without the need for any complex configuration besides the --release 8 option. So how does this work? Does the JDK come with full class libraries of all the earlier Java versions which it supports? Considering that the modules file of Java 16 has a size of more than one hundred megabytes (to be precise, 118 MB on macOS), that\u0026#8217;d clearly be not a good idea; We\u0026#8217;d end up with a JDK size of nearly one gigabyte.\n What\u0026#8217;s happening instead is that the JDK ships \"stripped-down class files corresponding to class files from the target platform versions\", as we can read in JEP 247 (\"Compile for Older Platform Versions\"), which introduced the --release option. Details about the implementation are sparse, though. The JEP only mentions a ZIP file named ct.sym which contains those signature files. So I started by taking a look at what\u0026#8217;s in there:\n $ unzip -l $JAVA_HOME/lib/ct.sym Archive: /Library/Java/JavaVirtualMachines/jdk-16.sdk/Contents/Home/lib/ct.sym Length Date Time Name --------- ---------- ----- ---- 0 03-26-2021 18:11 7/java.base/java/awt/peer/ 2557 03-26-2021 18:11 7/java.base/java/awt/peer/ComponentPeer.sig 542 03-26-2021 18:11 7/java.base/java/awt/peer/FramePeer.sig ... 856 03-26-2021 18:11 879A/java.activation/javax/activation/ActivationDataFlavor.sig 491 03-26-2021 18:11 879A/java.activation/javax/activation/CommandInfo.sig 299 03-26-2021 18:11 879A/java.activation/javax/activation/CommandObject.sig ... 1566 03-26-2021 18:11 9ABCDE/java.base/java/lang/Byte.sig 1616 03-26-2021 18:11 9ABCDE/java.base/java/lang/Short.sig ...   That\u0026#8217;s interesting, lots of *.sig files, organized in some at first odd-looking directory structure. So let\u0026#8217;s see what\u0026#8217;s there for the java.util.List class:\n $ unzip -l $JAVA_HOME/lib/ct.sym | grep \"java/util/List.sig\" 1481 03-26-2021 18:11 7/java.base/java/util/List.sig 1771 03-26-2021 18:11 8/java.base/java/util/List.sig 4040 03-26-2021 18:11 9/java.base/java/util/List.sig 4184 03-26-2021 18:11 A/java.base/java/util/List.sig 4097 03-26-2021 18:11 BCDEF/java.base/java/util/List.sig   Five different versions altogether, under the directories 7, 8, 9, A, and BCDEF. It took a few moments until the structure began to make sense to me: the top-level directory names encode Java version(s), and there\u0026#8217;s a new version of the signature file whenever its API changed. I.e. java.util.List changed in Java 7, 8, 9, 10 (A), and 11 (B), and has remained stable since then, i.e. from version 11 to 16, there have been no changes to the public List API.\n So let\u0026#8217;s dive in a bit further and compare the signature files of Java 8 and 9. As JEP 247 states that these files are (stripped-down) class files, we should be able to examine them using javap. In order to so, I had to change the file extensions from *.sig to *.class, though. After that, I could decompile the files using javap, save the result in text files and compare them using git:\n $ javap List8.class \u0026gt; List8.txt $ javap List9.class \u0026gt; List9.txt $ git diff --no-index List8.txt List9.txt diff --git a/List8.txt b/List9.txt index b2ca320..b276286 100644 --- a/List8.txt +++ b/List9.txt @@ -27,4 +27,16 @@ public interface java.util.List\u0026lt;E\u0026gt; extends java.util.Collection\u0026lt;E\u0026gt; { public abstract java.util.ListIterator\u0026lt;E\u0026gt; listIterator(int); public abstract java.util.List\u0026lt;E\u0026gt; subList(int, int); public default java.util.Spliterator\u0026lt;E\u0026gt; spliterator(); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E, E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E, E, E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E, E, E, E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E, E, E, E, E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E, E, E, E, E, E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E, E, E, E, E, E, E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E, E, E, E, E, E, E, E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E, E, E, E, E, E, E, E, E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E, E, E, E, E, E, E, E, E, E); + public static \u0026lt;E\u0026gt; java.util.List\u0026lt;E\u0026gt; of(E...); }   As expected, the diff between the two signature files reveals the addition of the different List.of() methods in Java 9, as such exactly the reason why the Hello World example from the beginning cannot be executed on Java 8.\n     Debugging the Java Compiler In order to understand in detail how the ct.sym file is used by the Java compiler, it can be useful to run javac in debug mode. As javac is written in Java itself, this can be done exactly the same way as when remote debugging any other Java application. You only need to start javac using the usual debug switches, which must be prepended with -J in this case:\n $ javac -J-Xdebug \\ -J-Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8000 \\ HelloWorld.java   Make sure to download the right version of the OpenJDK source code and set it up in your IDE, so that you also can step through internal classes whose source code isn\u0026#8217;t distributed with binary builds. An interesting starting point for your explorations could be the JDKPlatformProvider class.\n     To double-check, you could also confirm with the API diffs provided by the Java Version Almanac or the Adopt OpenJDK JDK API diff generator. While doing so, one more thing piqued my curiosity: these reports don\u0026#8217;t show any changes to java.util.List in Java 11, whereas ct.sym contains a new version of the corresponding signature file; To find out what\u0026#8217;s going on, again javap\u0026#8201;\u0026#8212;\u0026#8201;this time with a bit more detail level\u0026#8201;\u0026#8212;\u0026#8201;came in handy:\n $ javap -p -c -s -v -l List10.class \u0026gt; List10.txt $ javap -p -c -s -v -l List11.class \u0026gt; List11.txt $ git diff --no-index -w List10.txt List11.txt ... - #96 = Utf8 RuntimeInvisibleAnnotations - #97 = Utf8 Ljdk/Profile+Annotation; - #98 = Utf8 value - #99 = Integer 1 { public abstract int size(); descriptor: ()I @@ -308,8 +304,3 @@ Constant pool: Signature: #87 // \u0026lt;E:Ljava/lang/Object;\u0026gt;(Ljava/util/Collection\u0026lt;+TE;\u0026gt;;)Ljava/util/List\u0026lt;TE;\u0026gt;; } Signature: #95 // \u0026lt;E:Ljava/lang/Object;\u0026gt;Ljava/lang/Object;Ljava/util/Collection\u0026lt;TE;\u0026gt;; -RuntimeInvisibleAnnotations: - 0: #97(#98=I#99) - jdk.Profile+Annotation( - value=1 - )   An annotation with the interesting name @jdk.Profile+Annotion(1) got removed. Now, if you look at the List.java source file in Java 10, you won\u0026#8217;t find this annotation anywhere. In fact, this annotation type doesn\u0026#8217;t exist at all. By grepping through the OpenJDK source code for ct.sym, I learned that it is a synthetic annotation which gets added during the process of creating the signature files, denoting which compact profile a class belongs to.\n     Compact Profiles Compact Profiles are a notion in Java 8 which defines three specific sub-sets of the Java platform: compact1, compact2, and compact3. Each profile contains a fixed set of JDK packages and build upon each other, allowing for more size-efficient deployments to constrained devices, if such profile is sufficient for a given application. With Java 9, the module system, and the ability to create custom runtime images on a much more granular level (using jlink), compact profiles became pretty much obsolete.\n     So that\u0026#8217;s another purpose of the ct.sym file: it allows the compiler to ensure compatibility with a chosen compact profile. In current JDKs, javac still supports the -profile option, but only when compiling for Java 8. In that light, it\u0026#8217;s not quite clear why that annotation only was removed from the signature file with Java 11.\n Summing up, since Java 9 the javac compiler provides powerful means of ensuring API compatibility with earlier Java versions. With a size of 7.2 MB for Java 16, the ct.sym file contains the JDK API signature versions all the way back to Java 7. Using the --release compiler option, backwards-compatible builds, fully portable, and without the need for actually installing earlier JDKs, are straight foward. With that tool in your box, there\u0026#8217;s really no need any longer for using the -source and -target options. Not only that, --release will also help to spot subtle compatibility issues related to overriding methods with co-variant return types, such as ByteBuffer.position().\n","id":4,"publicationdate":"Apr 26, 2021","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eOne of the ultimate strengths of Java is its strong notion of backwards compatibility:\nJava applications and libraries built many years ago oftentimes run without problems on current JVMs,\nand the compiler of current JDKs can produce byte code, that is executable with earlier Java versions.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eFor instance, JDK 16 supports byte code levels going back as far as to Java 1.7;\nBut: \u003cem\u003ehic sunt dracones\u003c/em\u003e.\nThe emitted byte code level is just one part of the story.\nIt\u0026#8217;s equally important to consider which APIs of the JDK are used by the compiled code,\nand whether they are available in the targeted Java runtime version.\nAs an example, let\u0026#8217;s consider this simple \"Hello World\" program:\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"The Anatomy of ct.sym — How javac Ensures Backwards Compatibility","uri":"https://www.morling.dev/blog/the-anatomy-of-ct-sym-how-javac-ensures-backwards-compatibility/"},{"content":"Java 16 is around the corner, so there\u0026#8217;s no better time than now for learning more about the features which the new version will bring. After exploring the support for Unix domain sockets a while ago, I\u0026#8217;ve lately been really curious about the incubating Vector API, as defined by JEP 338, developed under the umbrella of Project Panama, which aims at \"interconnecting JVM and native code\".\n Vectors?!? Of course this is not about renewing the ancient Java collection types like java.util.Vector (\u0026lt;insert some pun about this here\u0026gt;), but rather about an API which lets Java developers take advantage of the vector calculation capabilities you can find in most CPUs these days. Now I\u0026#8217;m by no means an expert on low-level programming leveraging specific CPU instructions, but exactly that\u0026#8217;s why I hope to make the case with this post that the new Vector API makes these capabilities approachable to a wide audience of Java programmers.\n What\u0026#8217;s SIMD Anyways? Before diving into a specific example, it\u0026#8217;s worth pointing out why that API is so interesting, and what it could be used for. In a nutshell, CPU architectures like x86 or AArch64 provide extensions to their instruction sets which allow you to apply a single operation to multiple data items at once (SIMD\u0026#8201;\u0026#8212;\u0026#8201;single instruction, multiple data). If a specific computing problem can be solved using an algorithm that lends itself to such parallelization, substantial performance improvements can be gained. Examples for such SIMD instruction set extensions include SSE and AVX for x64, and Neon of AArch64 (Arm).\n As such, they complement other means of compute parallelization: scaling out across multiple machines which collaborate in a cluster, and multi-threaded programming. Unlike these though, vectorized computations are done within the scope of an individual method, e.g. operating on multiple elements of an array at once.\n So far, there was no way for Java developers to directly work with such SIMD instructions. While you can use SIMD intrinsics in languages closer to the metal such as C/C++, no such option exists in Java so far. Note this doesn\u0026#8217;t mean Java wouldn\u0026#8217;t take advantage of SIMD at all: the JIT compiler can auto-vectorize code in specific situations, i.e. transforming code from a loop into vectorized code. Whether that\u0026#8217;s possible or not isn\u0026#8217;t easy to determine, though; small changes to a loop which the compiler was able to vectorize before, may lead to scalar execution, resulting in a performance regression.\n JEP 338 aims to improve this situation: introducing a portable vector computation API, it allows Java developers to benefit from SIMD execution by means of explicitly vectorized algorithms. Unlike C/C++ style intrinsics, this API will be mapped automatically by the C2 JIT compiler to the corresponding instruction set of the underlying platform, falling back to scalar execution if the platform doesn\u0026#8217;t provide the required capabilities. A pretty sweet deal, if you ask me!\n Now, why would you be interested in this? Doesn\u0026#8217;t \"vector calculation\" sound an awful lot like mathematics-heavy, low-level algorithms, which you don\u0026#8217;t tend to find that much in your typical Java enterprise applications? I\u0026#8217;d say, yes and no. Indeed it may not be that beneficial for say a CRUD application copying some data from left to right. But there are many interesting applications in areas like image processing, AI, parsing, (SIMD-based JSON parsing being a prominently discussed example), text processing, data type conversions, and many others. In that regard, I\u0026#8217;d expect that JEP 338 will pave the path for using Java in many interesting use cases, where it may not be the first choice today.\n   Vectorizing FizzBuzz To see how the Vector API can help with improving the performance of some calculation, let\u0026#8217;s consider FizzBuzz. Originally, FizzBuzz is a game to help teaching children division; but interestingly, it also serves as entry-level interview question for hiring software engineers in some places. In any case, it\u0026#8217;s a nice example for exploring how some calculation can benefit from vectorization. The rules of FizzBuzz are simple:\n   Numbers are counted and printed out: 1, 2, 3, \u0026#8230;\u0026#8203;\n  If a number if divisible by 3, instead of printing the number, print \"Fizz\"\n  If a number if divisible by 5, print \"Buzz\"\n  If a number if divisible by 3 and 5, print \"FizzBuzz\"\n   As the Vector API concerns itself with numeric values instead of strings, rather than \"Fizz\", \"Buzz\", and \"FizzBuzz\", we\u0026#8217;re going to emit -1, -2, and -3, respectively. The input of the program will be an array with the numbers from 1 \u0026#8230;\u0026#8203; 256, the output an array with the FizzBuzz sequence:\n 1, 2, -1, 4, -2, -1, 7, 8, -1, -2, 11, -1, 13, 14, -3, 16, ...   The task is easily solved using a plain for loop processing scalar values one by one:\n private static final int FIZZ = -1; private static final int BUZZ = -2; private static final int FIZZ_BUZZ = -3; public int[] scalarFizzBuzz(int[] values) { int[] result = new int[values.length]; for (int i = 0; i \u0026lt; values.length; i++) { int value = values[i]; if (value % 3 == 0) { if (value % 5 == 0) { (1) result[i] = FIZZ_BUZZ; } else { result[i] = FIZZ; (2) } } else if (value % 5 == 0) { result[i] = BUZZ; (3) } else { result[i] = value; (4) } } return result; }     1 The current number is divisible by 3 and 5: emit FIZZ_BUZZ (-3)   2 The current number is divisible by 3: emit FIZZ (-1)   3 The current number is divisible by 5: emit BUZZ (-2)   4 The current number is divisible by neither 3 nor 5: emit the number itself    As a baseline, this implementation can be executed ~2.2M times per second in a simple JMH benchmark running on my Macbook Pro 2019, with a 2.6 GHz 6-Core Intel Core i7 CPU:\n Benchmark (arrayLength) Mode Cnt Score Error Units FizzBuzzBenchmark.scalarFizzBuzz 256 thrpt 5 2204774,792 ± 76581,374 ops/s   Now let\u0026#8217;s see how this calculation could be vectorized and what performance improvements can be gained by doing so. When looking at the incubating Vector API, you may be overwhelmed at first by its large API surface. But it\u0026#8217;s becoming manageable once you realize that all the types like IntVector, LongVector, etc. essentially expose the same set of methods, only specific for each of the supported data types (and indeed, as per the JavaDoc, all these classes were not hand-written by some poor soul, but generated, from some sort of parameterized template supposedly).\n Amongst the plethora of API methods, there is no modulo operation, though (which makes sense, as for instance there isn\u0026#8217;t such instruction in any of the x86 SIMD extensions). So what could we do to solve the FizzBuzz task? After skimming through the API for some time, the method blend​(Vector\u0026lt;Integer\u0026gt; v, VectorMask\u0026lt;Integer\u0026gt; m) caught my attention:\n  Replaces selected lanes of this vector with corresponding lanes from a second input vector under the control of a mask. [\u0026#8230;\u0026#8203;]\n   For any lane set in the mask, the new lane value is taken from the second input vector, and replaces whatever value was in the that lane of this vector.\n  For any lane unset in the mask, the replacement is suppressed and this vector retains the original value stored in that lane.\n     This sounds pretty useful; The pattern of expected -1, -2, and -3 values repeats every 15 input values. So we can \"pre-calculate\" that pattern once and persist it in form of vectors and masks for the blend() method. While stepping through the input array, the right vector and mask are obtained based on the current position and are used with blend() in order to mark the values divisible by 3, 5, and 15 (another option could be min(Vector\u0026lt;Integer\u0026gt; v), but I decided against it, as we\u0026#8217;d need some magic value for representing those numbers which should be emitted as-is).\n Here is a visualization of the approach, assuming a vector length of eight elements (\"lanes\"):\n   So let\u0026#8217;s see how we can implement this using the Vector API. The mask and second input vector repeat every 120 elements (least common multiple of 8 and 15), so 15 masks and vectors need to be determined. They can be created like so:\n public class FizzBuzz { private static final VectorSpecies\u0026lt;Integer\u0026gt; SPECIES = IntVector.SPECIES_256; (1) private final List\u0026lt;VectorMask\u0026lt;Integer\u0026gt;\u0026gt; resultMasks = new ArrayList\u0026lt;\u0026gt;(15); private final IntVector[] resultVectors = new IntVector[15]; public FizzBuzz() { List\u0026lt;VectorMask\u0026lt;Integer\u0026gt;\u0026gt; threes = Arrays.asList( (2) VectorMask.\u0026lt;Integer\u0026gt;fromLong(SPECIES, 0b00100100), VectorMask.\u0026lt;Integer\u0026gt;fromLong(SPECIES, 0b01001001), VectorMask.\u0026lt;Integer\u0026gt;fromLong(SPECIES, 0b10010010) ); List\u0026lt;VectorMask\u0026lt;Integer\u0026gt;\u0026gt; fives = Arrays.asList( (3) VectorMask.\u0026lt;Integer\u0026gt;fromLong(SPECIES, 0b00010000), VectorMask.\u0026lt;Integer\u0026gt;fromLong(SPECIES, 0b01000010), VectorMask.\u0026lt;Integer\u0026gt;fromLong(SPECIES, 0b00001000), VectorMask.\u0026lt;Integer\u0026gt;fromLong(SPECIES, 0b00100001), VectorMask.\u0026lt;Integer\u0026gt;fromLong(SPECIES, 0b10000100) ); for(int i = 0; i \u0026lt; 15; i++) { (4) VectorMask\u0026lt;Integer\u0026gt; threeMask = threes.get(i%3); VectorMask\u0026lt;Integer\u0026gt; fiveMask = fives.get(i%5); resultMasks.add(threeMask.or(fiveMask)); (5) resultVectors[i] = IntVector.zero(SPECIES) (6) .blend(FIZZ, threeMask) .blend(BUZZ, fiveMask) .blend(FIZZ_BUZZ, threeMask.and(fiveMask)); } } }     1 A vector species describes the combination of an vector element type (in this case Integer) and a vector shape (in this case 256 bit); i.e. here we\u0026#8217;re going to deal with vectors that hold 8 32 bit int values   2 Vector masks describing the numbers divisible by three (read the bit values from right to left)   3 Vector masks describing the numbers divisible by five   4 Let\u0026#8217;s create the fifteen required result masks and vectors   5 A value in the output array should be set to another value if it\u0026#8217;s divisible by three or five   6 Set the value to -1, -2, or -3, depending on whether its divisible by three, five, or fifteen, respectively; otherwise set it to the corresponding value from the input array    With this infrastructure in place, we can implement the actual method for calculating the FizzBuzz values for an arbitrarily long input array:\n public int[] simdFizzBuzz(int[] values) { int[] result = new int[values.length]; int i = 0; int upperBound = SPECIES.loopBound(values.length); (1) for (; i \u0026lt; upperBound; i += SPECIES.length()) { (2) IntVector chunk = IntVector.fromArray(SPECIES, values, i); (3) int maskIdx = (i/SPECIES.length())%15; (4) IntVector fizzBuzz = chunk.blend(resultValues[maskIdx], resultMasks[maskIdx]); (5) fizzBuzz.intoArray(result, i); (6) } for (; i \u0026lt; values.length; i++) { (7) int value = values[i]; if (value % 3 == 0) { if (value % 5 == 0) { result[i] = FIZZ_BUZZ; } else { result[i] = FIZZ; } } else if (value % 5 == 0) { result[i] = BUZZ; } else { result[i] = value; } } return result; }     1 determine the maximum index in the array that\u0026#8217;s divisible by the species length; e.g. if the input array is 100 elements long, that\u0026#8217;d be 96 in the case of vectors with eight elements each   2 Iterate through the input array in steps of the vector length   3 Load the current chunk of the input array into an IntVector   4 Obtain the index of the right result vector and mask   5 Determine the FizzBuzz numbers for the current chunk (i.e. that\u0026#8217;s the actual SIMD instruction, processing all eight elements of the current chunk at once)   6 Copy the result values at the right index into the result array   7 Process any remainder (e.g. the last four remaining elements in case of an input array with 100 elements) using the traditional scalar approach, as those values couldn\u0026#8217;t fill up another vector instance    To reiterate what\u0026#8217;s happening here: instead of processing the values of the input array one by one, they are processed in chunks of eight elements each by means of the blend() vector operation, which can be mapped to an equivalent SIMD instruction of the CPU. In case the input array doesn\u0026#8217;t have a length that\u0026#8217;s a multiple of the vector length, the remainder is processed in the traditional scalar way. The resulting duplication of the logic seems a bit inelegant, we\u0026#8217;ll discuss in a bit what can be done about that.\n For now, let\u0026#8217;s see whether our efforts pay off; i.e. is this vectorized approach actually faster then the basic scalar implementation? Turns out it is! Here are the numbers I get from JMH on my machine, showing through-put increasing by factor 3:\n Benchmark (arrayLength) Mode Cnt Score Error Units FizzBuzzBenchmark.scalarFizzBuzz 256 thrpt 5 2204774,792 ± 76581,374 ops/s FizzBuzzBenchmark.simdFizzBuzz 256 thrpt 5 6748723,261 ± 34725,507 ops/s   Is there anything that could be further improved? I\u0026#8217;m pretty sure, but as said I\u0026#8217;m not an expert here, so I\u0026#8217;ll leave it to smarter folks to point out more efficient implementations in the comments. One thing I figured is that the division and modulo operation for obtaining the current mask index isn\u0026#8217;t ideal. Keeping a separate loop variable that\u0026#8217;s reset to 0 after reaching 15 proved to be quite a bit faster:\n public int[] simdFizzBuzz(int[] values) { int[] result = new int[values.length]; int i = 0; int j = 0; int upperBound = SPECIES.loopBound(values.length); for (; i \u0026lt; upperBound; i += SPECIES.length()) { IntVector chunk = IntVector.fromArray(SPECIES, values, i); IntVector fizzBuzz = chunk.blend(resultValues[j], resultMasks[j]); fizzBuzz.intoArray(result, i); j++; if (j == 15) { j = 0; } } // processing of remainder... }   Benchmark (arrayLength) Mode Cnt Score Error Units FizzBuzzBenchmark.scalarFizzBuzz 256 thrpt 5 2204774,792 ± 76581,374 ops/s FizzBuzzBenchmark.simdFizzBuzz 256 thrpt 5 6748723,261 ± 34725,507 ops/s FizzBuzzBenchmark.simdFizzBuzzSeparateMaskIndex 256 thrpt 5 8830433,250 ± 69955,161 ops/s   This makes for another nice improvement, yielding 4x the throughput of the original scalar implementation. Now, to make this a true apple-to-apple comparison, a mask-based approach can also be applied to the purely scalar implementation, only that each value needs to be looked up individually:\n private int[] serialMask = new int[] {0, 0, -1, 0, -2, -1, 0, 0, -1, -10, 0, -1, 0, 0, -3}; public int[] serialFizzBuzzMasked(int[] values) { int[] result = new int[values.length]; int j = 0; for (int i = 0; i \u0026lt; values.length; i++) { int res = serialMask[j]; result[i] = res == 0 ? values[i] : res; j++; if (j == 15) { j = 0; } } return result; }   Indeed, this implementation is quite a bit better than the original one, but still the SIMD-based approach is more than twice as fast:\n Benchmark (arrayLength) Mode Cnt Score Error Units FizzBuzzBenchmark.scalarFizzBuzz 256 thrpt 5 2204774,792 ± 76581,374 ops/s FizzBuzzBenchmark.scalarFizzBuzzMasked 256 thrpt 5 4156751,424 ± 23668,949 ops/s FizzBuzzBenchmark.simdFizzBuzz 256 thrpt 5 6748723,261 ± 34725,507 ops/s FizzBuzzBenchmark.simdFizzBuzzSeparateMaskIndex 256 thrpt 5 8830433,250 ± 69955,161 ops/s     Examining the Native Code This all is pretty cool, but can we trust that under the hood things actually happen the way we expect them to happen? In order to verify that, let\u0026#8217;s take a look at the native assembly code that gets produced by the JIT compiler for this implementation. This requires you to run the JVM with the hsdis plug-in; see this post for instructions on how to build and install hsdis. Let\u0026#8217;s create a simple main class which executes the method in question in a loop, so to make sure the method actually gets JIT-compiled:\n public class Main { public static int[] blackhole; public static void main(String[] args) { FizzBuzz fizzBuzz = new FizzBuzz(); var values = IntStream.range(1, 257).toArray(); for(int i = 0; i \u0026lt; 5_000_000; i++) { blackhole = fizzBuzz.simdFizzBuzz(values); } } }   Run the program, enabling the output of the assembly, and piping its output into a log file:\n java -XX:+UnlockDiagnosticVMOptions \\ -XX:+PrintAssembly -XX:+LogCompilation \\ --add-modules=jdk.incubator.vector \\ --class-path target/classes \\ dev.morling.demos.simdfizzbuzz.Main \u0026gt; fizzbuzz.log   Open the fizzbuzz.log file and look for the C2-compiled nmethod block of the simdFizzBuzz method. Somewhere within the method\u0026#8217;s native code, you should find the vpblendvb instruction (output slightly adjusted for better readability):\n ... =========================== C2-compiled nmethod ============================ --------------------------------- Assembly --------------------------------- Compiled method (c2) ... dev.morling.demos.simdfizzbuzz.FizzBuzz:: ↩ simdFizzBuzz (161 bytes) ... 0x000000011895e18d: vpmovsxbd %xmm7,%ymm7 ↩ ;*invokestatic store {reexecute=0 rethrow=0 return_oop=0} ; - jdk.incubator.vector.IntVector::intoArray@42 (line 2962) ; - dev.morling.demos.simdfizzbuzz.FizzBuzz::simdFizzBuzz@76 (line 92) 0x000000011895e192: vpblendvb %ymm7,%ymm5,%ymm8,%ymm0 ↩ ;*invokestatic blend {reexecute=0 rethrow=0 return_oop=0} ; - jdk.incubator.vector.IntVector::blendTemplate@26 (line 1895) ; - jdk.incubator.vector.Int256Vector::blend@11 (line 376) ; - jdk.incubator.vector.Int256Vector::blend@3 (line 41) ; - dev.morling.demos.simdfizzbuzz.FizzBuzz::simdFizzBuzz@67 (line 91) ...   vpblendvb is part of the x86 AVX2 instruction set and \"conditionally copies byte elements from the source operand (second operand) to the destination operand (first operand) depending on mask bits defined in the implicit third register argument\", as such exactly corresponding to the blend() method in the JEP 338 API.\n One detail not quite clear to me is why vpmovsxbd for copying the results into the output array (the intoArray() call) shows up before vpblendvb. If you happen to know the reason for this, I\u0026#8217;d love to hear from you and learn about this.\n   Avoiding Scalar Processing of Tail Elements Let\u0026#8217;s get back to the scalar processing of the potential remainder of the input array. This feels a bit \"un-DRY\", as it requires the algorithm to be implemented twice, once vectorized and once in a scalar way.\n The Vector API recognizes the desire for avoiding this duplication and provides masked versions of all the required operations, so that during the last iteration no access beyond the array length will happen. Using this approach, the SIMD FizzBuzz method looks like this:\n public int[] simdFizzBuzzMasked(int[] values) { int[] result = new int[values.length]; int j = 0; for (int i = 0; i \u0026lt; values.length; i += SPECIES.length()) { var mask = SPECIES.indexInRange(i, values.length); (1) var chunk = IntVector.fromArray(SPECIES, values, i, mask); (2) var fizzBuzz = chunk.blend(resultValues[j], resultMasks.get(j)); fizzBuzz.intoArray(result, i, mask); (2) j++; if (j == 15) { j = 0; } } return result; }     1 Obtain a mask which, during the last iteration, will have bits for those lanes unset, which are larger than the last encountered multiple of the vector length   2 Perform the same operations as above, but using the mask to prevent any access beyond the array length    The implementation looks quite a bit nicer than the version with the explicit scalar processing of the remainder portion. But the impact on throughput is significant, the result is quite a disappointing:\n Benchmark (arrayLength) Mode Cnt Score Error Units FizzBuzzBenchmark.scalarFizzBuzz 256 thrpt 5 2204774,792 ± 76581,374 ops/s FizzBuzzBenchmark.scalarFizzBuzzMasked 256 thrpt 5 4156751,424 ± 23668,949 ops/s FizzBuzzBenchmark.simdFizzBuzz 256 thrpt 5 6748723,261 ± 34725,507 ops/s FizzBuzzBenchmark.simdFizzBuzzSeparateMaskIndex 256 thrpt 5 8830433,250 ± 69955,161 ops/s FizzBuzzBenchmark.simdFizzBuzzMasked 256 thrpt 5 1204128,029 ± 5556,553 ops/s   In its current form, this approach is even slower than the pure scalar implementation. It remains to be seen whether and how performance gets improved here, as the Vector API matures. Ideally, the mask would have to be only applied during the very last iteration. This is something we either could do ourselves\u0026#8201;\u0026#8212;\u0026#8201;re-introducing some special remainder handling, albeit less different from the core implementation than with the pure scalar approach discussed above\u0026#8201;\u0026#8212;\u0026#8201;or perhaps even the compiler itself may be able to apply such transformation.\n One important take-away from this is that a SIMD-based approach does not necessarily have to be faster than a scalar one. So every algorithmic adjustment should be validated with a corresponding benchmark, before drawing any conclusions. Speaking of which, I also ran the benchmark on that shiny new Mac Mini M1 (i.e. an AArch64-based machine) that found its way to my desk recently, and numbers are, mh, interesting:\n Benchmark (arrayLength) Mode Cnt Score Error Units FizzBuzzBenchmark.scalarFizzBuzz 256 thrpt 5 2717990,097 ± 4203,628 ops/s FizzBuzzBenchmark.scalarFizzBuzzMasked 256 thrpt 5 5750402,582 ± 2479,462 ops/s FizzBuzzBenchmark.simdFizzBuzz 256 thrpt 5 1297631,404 ± 15613,288 ops/s FizzBuzzBenchmark.simdFizzBuzzMasked 256 thrpt 5 374313,033 ± 2219,940 ops/s FizzBuzzBenchmark.simdFizzBuzzMasksInArray 256 thrpt 5 1316375,073 ± 1178,704 ops/s FizzBuzzBenchmark.simdFizzBuzzSeparateMaskIndex 256 thrpt 5 998979,324 ± 69997,361 ops/s   The scalar implementation on the M1 out-performs the x86 MacBook Pro by quite a bit, but SIMD numbers are significantly lower.\n I haven\u0026#8217;t checked the assembly code, but solely based on the figures, my guess is that the JEP 338 implementation in the current JDK 16 builds does not yet support AArch64, and the API falls back to scalar execution.\n Here it would be nice to have some method in the API which reveals whether SIMD support is provided by the current platform or not, as e.g. done by .NET with its Vector.IsHardwareAccelerated() method.\n Update, March 9th: After asking about this on the panama-dev mailing list, Ningsheng Jian from Arm explained that the AArch64 NEON instruction set has a maximum hardware vector size of 128 bits; hence the Vector API is transparently falling back to the Java implementation in our case of using 256 bits. By passing the -XX:+PrintIntrinsics flag you can inspect which API calls get intrinsified (i.e. executed via corresponding hardware instructions) and which ones not. When running the main class from above with this option, we get the relevant information (output slightly adjusted for better readability):\n @ 31 jdk.internal.vm.vector.VectorSupport::load (38 bytes) ↩ failed to inline (intrinsic) ... @ 26 jdk.internal.vm.vector.VectorSupport::blend (38 bytes) ↩ failed to inline (intrinsic) ... @ 42 jdk.internal.vm.vector.VectorSupport::store (38 bytes) ↩ failed to inline (intrinsic) ** not supported: arity=0 op=load vlen=8 etype=int ismask=no ** not supported: arity=2 op=blend vlen=8 etype=int ismask=useload ** not supported: arity=1 op=store vlen=8 etype=int ismask=no   Fun fact: during the entire benchmark runtime of 10 min the fan of the Mac Mini was barely to hear, if at all. Definitely a very exciting platform, and I\u0026#8217;m looking forward to doing more Java experiments on it soon.\n   Wrap-Up Am I suggesting you should go and implement your next FizzBuzz using SIMD? Of course not, FizzBuzz just served as an example here for exploring how a well-known \"problem\" can be solved more efficiently via the new Java Vector API (at the cost of increased complexity in the code), also without being a seasoned systems programmer. On the other hand, it may make an impression during your next job interview ;)\n If you want to get started with your own experiments around the Vector API and SIMD, install a current JDK 16 RC (release candidate) build and grab the SIMD FizzBuzz example from this GitHub repo. A nice twist to explore would for instance be using ShortVector instead of IntVector (allowing to put 16 values into 256-bit vector), running the benchmark on machines with the AVX-512 extension (e.g. via the C5 instance type on AWS EC2), or both :)\n Apart from the JEP document itself, there isn\u0026#8217;t too much info out yet about the Vector API; a great starting point are the \"vector\" tagged posts on the blog of Richard Startin. Another inspirational resource is August Nagro\u0026#8217;s project for vectorized UTF-8 validation based on a paper by John Keiser and Daniel Lemire. Kishor Kharbas and Paul Sandoz did a talk about the Vector API at CodeOne a while ago.\n Taking a step back, it\u0026#8217;s hard to overstate the impact which the Vector API potentially will have on the Java platform. Providing SIMD capabilities in a rather easy-to-use, portable way, without having to rely on CPU instruction set specific intrinsics, may result in nothing less than a \"democratization of SIMD\", making these powerful means of parallelizing computations available to a much larger developer audience.\n Also the JDK class library itself may benefit from the Vector API; while JDK authors\u0026#8201;\u0026#8212;\u0026#8201;unlike Java application developers\u0026#8201;\u0026#8212;\u0026#8201;already have the JVM intrinsics mechanism at their disposal, the new API will \"make prototyping easier, and broaden what might be economical to consider\", as pointed out by Claes Redestad.\n But nothing in life is free, and code will have to be restructured or even re-written in order to benefit from this. Some problems lend themselves better than others to SIMD-style processing, and only time will tell in which areas the new API will be adopted. As said above, use cases like image processing and AI can benefit from SIMD a lot, due to the nature of the underlying calculations. Also specific data store operations can be sped up significantly using SIMD instructions; so my personal hope is that the Vector API can contribute to making Java an attractive choice for such applications, which previously were not considered a sweet spot for the Java platform.\n As such, I can\u0026#8217;t think of many recent Java API additions which may prove as influential as the Vector API.\n  ","id":5,"publicationdate":"Mar 8, 2021","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eJava 16 is around the corner, so there\u0026#8217;s no better time than now for learning more about the features which the new version will bring.\nAfter exploring the support for \u003ca href=\"/blog/talking-to-postgres-through-java-16-unix-domain-socket-channels/\"\u003eUnix domain sockets\u003c/a\u003e a while ago,\nI\u0026#8217;ve lately been really curious about the incubating Vector API,\nas defined by \u003ca href=\"https://openjdk.java.net/jeps/338\"\u003eJEP 338\u003c/a\u003e,\ndeveloped under the umbrella of \u003ca href=\"https://openjdk.java.net/projects/panama/\"\u003eProject Panama\u003c/a\u003e,\nwhich aims at \"interconnecting JVM and native code\".\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003e\u003cem\u003eVectors?!?\u003c/em\u003e\nOf course this is not about renewing the ancient Java collection types like \u003ccode\u003ejava.util.Vector\u003c/code\u003e\n(\u0026lt;insert some pun about this here\u0026gt;),\nbut rather about an API which lets Java developers take advantage of the vector calculation capabilities you can find in most CPUs these days.\nNow I\u0026#8217;m by no means an expert on low-level programming leveraging specific CPU instructions,\nbut exactly that\u0026#8217;s why I hope to make the case with this post that the new Vector API makes these capabilities approachable to a wide audience of Java programmers.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"FizzBuzz – SIMD Style!","uri":"https://www.morling.dev/blog/fizzbuzz-simd-style/"},{"content":"Update Feb 5: This post is discussed on Hacker News\n Reading a blog post about what\u0026#8217;s coming up in JDK 16 recently, I learned that one of the new features is support for Unix domain sockets (JEP 380). Before Java 16, you\u0026#8217;d have to resort to 3rd party libraries like jnr-unixsocket in order to use them. If you haven\u0026#8217;t heard about Unix domain sockets before, they are \"data communications [endpoints] for exchanging data between processes executing on the same host operating system\". Don\u0026#8217;t be put off by the name btw.; Unix domain sockets are also supported by macOS and even Windows since version 10.\n Databases such as Postgres or MySQL use them for offering an alternative to TCP/IP-based connections to client applications running on the same machine as the database. In such scenario, Unix domain sockets are both more secure (no remote access to the database is exposed at all; file system permissions can be used for access control), and also more efficient than TCP/IP loopback connections.\n A common use case are proxies for accessing Cloud-based databases, such as as the GCP Cloud SQL Proxy. Running on the same machine as a client application (e.g. in a sidecar container in case of Kubernetes deployments), they provide secure access to a managed database, for instance taking care of the SSL handling.\n My curiousity was piqued and I was wondering what it\u0026#8217;d take to make use of the new Java 16 Unix domain socket for connecting to Postgres. It was your regular evening during the pandemic, without much to do, so I thought \"Let\u0026#8217;s give this a try\". To have a testing bed, I started with installing Postgres 13 on Fedora 33. Fedora might not always have the latest Postgres version packaged just yet, but following the official Postgres instructions it is straight-forward to install newer versions.\n In order to connect with user name and password via a Unix domain socket, one small adjustment to /var/lib/pgsql/13/data/pg_hba.conf is needed: the access method for the local connection type must be switched from the default value peer (which would try to authenticate using the operating system user name of the client process) to md5.\n ... # TYPE DATABASE USER ADDRESS METHOD # \"local\" is for Unix domain socket connections only local all all md5 ...   Make sure to apply the changed configuration by restarting the database (systemctl restart postgresql-13), and things are ready to go.\n The Postgres JDBC Driver The first thing I looked into was the Postgres JDBC driver. Since version 9.4-1208 (released in 2016) it allows you to configure custom socket factories, a feature which explicitly was added considering Unix domain sockets. The driver itself doesn\u0026#8217;t come with a socket factory implementation that\u0026#8217;d actually support Unix domain sockets, but a few external open-source implementations exist. Most notably junixsocket provides such socket factory.\n Custom socket factories must extend javax.net.SocketFactory, and their fully-qualified class name needs to be specified using the socketFactory driver parameter. So it should be easy to create SocketFactory implementation based on the new UnixDomainSocketAddress class, right?\n public class PostgresUnixDomainSocketFactory extends SocketFactory { @Override public Socket createSocket() throws IOException { var socket = new Socket(); socket.connect(UnixDomainSocketAddress.of( \"/var/run/postgresql/.s.PGSQL.5432\")); (1) return socket; } // other create methods ... }     1 Create a Unix domain socket address for the default path of the socket on Fedora and related systems    It compiles just fine; but it turns out not all socket addresses are equal, and java.net.Socket only connects to addresses of type InetSocketAddress (and the PG driver maintainers seem to sense some air of mystery around these \"unusual\" events, too):\n org.postgresql.util.PSQLException: Something unusual has occurred to cause the driver to fail. Please report this exception. at org.postgresql.Driver.connect(Driver.java:285) ... Caused by: java.lang.IllegalArgumentException: Unsupported address type at java.base/java.net.Socket.connect(Socket.java:629) at java.base/java.net.Socket.connect(Socket.java:595) at dev.morling.demos.PostgresUnixDomainSocketFactory.createSocket(PostgresUnixDomainSocketFactory.java:19) ...   Now JEP 380 solely speaks about SocketChannel and stays silent about Socket; but perhaps obtaining a socket from a domain socket channel works?\n public Socket createSocket() throws IOException { var sc = SocketChannel.open(UnixDomainSocketAddress.of( \"/var/run/postgresql/.s.PGSQL.5432\")); return sc.socket(); }   Nope, no luck either:\n java.lang.UnsupportedOperationException: Not supported at java.base/sun.nio.ch.SocketChannelImpl.socket(SocketChannelImpl.java:226) at dev.morling.demos.PostgresUnixDomainSocketFactory.createSocket(PostgresUnixDomainSocketFactory.java:17)   Indeed it looks like JEP 380 is concerning itself only with the non-blocking SocketChannel API, while users of the blocking Socket API do not get to benefit from it. It should be possible to create a custom Socket implementation based on the socket channel support of JEP 380, but that\u0026#8217;s going beyond the scope of my little exploration.\n   The Vert.x Postgres Client If the Postgres JDBC driver doesn\u0026#8217;t easily benefit from the JEP, what about other Java Postgres clients then? There are several non-blocking options, including the Vert.x Postgres client and R2DBC. The former is used to bring Reactive capabilities for Postgres into the Quarkus stack, too, so I turned my attention to it.\n Now the Vert.x Postgres Client already has support for Unix domain sockets, by means of adding the right Netty native transport dependency to your project. So purely from functionality perspective, there\u0026#8217;s not that much to be gained here. But being able to use domain sockets also with the default NIO transport would still be nice, as it means one less dependency to take care of. So I dug a bit into the code of the Postgres client and Vert.x itself and figured out, that two things needed adjustment:\n   The NIO-based Transport class of Vert.x needs to learn about the fact that SocketChannel now also supports Unix domain sockets (currently, an exception is raised when trying to use them without a Netty native transport)\n  Netty\u0026#8217;s NioSocketChannel needs some small changes, as it tries to obtain a Socket from the underlying SocketChannel, which doesn\u0026#8217;t work for domain sockets as we\u0026#8217;ve seen above\n   Step 1 was quickly done by creating a custom sub-class of the default Transport class. Two methods needed changes: channelFactory() for obtaining a factory for the actual Netty transport channel, and convert() for converting a Vert.x SocketAddress into a NIO one:\n public class UnixDomainTransport extends Transport { @Override public ChannelFactory\u0026lt;? extends Channel\u0026gt; channelFactory( boolean domainSocket) { if (!domainSocket) { (1) return super.channelFactory(domainSocket); } else { return () -\u0026gt; { try { var sc = SocketChannel.open(StandardProtocolFamily.UNIX); (2) return new UnixDomainSocketChannel(null, sc); } catch(Exception e) { throw new RuntimeException(e); } }; } } @Override public SocketAddress convert(io.vertx.core.net.SocketAddress address) { if (!address.isDomainSocket()) { (3) return super.convert(address); } else { return UnixDomainSocketAddress.of(address.path()); (4) } } }     1 Delegate creation of non domain socket factories to the regular NIO transport implementation   2 This channel factory returns instances of our own UnixDomainSocketChannel type (see below), passing a socket channel based on the new UNIX protocol family   3 Delegate conversion of non domain socket addresses to the regular NIO transport implementation   4 Create a UnixDomainSocketAddress for the socket\u0026#8217;s file system path    Now let\u0026#8217;s take a look at the UnixDomainSocketChannel class. I was hoping to get away again with creating a sub-class of the NIO-based implementation, io.netty.channel.socket.nio.NioSocketChannel in this case. Unfortunately, though, the NioSocketChannel constructor invokes the taboo SocketChannel#socket() method. Of course that\u0026#8217;d not be a problem when doing this change in Netty itself, but for my little exploration I ended up copying the class and doing the required adjustments in that copy. I ended up doing two small changes:\n   Avoiding the call to SocketChannel#socket() in the constructor:\npublic UnixDomainSocketChannel(Channel parent, SocketChannel socket) { super(parent, socket); config = new NioSocketChannelConfig(this, new Socket()); (1) }     1 Passing a dummy socket instead of socket.socket(), it shouldn\u0026#8217;t be accessed in our case anyways      A few methods call the Socket methods isInputShutdown() and isOutputShutdown(); those should be possible to be by-passed by keeping track of the two shutdown flags ourselves\n  As I was creating the UnixDomainSocketChannel in my own namespace instead of Netty\u0026#8217;s packages, a few references to the non-public method NioChannelOption#getOptions() needed commenting out, which again shouldn\u0026#8217;t be relevant for the domain socket case\n   You can find the complete change in this commit. All in all, not exactly an artisanal piece of software engineering, but the little hack seemed good enough at least for taking a quick glimpse at the new domain socket support. Of course a real implementation could be done much more properly within the Netty project itself.\n So it was time to give this thing a test ride. As we need to configure the custom Transport implementation, retrieval of a PgPool instance is a tad more verbose than usual:\n PgConnectOptions connectOptions = new PgConnectOptions() .setPort(5432) (1) .setHost(\"/var/run/postgresql\") .setDatabase(\"test_db\") .setUser(\"test_user\") .setPassword(\"topsecret!\"); PoolOptions poolOptions = new PoolOptions() .setMaxSize(5); VertxFactory fv = new VertxFactory(); fv.transport(new UnixDomainTransport()); (2) Vertx v = fv.vertx(); PgPool client = PgPool.pool(v, connectOptions, poolOptions); (3)     1 The Vert.x Postgres client constructs the domain socket path from the given port and path (via setHost()); the full path will be /var/run/postgresql/.s.PGSQL.5432, just as above   2 Construct a Vertx instance with the custom transport class   3 Obtain a PgPool instance using the customized Vertx instance    We then can can use the client instance as usual, only that it now will connect to Postgres using the domain socket instead of via TCP/IP. All this solely using the default NIO-based transports, without the need for adding any Netty native dependency, such as its epoll-based transport.\n I haven\u0026#8217;t done any real performance benchmark at this point; in a quick ad-hoc test of executing a trivial SELECT query on a primay key 200,000 times, I observed a latency of ~0.11 ms when using Unix domain sockets\u0026#8201;\u0026#8212;\u0026#8201;with both, netty-transport-native-epoll and JDK 16 Unix domain sockets\u0026#8201;\u0026#8212;\u0026#8201;and ~0.13 ms when connecting via TCP/IP. So definitely a significant improvement which can be a deciding factor for low-latency use cases, though in comparison to other reports, the latency reduction of ~15% appears to be at the lower end of the spectrum.\n Some more sincere performance evaluation should be done, for instance also examining the impact on garbage collection. And it goes without saying that you should only trust your own measurements, on your own hardware, based on your specific workloads, in order to decide whether you would benefit from domain sockets or not.\n   Other Use Cases Database connectivity is just one of the use cases for domain sockets; highly performant local inter-process communication comes in handy for all kinds of use cases. One which I find particularly intriguing is the creation of modular applications based on a multi-process architecture.\n When thinking of classic Java Jakarta EE application servers for instance, you could envision a model where both the application server and each deployment are separate processes, communicating through domain sockets. This would have some interesting advantages, such as stricter isolation (so for instance an OutOfMemoryError in one deployed application won\u0026#8217;t impact others) and re-deployments without any risk of classloader leaks, as the JVM of an deployment would be restarted. On the downside, you\u0026#8217;d be facing a higher overall memory consumption (although that can at least partly be mitigated through class data sharing, which also works across JVM boundaries) and more costly (remote) method invocations between deployments.\n Now the application server model has fallen out of favour for various reasons, but such multi-process design still is very interesting, for instance for building modular applications that should expose a single web endpoint, while being assembled from a set of processes which are developed and deployed by several, independent teams. Another use case would be desktop applications that are made up of a set of processes for isolation purposes, as it\u0026#8217;s e.g. done by most web browsers noawadays with distinct processes for separate tabs. JEP 380 should facilitate this model when creating Java applications, e.g. considering rich clients built with JavaFX.\n Another, really interesting feature of Unix domain sockets is the ability to transfer open file descriptors from one process to another. This allows for non-disruptive upgrades of server applications, without dropping any open TCP connections. This technique is used for instance by Envoy Proxy for applying configuration changes: upon a configuration change, a second Envoy instance with the new configuration is started up, takes over the active sockets from the previous instance and after some \"draining period\" triggers a shutdown of the old instance. This approach enables a truly immutable application design within Envoy itself, with all its advantages, without the need for in-process configuration reloads. I highly recommend to read the two posts linked above, they are super-interesting.\n Unfortunately, JEP 380 doesn\u0026#8217;t seem to support file descriptor transfers. So for this kind of architecture, you\u0026#8217;d still have to refrain to the aforementioned junixsocket library, which explicitly lists file transcriptor transfer support as one of its features. While you couldn\u0026#8217;t take advantage of that using Java\u0026#8217;s NIO API, it should be doable using alternative networking frameworks such as Netty. Probably a topic for another blog post on another one of those pandemic weekends ;)\n And that completes my small exploration of Java 16\u0026#8217;s support for Unix domain sockets. If you want to do your own experiments of using them to connect to Postgres, make sure to install the latest JDK 16 EA build and grab the source code of my experimentation from this GitHub repo.\n It\u0026#8217;d be my hope that frameworks like Netty and Vert.x make use of this JDK feature fairly quickly, as only a small amount of code changes is required, and users get to benefit from the higher performance of domain sockets without having to pull in any additional dependencies. In order to keep compatibility with Java versions prior to 16, multi-release JARs offer one avenue for integrating this feature.\n  ","id":6,"publicationdate":"Jan 31, 2021","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003e\u003cem\u003eUpdate Feb 5: This post is \u003ca href=\"https://news.ycombinator.com/item?id=26012466\"\u003ediscussed on Hacker News\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eReading a blog post about what\u0026#8217;s \u003ca href=\"https://www.loicmathieu.fr/wordpress/en/informatique/java-16-quoi-de-neuf/\"\u003ecoming up in JDK 16\u003c/a\u003e recently,\nI learned that one of the new features is support for Unix domain sockets (\u003ca href=\"https://openjdk.java.net/jeps/380\"\u003eJEP 380\u003c/a\u003e).\nBefore Java 16, you\u0026#8217;d have to resort to 3rd party libraries like \u003ca href=\"https://github.com/jnr/jnr-unixsocket\"\u003ejnr-unixsocket\u003c/a\u003e in order to use them.\nIf you haven\u0026#8217;t heard about \u003ca href=\"https://en.wikipedia.org/wiki/Unix_domain_socket\"\u003eUnix domain sockets\u003c/a\u003e before,\nthey are \"data communications [endpoints] for exchanging data between processes executing on the same host operating system\".\nDon\u0026#8217;t be put off by the name btw.;\nUnix domain sockets are also supported by macOS and even Windows since \u003ca href=\"https://devblogs.microsoft.com/commandline/af_unix-comes-to-windows/\"\u003eversion 10\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Talking to Postgres Through Java 16 Unix-Domain Socket Channels","uri":"https://www.morling.dev/blog/talking-to-postgres-through-java-16-unix-domain-socket-channels/"},{"content":"Discussions around Java\u0026#8217;s jlink tool typically center around savings in terms of (disk) space. Instead of shipping an entire JDK, a custom runtime image created with jlink contains only those JDK modules which an application actually requires, resulting in smaller distributables and container images.\n But the contribution of jlink\u0026#8201;\u0026#8212;\u0026#8201;as a part of the Java module system at large\u0026#8201;\u0026#8212;\u0026#8201;to the development of Java application\u0026#8217;s is bigger than that: with the notion of link time it defines an optional complement to the well known phases compile time and application run-time:\n  Link time is an opportunity to do whole-world optimizations that are otherwise difficult at compile time or costly at run-time. An example would be to optimize a computation when all its inputs become constant (i.e., not unknown). A follow-up optimization would be to remove code that is no longer reachable.\n   Other examples for link time optimizations are the removal of unnecessary classes and resources, the conversion of (XML-based) deployment descriptors into binary representations (which will be more efficiently processable at run-time), obfuscation, or the generation of annotation indexes. It would also be very interesting to create AppCDS archives for all the classes of a runtime image at link time and bake that archive into the image, resulting in faster application start-up, without any further manual configuration needed.\n While these use cases mostly relate to optimization of the runtime image in one way or another, the link time phase also is beneficial for the validation of applications. In the remainder of this post, I\u0026#8217;d like to discuss how link time validation can be employed to ensure the consistency of API signatures within a modularized Java application. This helps to avoid potential NoSuchMethodErrors and related errors which would otherwise be raised by the JVM at application run-time, stemming from the usage of incompatible module versions, different from the ones used at compile time.\n The Example To make things more tangible, let\u0026#8217;s look at an application made up of two modules, customer and order. As always, the full source code is available online, for you to play with. The customer module defines a service interface with the following signature:\n 1 2 3 public interface CustomerService { void incrementLoyaltyPoints(long customerId, long orderValue); }    The CustomerService interface is part of the customer module\u0026#8217;s public API and is invoked from within the order module like so:\n 1 2 3 4 5 6 7 public class OrderService { public static void main(String[] args) { CustomerService customerService = ...; customerService.incrementLoyaltyPoints(123, 4999); } }    Now let\u0026#8217;s assume there\u0026#8217;s a new version of the customer module; the signature of the incrementLoyaltyPoints() method got slightly changed for the sake of a more expressive and type-safe API:\n 1 2 3 4 5 // record CustomerId(long id) {} public interface CustomerService { void incrementLoyaltyPoints(CustomerId customerId, long orderValue); }    We now create a custom runtime image for the application. But we\u0026#8217;re at the end of a tough week, so accidentally we add version 2 of the customer module and the unchanged order module:\n 1 2 3 4 $ $JAVA_HOME/bin/jlink \\ --module-path=path/to/customer-2.0.0.jar:path/to/order-1.0.0.jar \\ --add-modules=com.example.order \\ --output=target/runtime-image    Note that jlink won\u0026#8217;t complain about this and create the runtime image. When executing the application via the image we\u0026#8217;re in for a bad surprise, though (slightly modified for the sake of readability):\n 1 2 3 4 5 $ ./target/runtime-image/bin/java com.example.order.OrderService Exception in thread \"main\" java.lang.NoSuchMethodError: 'void c.e.customer.CustomerService.incrementLoyaltyPoints(long, long)' at com.example.order@1.0.0/c.e.order.OrderService.main(OrderService.java:5)    This might be surprising at first; while jlink and the module system in general put a strong emphasis on reliability and e.g. flag referenced yet missing modules, mismatching API signatures like this are not raised as an issue and will only show up as an error at application run-time.\n Indeed, when I did a quick non-representative poll about this on Twitter, it turned out that more than 40% of participants were not aware of this pitfall:\n    Needless to say that it\u0026#8217;d be much more desirable to spot this error already early on at link time, before shipping the affected application to production, and suffering from all the negative consequences associated to that.\n   The API Signature Check jlink Plug-in While jlink doesn\u0026#8217;t detect this kind of API signature mismatch by itself, it comes with a plug-in API, which allows to hook into and enrich the linking process. By creating a custom jlink plug-in, we can implement the API signature check and fail the image creation process when detecting any invalid method references like the one above.\n Unfortunately though, the plug-in mechanism isn\u0026#8217;t an official, supported API at this point. As a matter of fact, it is not even exported within jlink\u0026#8217;s own module definition. With the right set of javac/java flags and the help of a small Java agent, it is possible though to compile custom plug-ins and have them picked up by jlink. To learn more about the required sorcery, check out this blog post which I wrote a while ago over on the Hibernate team blog.\n Let\u0026#8217;s start with creating the basic structure of the plug-in implementation class:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import jdk.tools.jlink.plugin.Plugin; public class SignatureCheckPlugin implements Plugin { @Override public String getName() { (1) return \"check-signatures\"; } @Override public Category getType() { (2) return Category.VERIFIER; } @Override public String getDescription() { (3) return \"Checks the API references amongst the modules of \" + \"an application for consistency\"; } }      1 Returns the name for the option to enable this plug-in when running the jlink command   2 Returns the category of this plug-in, which impacts the ordering within the plug-in stack (other types include TRANSFORMER, FILTER, etc.)   3 A description which will be shown when listing all plug-ins    There are a few more optional methods which we could implement, e.g. if the plug-in had any parameters for controlling its behaviors, or if we wanted it to be enabled by default. But as that\u0026#8217;s not the case for the plug-in at hand, the only method that\u0026#8217;s missing is transform(), which does the actual heavy-lifting of the plug-in\u0026#8217;s work.\n Now implementing the complete rule set of the JVM applied when loading and linking classes at run-time would be a somewhat daunting task. As I am lazy and this is just meant to be a basic PoC, I\u0026#8217;m going to limit myself to the detection of mismatching signatures of invoked methods, as shown in the customer/order example above. The reason being that this task can be elegantly delegated to an existing tool (I told you, I\u0026#8217;m lazy): Animal Sniffer.\n While typically used as build tool plug-in for verifying that classes built on a newer JDK version can also be executed with older Java versions (and as such mostly obsoleted by the JDK\u0026#8217;s --release option), Animal Sniffer also provides an API for creating and verifying custom signatures. This comes in handy for our jlink plug-in implementation.\n The general design of the transform() mechanism is that of a classic input-process-output pipeline. The method receives a ResourcePool object, which allows to traverse and examine the set of resources going into the image, such as class files, resource bundles, or manifests. A new resource pool is to be returned, which could contain exactly the same resources as the original one (as in our case); but of course it could also contain less or newly generated resources, or modified ones:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 @Override public ResourcePool transform(ResourcePool in, ResourcePoolBuilder out) { try { byte[] signature = createSignature(in); (1) boolean broken = checkSignature(in, signature); (2) if (broken) { (3) throw new PluginException(\"There are API signature \" + \"inconsistencies, please check the logs\"); } } catch(PluginException e) { throw e; } catch(Exception e) { throw new RuntimeException(e); } in.transformAndCopy(e -\u0026gt; e, out); (4) return out.build(); } /** * Creates a signature for all classes in the resource pool. */ private byte[] createSignature(ResourcePool in) throws IOException { ByteArrayOutputStream signatureStream = new ByteArrayOutputStream(); var builder = new StreamSignatureBuilder(signatureStream, new PrintWriterLogger(System.out)); in.entries() (5) .filter(e -\u0026gt; isClassFile(e) \u0026amp;\u0026amp; !isModuleInfo(e)) .forEach(e -\u0026gt; builder.process(e.path(), e.content())); builder.close(); return signatureStream.toByteArray(); } /** * Checks all classes against the given signature. */ private boolean checkSignature(ResourcePool in, byte[] signature) throws IOException { var checker = new StreamSignatureChecker( new ByteArrayInputStream(signature), Collections.\u0026lt;String\u0026gt;emptySet(), new PrintWriterLogger(System.out) ); checker.setSourcePath(Collections.\u0026lt;File\u0026gt;emptyList()); in.entries() (6) .filter(e -\u0026gt; isClassFile(e) \u0026amp;\u0026amp; !isModuleInfo(e) \u0026amp;\u0026amp; !isJdkClass(e)) .forEach(e -\u0026gt; checker.process(e.path(), e.content())); return checker.isSignatureBroken(); } private boolean isJdkClass(ResourcePoolEntry e) { return e.path().startsWith(\"/java.\") || e.path().startsWith(\"/javax.\") || e.path().startsWith(\"/jdk.\"); } private boolean isModuleInfo(ResourcePoolEntry e) { return e.path().endsWith(\"module-info.class\"); } private boolean isClassFile(ResourcePoolEntry e) { return e.path().endsWith(\"class\"); }      1 Create an Animal Sniffer signature for all the APIs in modules added to the runtime image   2 Verify all classes against that signature   3 If there\u0026#8217;s a signature violation, fail the jlink execution by raising a PluginException   4 All classes are passed on as-is   5 Feed each class to Animal Sniffer\u0026#8217;s signature builder for creating the signature; non-class resources and module descriptors are ignored   6 Verify each class against the signature; JDK classes can be skipped here, we assume there\u0026#8217;s no inconsistencies amongst the JDK\u0026#8217;s own modules    The input resource pool is traversed twice: first to create an Animal Sniffer signature of all the APIs, then a second time to validate the image\u0026#8217;s classes against that signature.\n Let me re-iterate that this a very basic, PoC-level implementation of link time API signature validation. A number of incompatibilities would not be detected by this, e.g. adding an abstract method to a superclass or interface, modifying the number and specification of the type parameters of a class, and others. The implementation could also be further optimized by validating only cross-module references. Still, this implementation is good enough to demonstrate the general principle and advantages of link time API consistency validation.\n With the implementation in place (see the README in the PoC\u0026#8217;s GitHub repository for details on building the project), it\u0026#8217;s time to invoke jlink again, this time activating the new plug-in. Now, as mentioned before, the jlink plug-in API isn\u0026#8217;t publicly exposed as of Java 15 (the current Java version at the point of writing), which means we need to jump some hoops in order to enable the plug-in and expose it to the jlink tool itself.\n In a nutshell, a Java agent can be used to bend the module configurations as needed. Details can be found in aforementioned post on the Hibernate blog (the agent\u0026#8217;s source code is here). The required boiler plate can be nicely encapsulated within a shell function:\n 1 2 3 4 5 6 function myjlink { \\ $JAVA_HOME/bin/jlink \\ -J-javaagent:signature-check-jlink-plugin-registration-agent-1.0-SNAPSHOT.jar \\ -J--module-path=signature-check-jlink-plugin-1.0-SNAPSHOT.jar:path/to/animal-sniffer-1.19.jar:path/to/asm-9.0.jar \\ -J--add-modules=dev.morling.jlink.plugins.sigcheck \"$@\" \\ }    All the -J options are VM options passed through to the jlink tool, in order to register the required Java agent and add the plug-in module to jlink\u0026#8217;s module path. Instead of directly calling jlink binary itself, this wrapper function can now be used to invoke jlink with the custom plug-in. Let\u0026#8217;s first take a look at the description in the plug-in list:\n 1 2 3 4 5 6 7 8 9 10 11 $ myjlink --list-plugins ... Plugin Name: check-signatures Plugin Class: dev.morling.jlink.plugins.sigcheck.SignatureCheckPlugin Plugin Module: dev.morling.jlink.plugins.sigcheck Category: VERIFIER Functional state: Functional. Option: --check-signatures Description: Checks the API references amongst the modules of an application for consistency ...    Now let\u0026#8217;s try and create the runtime image with the mismatching customer and order modules again:\n 1 2 3 4 5 6 7 8 9 10 myjlink --module-path=path/to/customer-2.0.0.jar:path/to/order-1.0.0.jar \\ --add-modules=com.example.order \\ --output=target/runtime-image \\ --check-signatures [INFO] Wrote signatures for 6156 classes. [ERROR] /com.example.order/com/example/order/OrderService.class:5: Undefined reference: void com.example.customer.CustomerService .incrementLoyaltyPoints(long, long) Error: Signature violations, check the logs    Et voilà! The mismatching signature of the incrementLoyaltyPoints() method was spotted and the creation of the runtime image failed. Now we could take action, examine our module path and make sure to feed correctly matching versions of the customer and order modules to the image creation process.\n   Summary The link time phase\u0026#8201;\u0026#8212;\u0026#8201;added to the Java platform as part of the module system in version 9, and positioned between the well-known compile time and run-time phases\u0026#8201;\u0026#8212;\u0026#8201;opens up very interesting opportunities to apply whole-world optimizations and validations to Java applications. One example is the checking the API definitions and usages across the different modules of a Java application for consistency. By means of a custom plug-in for the jlink tool, this validation can happen at link time, allowing to detect any mismatches when assembling an application, so that this kind of error can be fixed early on, before it hits an integration test or even production environment.\n This is particularly interesting when using the Java module system for building large, modular monolithic applications. Unless you\u0026#8217;re working with custom module layers\u0026#8201;\u0026#8212;\u0026#8201;e.g. via the Layrry launcher\u0026#8201;\u0026#8212;\u0026#8201;only one version of a given module may be present on the module path. If multiple modules of an application depend on different versions of a transitive dependency, link time API signature validation can help to identify inconsistencies caused by converging to a single version of that dependency.\n The approach can also help saving build time; when only modifying a single module of a larger modularized application, instead of re-compiling everything from scratch, you could just re-build that single module. Then, when re-creating the runtime image using this module and the other existing ones, you would be sure that all module API signature definitions and usages still match.\n The one caveat is the fact that the jlink plug-in API isn\u0026#8217;t a public, supported API of the JDK yet. I hope this is going to change some time soon, though. E.g. the next planned LTS release, Java 17, would be a great opportunity for officially adding the ability to build and use custom jlink plug-ins. This would open the road towards more wide-spread use of link time optimizations and validations, beyond those provided by the JDK and the jlink tool itself.\n Until then, you can explore this area starting from the source code of the signature check plug-in and its accompanying Java agent for enabling its usage with jlink.\n  ","id":7,"publicationdate":"Dec 28, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eDiscussions around Java\u0026#8217;s \u003ca href=\"https://openjdk.java.net/jeps/282\"\u003ejlink\u003c/a\u003e tool typically center around savings in terms of (disk) space.\nInstead of shipping an entire JDK,\na custom runtime image created with jlink contains only those JDK modules which an application actually requires,\nresulting in smaller distributables and \u003ca href=\"blog/smaller-faster-starting-container-images-with-jlink-and-appcds/\"\u003econtainer images\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eBut the contribution of jlink\u0026#8201;\u0026#8212;\u0026#8201;as a part of the Java module system at large\u0026#8201;\u0026#8212;\u0026#8201;to the development of Java application\u0026#8217;s is bigger than that:\nwith the notion of \u003cem\u003elink time\u003c/em\u003e it defines an optional complement to the well known phases \u003cem\u003ecompile time\u003c/em\u003e and application \u003cem\u003erun-time\u003c/em\u003e:\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"quoteblock\"\u003e\n\u003cblockquote\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eLink time is an opportunity to do whole-world optimizations that are otherwise difficult at compile time or costly at run-time. An example would be to optimize a computation when all its inputs become constant (i.e., not unknown). A follow-up optimization would be to remove code that is no longer reachable.\u003c/p\u003e\n\u003c/div\u003e\n\u003c/blockquote\u003e\n\u003c/div\u003e","tags":null,"title":"jlink's Missing Link: API Signature Validation","uri":"https://www.morling.dev/blog/jlinks-missing-link-api-signature-validation/"},{"content":"The other day, a user in the Debezium community reported an interesting issue; They were using Debezium with Java 1.8 and got an odd NoSuchMethodError:\n java.lang.NoSuchMethodError: java.nio.ByteBuffer.position(I)Ljava/nio/ByteBuffer; at io.debezium.connector.postgresql.connection.Lsn.valueOf(Lsn.java:86) at io.debezium.connector.postgresql.connection.PostgresConnection.tryParseLsn(PostgresConnection.java:270) at io.debezium.connector.postgresql.connection.PostgresConnection.parseConfirmedFlushLsn(PostgresConnection.java:235) ...   A NoSuchMethodError typically is an indication for a mismatch of the Java version used to compile some code, and the Java version used for running it: some method existed at compile time, but it\u0026#8217;s not available at runtime.\n Now indeed we use JDK 11 for building the Debezium code base, while targeting Java 1.8 as the minimal required version at runtime. But there is a method position(int) defined on the Buffer class (which ByteBuffer extends) also in Java 1.8. And as a matter of fact, the Debezium code compiles just fine with that version, too. So why would the user run into this error then?\n To understand what\u0026#8217;s going on, let\u0026#8217;s create a very simple class for reproducing the issue:\n 1 2 3 4 5 6 7 8 9 import java.nio.ByteBuffer; public class ByteBufferTest { public static void main(String... args) { ByteBuffer buffer = ByteBuffer.wrap(new byte[] { 1, 2, 3 }); buffer.position(1); (1) System.out.println(buffer.get()); } }      1 Why does this not work with Java 1.8 when compiled with JDK 9 or newer?    Compile this with a current JDK:\n $ javac --source 1.8 --target 1.8 ByteBufferTest.java   And sure enough, the NoSuchMethodError shows up when running this with Java 1.8:\n $ java ByteBufferTest Exception in thread \"main\" java.lang.NoSuchMethodError: java.nio.ByteBuffer.position(I)Ljava/nio/ByteBuffer; at ByteBufferTest.main(ByteBufferTest.java:6)   Whereas, when using 1.8 to compile and run this code, it just works fine. Now, if we take a closer look at the error message again, the missing method is defined as ByteBuffer position(int). I.e. for an invoked method like position(), not only its name, parameter type(s), and the name of the declaring class are part of the byte code for that invocation, but also the method\u0026#8217;s return type. A look at the byte code of the class using javap confirms that:\n $ javap -p -c -s -v -l -constants ByteBufferTest ... public static void main(java.lang.String...); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC, ACC_VARARGS Code: stack=4, locals=2, args_size=1 ... 19: aload_1 20: iconst_1 21: invokevirtual #13 // Method java/nio/ByteBuffer.position:(I)Ljava/nio/ByteBuffer; ...   And this points us to the right direction; In Java 1.8, indeed there is no such method, only the position() method on Buffer, which, of course, returns Buffer and not ByteBuffer. Whereas since Java 9, this method (and several others) is overridden in ByteBuffer\u0026#8201;\u0026#8212;\u0026#8201;leveraging Java\u0026#8217;s support for co-variant return types\u0026#8201;\u0026#8212;\u0026#8201;to return ByteBuffer. The Java compiler will now select that method, ByteBuffer position(int), and record that as the invoked method signature in the byte code of the caller class.\n This is per-se a nice usability improvement, as it allows to invoke further ByteBuffer methods on the return value, instead of just those methods declared by Buffer. But as we\u0026#8217;ve seen, it comes with this little surprise when compiling code on JDK 9 or newer, while trying to keep compatibility with older Java versions. And as it turns out, we were not the first or only ones to encounter this issue. Quite a few open-source projects ran into this, e.g. Eclipse Jetty, Apache Pulsar, Eclipse Vert.x, Apache Thrift, the Yugabyte DB client, and a few others.\n How to Prevent This Situation? So what can you do in order to prevent this issue from happening? One first idea could be to enforce selection of the right method by casting to Buffer:\n 1 ((java.nio.Buffer) buffer).position(1);    But while this produces the desired byte code indeed, it isn\u0026#8217;t exactly the best way for doing so. You\u0026#8217;d have to remember to do so for every invocation of any of the affected ByteBuffer methods, and the seemling unneeded cast might be an easy target for some \"clean-up\" by unsuspecting co-workers on our team.\n Luckily, there\u0026#8217;s a much better way, and this is to rely on the Java compiler\u0026#8217;s --release parameter, which was introduced via JEP 247 (\"Compile for Older Platform Versions\"), added to the platform also in JDK 9. In contrast to the more widely known pair of --source and --target, the --release switch will ensure that only byte code is produced which actually will be useable with the specified Java version. For this purpose, the JDK contains the signature data for all supported Java versions (stored in the $JAVA_HOME/lib/ct.sym file).\n So all that\u0026#8217;s needed really is compiling the code with --release=8:\n $ javac --release=8 ByteBufferTest.java   Examine the bytecode using javap again, and now the expected signature is in place:\n 21: invokevirtual #13 // Method java/nio/ByteBuffer.position:(I)Ljava/nio/Buffer;   When run on Java 1.8, this virtual method call will be resolved to Buffer#position(int) at runtime, whereas on Java 9 and later, it\u0026#8217;d resolve to the bridge method inserted by the compiler into the class file of ByteBuffer due to the co-variant return type, which itself calls the overriding ByteBuffer#position(int) method.\n Now let\u0026#8217;s see what happens if we actually try to make use of the overriding method version in ByteBuffer by re-assigning the result:\n 1 2 3 4 ... ByteBuffer buffer = ByteBuffer.wrap(new byte[] { 1, 2, 3 }); buffer = buffer.position(1); ...    Et voilà, this gets rejected by the compiler when targeting Java 1.8, as the return type of the JDK 1.8 method Buffer#position(int) cannot be assigned to ByteBuffer:\n $ javac --release=8 ByteBufferTest.java ByteBufferTest.java:6: error: incompatible types: Buffer cannot be converted to ByteBuffer buffer = buffer.position(1);   To cut a long story short, we\u0026#8201;\u0026#8212;\u0026#8201;and many other projects\u0026#8201;\u0026#8212;\u0026#8201;should have used the --release switch instead of --source/--target, and the user would not have had that issue. In order to achieve the same in your Maven-based build, just specify the following property in your pom.xml:\n 1 2 3 4 5 ... \u0026lt;properties\u0026gt; \u0026lt;maven.compiler.release\u0026gt;8\u0026lt;/maven.compiler.release\u0026gt; \u0026lt;/properties\u0026gt; ...    Note that theoretically you could achieve the same effect also when using --source and --target; by means of the --boot-class-path option, you could advise the compiler to use a specific set of bootstrap class files instead of those from the JDK used for compilation. But that\u0026#8217;d be quite a bit more cumbersome as it requires you to actually provide the classes of the targeted Java version, whereas --release will make use of the signature data coming with the currently used JDK itself.\n  ","id":8,"publicationdate":"Dec 21, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe other day, a user in the \u003ca href=\"https://debezium.io/\"\u003eDebezium\u003c/a\u003e community reported an interesting issue;\nThey were using Debezium with Java 1.8 and got an odd \u003ccode\u003eNoSuchMethodError\u003c/code\u003e:\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"ByteBuffer and the Dreaded NoSuchMethodError","uri":"https://www.morling.dev/blog/bytebuffer-and-the-dreaded-nosuchmethoderror/"},{"content":"Functional unit and integration tests are a standard tool of any software development organization, helping not only to ensure correctness of newly implemented code, but also to identify regressions\u0026#8201;\u0026#8212;\u0026#8201;bugs in existing functionality introduced by a code change. The situation looks different though when it comes to regressions related to non-functional requirements, in particular performance-related ones: How to detect increased response times in a web application? How to identify decreased throughput?\n These aspects are typically hard to test in an automated and reliable way in the development workflow, as they are dependent on the underlying hardware and the workload of an application. For instance assertions on the duration of specific requests of a web application typically cannot be run in a meaningful way on a developer laptop, which differs from the actual production hardware (ironically, nowadays both is an option, the developer laptop being less or more powerful than the actual production environment). When run in a virtualized or containerized CI environment, such tests are prone to severe measurement distortions due to concurrent load of other applications and jobs.\n This post introduces the JfrUnit open-source project, which offers a fresh angle to this topic by supporting assertions not on metrics like latency/throughput themselves, but on indirect metrics which may impact those. JfrUnit allows you define expected values for metrics such as memory allocation, database I/O, or number of executed SQL statements, for a given workload and asserts the actual metrics values\u0026#8201;\u0026#8212;\u0026#8201;which are obtained from JDK Flight Recorder events\u0026#8201;\u0026#8212;\u0026#8201;against these expected values. Starting off from a defined base line, future failures of such assertions are an indicator for potential performance regressions in an application, as a code change may have introduced higher GC pressure, the retrieval of unneccessary data from the database, or SQL problems commonly induced by ORM tools, like N+1 SELECT statements.\n JfrUnit provides means of identifying and analyzing such anomalies in a reliable, environment independent way in standard JUnit tests, before they manifest as actual performance regressions in production. Test results are independent from wall clock time and thus provide actionable information, also when not testing with production-like hardware and data volumes.\n This post is a bit longer than usual (I didn\u0026#8217;t have the time to write shorter ;), but it\u0026#8217;s broken down into several sections, so you can pause and continue later on with fresh energy:\n   Getting Started With JfrUnit\n  Case Study 1: Spotting Increased Memory Allocation\n  Case Study 2: Identifying Increased I/O With the Database\n  Discussion\n  Summary and Outlook\n   Getting Started With JfrUnit JfrUnit is an extension for JUnit 5 which integrates Flight Recorder into unit tests; it makes it straight forward to initiate a JFR recording for a given set of event types, execute some test routine, and then assert the JFR events which should have been produced.\n Here is a basic example of a JfrUnit test:\n @JfrEventTest (1) public class JfrUnitTest { public JfrEvents jfrEvents = new JfrEvents(); @Test @EnableEvent(\"jdk.GarbageCollection\") (2) @EnableEvent(\"jdk.ThreadSleep\") public void shouldHaveGcAndSleepEvents() throws Exception { System.gc(); Thread.sleep(1000); jfrEvents.awaitEvents(); (3) ExpectedEvent event = event(\"jdk.GarbageCollection\"); (4) assertThat(jfrEvents).contains(event); event = event(\"jdk.GarbageCollection\") (4) .with(\"cause\", \"System.gc()\")); assertThat(jfrEvents).contains(event); event = event(\"jdk.ThreadSleep\"). with(\"time\", Duration.ofSeconds(1))); assertThat(jfrEvents).contains(event); assertThat(jfrEvents.ofType(\"jdk.GarbageCollection\")).hasSize(1); (5) } }     1 @JfrEventTest marks this as a JfrUnit test, activating its extension   2 All JFR event types to be recorded must be enabled via @EnableEvent   3 After running the test logic, awaitEvents() must be invoked as a synchronization barrier, making sure all previously produced events have been received   4 Using the JfrEventsAssert#event() method, an ExpectedEvent instance can be created\u0026#8201;\u0026#8212;\u0026#8201;optionally specifying one or more expected attribute values\u0026#8201;\u0026#8212;\u0026#8201;which then is asserted via JfrEventsAssert#assertThat()   5 JfrEvents#ofType() allows to filter on specific event types, enabling arbitrary assertions against the returned stream of RecordedEvents    By means of a custom assertThat() matcher method for AssertJ, JfrUnit allows to validate that specific JFR events are raised during at test. Events to be matched are described via their event type name, and optionally one more event attribute vaues. As we\u0026#8217;ll see in a bit, JfrUnit also integrates nicely with the Java Stream API, allowing you to filter and aggregate recorded event atribute values and match them against expected values.\n JfrUnit persists a JFR recording file for each test method, which you can examine after a test failure, for instance using JDK Mission Control. To learn more about JfrUnit and its capabilities, take a look at the project\u0026#8217;s README. The project is in an early proof-of-concept stage at the moment, so changes to its APIs and semantics are likely.\n Now that you\u0026#8217;ve taken the JfrUnit quick tour, let\u0026#8217;s put that knowledge into practice. Our example project will be the Todo Manager Quarkus application you may already be familiar with from my earlier post about custom JFR events. We\u0026#8217;re going to discuss two examples for using JfrUnit to identify potential performance regressions.\n   Case Study 1: Spotting Increased Memory Allocation At first, let\u0026#8217;s explore how to identify increased memory allocation rates. Typically, it\u0026#8217;s mostly library and middleware authors who are interested in this. For a library such as Hibernate ORM it can make a huge difference whether a method that is invoked many times on a hot code path allocates a few objects more or less. Less object allocations mean less work for the garbage collector, which in turn means those precious CPU cores of your machine can spend more cycles processing your actual business logic.\n But also for application developers it can be beneficial to keep an eye on\u0026#8201;\u0026#8212;\u0026#8201;and systematically track\u0026#8201;\u0026#8212;\u0026#8201;object allocations, as regressions there lead to increased GC pressure, and in turn eventually to higher latencies and reduced throughput.\n The key for tracking object allocations with JFR are the jdk.ObjectAllocationInNewTLAB and jdk.ObjectAllocationOutsideTLAB events, which are emitted when\n   an object allocation triggered the creation of a new thread-local allocation buffer (TLAB)\n  an object got allocated outside of the thread\u0026#8217;s TLAB\n       Thread-local allocation buffers (TLAB) When creating new object instances on the heap, this primarily happens via thread-local allocation buffers. A TLAB is a pre-allocated memory block that\u0026#8217;s exclusively used by a single thread. Since this space is exclusively owned by the thread, creating new objects within a TLAB can happen without costly synchronization with other threads. Once a thread\u0026#8217;s current TLAB capacity is about to be exceeded by a new object allocation, a new TLAB will be allocated for that thread. In addition, large objects will typically need to be directly allocated outside of the more efficient TLAB space.\n To learn more about TLAB allocation, refer to part #4 of Aleksey Shipilёv\u0026#8217;s \"JVM Anatomy Quark\" blog series.\n     Note these events don\u0026#8217;t allow for tracking of each individual object allocation, as multiple objects will be allocated within a TLAB before a new one is required, and thus the jdk.ObjectAllocationInNewTLAB event will be emitted. But as that event exposes the size of the new TLAB, we can keep track of the overall amount of memory that\u0026#8217;s allocated while the application is running.\n In that sense, jdk.ObjectAllocationInNewTLAB represents a sampling of object allocations, which means we need to collect a reasonable number of events to identify those locations in the program which are the sources of high object allocation and thus frequently trigger new TLAB creations.\n So let\u0026#8217;s start and work on a test for spotting regressions in terms of object allocations of one of the Todo Manager app\u0026#8217;s API methods, GET /todo/{id}. To identify a baseline of the allocation to be expected, we first invoke that method in a loop and print out the actual allocation values. This should happen in intervals, e.g. every 10,000 invocations, so to average out numbers from individual API calls.\n @Test @EnableEvent(\"jdk.ObjectAllocationInNewTLAB\") (1) @EnableEvent(\"jdk.ObjectAllocationOutsideTLAB\") public void retrieveTodoBaseline() throws Exception { Random r = new Random(); HttpClient client = HttpClient.newBuilder() .build(); for (int i = 1; i\u0026lt;= 100_000; i++) { executeRequest(r, client); if (i % 10_000 == 0) { jfrEvents.awaitEvents(); (2) long sum = jfrEvents.filter(this::isObjectAllocationEvent) (3) .filter(this::isRelevantThread) .mapToLong(this::getAllocationSize) .sum(); System.out.printf( Locale.ENGLISH, \"Requests executed: %s, memory allocated: (%,d bytes/request)%n\", i, sum/10_000 ); jfrEvents.reset(); (4) } } private void executeRequest(Random r, HttpClient client) throws Exception { int id = r.nextInt(20) + 1; HttpRequest request = HttpRequest.newBuilder() .uri(new URI(\"http://localhost:8081/todo/\" + id)) .headers(\"Content-Type\", \"application/json\") .GET() .build(); HttpResponse\u0026lt;String\u0026gt; response = client .send(request, HttpResponse.BodyHandlers.ofString()); assertThat(response.statusCode()).isEqualTo(200); } private boolean isObjectAllocationEvent(RecordedEvent re) { (5) String name = re.getEventType().getName(); return name.equals(\"jdk.ObjectAllocationInNewTLAB\") || name.equals(\"jdk.ObjectAllocationOutsideTLAB\"); } private long getAllocationSize(RecordedEvent re) { (6) return re.getEventType().getName() .equals(\"jdk.ObjectAllocationInNewTLAB\") ? re.getLong(\"tlabSize\") : re.getLong(\"allocationSize\"); } private boolean isRelevantThread(RecordedEvent re) { (7) return re.getThread().getJavaName().startsWith(\"vert.x-eventloop\") || re.getThread().getJavaName().startsWith(\"executor-thread\"); } }     1 Enable the jdk.ObjectAllocationInNewTLAB and jdk.ObjectAllocationOutsideTLAB JFR events   2 Every 10,000 events, wait for all the JFR events produced so far   3 Calculate the total allocation size, by summing up the TLAB allocations of all relevant threads   4 Reset the event stream for the next iteration   5 Is this a TLAB event?   6 Get the new TLAB size in case of an in TLAB allocation, otherwise the allocated object size out of TLAB   7 We\u0026#8217;re only interested in the web application\u0026#8217;s own threads, in particular ignoring the main thread which runs the HTTP client of the test    Note that unlike in the initial example showing the usage of JfrUnit, here we\u0026#8217;re not using the simple contains() AssertJ matcher, but rather calculate some custom value\u0026#8201;\u0026#8212;\u0026#8201;the overall object allocation in bytes\u0026#8201;\u0026#8212;\u0026#8201;by means of filtering and aggregating the relevant JFR events.\n Here are the numbers I got from running 100,000 invocations:\n Requests executed: 10000, memory allocated: 34096 bytes/request Requests executed: 20000, memory allocated: 31768 bytes/request Requests executed: 30000, memory allocated: 31473 bytes/request Requests executed: 40000, memory allocated: 31462 bytes/request Requests executed: 50000, memory allocated: 31547 bytes/request Requests executed: 60000, memory allocated: 31545 bytes/request Requests executed: 70000, memory allocated: 31537 bytes/request Requests executed: 80000, memory allocated: 31624 bytes/request Requests executed: 90000, memory allocated: 31703 bytes/request Requests executed: 100000, memory allocated: 31682 bytes/request   As we see, there\u0026#8217;s some warm-up phase during which allocation rates still go down, but after ~20 K requests, the allocation per request is fairly stable, with a volatility of ~1% when averaged out over 10K requests. This means that this initial phase should be excluded during the actual test.\n To emphasize the key part again, this allocation is per request, it is independent from wall clock time and thus is neither dependent from the machine running the test (i.e. the test should behave the same when running on a developer laptop and on a CI machine), nor is it subject to volatility induced by other workloads running concurrently.\n     Tracking Object Allocations in Java 16 The two TLAB allocation events provide all the information required for analysing object allocations in Java applications, but often it\u0026#8217;s not practical to enable them on a continuous basis when running in production. Due to the high amount of events produced, enabling them adds some overhead in terms of latency, also the size of JFR recording files can be hard to predict.\n Both issues are addressed by a JFR improvement that\u0026#8217;s proposed for inclusion into Java 16, \"JFR Event Throttling\". This will provide control over the emission rate of events, e.g. allowing to sample object allocations with a defined rate of 100 events per second, which addresses both the overhead as well as the recording size issue. A new event type, jdk.ObjectAllocationSample will be added, too, which will be enabled in the JFR default configuration.\n For JfrUnit, explicit control over the event sampling rate will be a very interesting capability, as a higher sampling rate may lead to stable results more quickly, in turn resulting in shorter test execution times.\n     Based on that, the actual test could look like so:\n @Test @EnableEvent(\"jdk.ObjectAllocationInNewTLAB\") @EnableEvent(\"jdk.ObjectAllocationOutsideTLAB\") public void retrieveTodo() throws Exception { Random r = new Random(); HttpClient client = HttpClient.newBuilder().build(); for (int i = 1; i\u0026lt;= 20_000; i++) { (1) executeRequest(r, client); } jfrEvents.awaitEvents(); jfrEvents.reset(); for (int i = 1; i\u0026lt;= 10_000; i++) { (2) executeRequest(r, client); } jfrEvents.awaitEvents(); long sum = jfrEvents.filter(this::isObjectAllocationEvent) .filter(this::isRelevantThread) .mapToLong(this::getAllocationSize) .sum(); assertThat(sum / 10_000).isLessThan(33_000); (3) }     1 Warm-up phase   2 The actual test phase   3 Assert the memory allocation per request is within the expected boundary; note we could also add a lower boundary, so to make sure we notice any future improvements (e.g. caused by upgrading to new efficient versions of a library), which otherwise may hide subsequent regressions    Now let\u0026#8217;s assume we\u0026#8217;ve wrapped up the initial round of work on this application, and its tests have been passing on CI for a while. One day, the retrieveTodo() performance test method fails though:\n java.lang.AssertionError: Expecting: \u0026lt;388370L\u0026gt; to be less than: \u0026lt;33000L\u0026gt;   Ugh, it\u0026#8217;s suddenly allocating about ten times more memory per request than before! What has happened? To find the answer, we can take a look at the test\u0026#8217;s JFR recording, which JfrUnit persists under target/jfrunit:\n ls target/jfrunit dev.morling.demos.quarkus.TodoResourcePerformanceTest-createTodo.jfr dev.morling.demos.quarkus.TodoResourcePerformanceTest-retrieveTodo.jfr   Let\u0026#8217;s open the *.jfr file for the failing test in JDK Mission Control (JMC) in order to analyse all the recorded events (note that the recording will always contain some JfrUnit-internal events which are needed for synchronizing the recording stream and the events exposed to the test).\n When taking a look at the TLAB events of the application\u0026#8217;s executor thread, the culprit is identified quickly; a lot of the sampled TLAB allocations contain this stack trace (click on the image to enlarge):\n   Interesting, REST Assured loading a Jackson object mapper, what\u0026#8217;s going on there? Here\u0026#8217;s the full stacktrace:\n   So it seems a REST call to another service is made from within the TodoResource#get(long) method! At this point we know where to look into the source code of the application:\n @GET @Transactional @Produces(MediaType.APPLICATION_JSON) @Path(\"/{id}\") public Response get(@PathParam(\"id\") long id) throws Exception { Todo res = Todo.findById(id); User user = RestAssured.given().port(8082) .when() .get(\"/users/\" + res.userId) .as(User.class); res.userName = user.name; return Response.ok() .entity(res) .build(); }   Gasp, it looks like a developer on the team has been taking the microservices mantra a bit too far, and has changed the code so it invokes another service in order to obtain some additional data associated to the user who created the retrieved todo.\n While that\u0026#8217;s problematic in its own right due to the inherent coupling between the two services (how should the Todo Manager service react if the user service isn\u0026#8217;t available?), they made matters worse by using the REST Assured API as a REST client, in a less than ideal way. The API\u0026#8217;s simplicity and elegance makes it a great solution for testing (and indeed that\u0026#8217;s its primary use case), but this particular usage seems to be not such a good choice for production code.\n At this point you should ask yourself whether the increased allocation per request actually is a problem for your application or not. To determine if that\u0026#8217;s the case, you could run some tests on actual request latency and throughput in a production-like environment. If there\u0026#8217;s no impact based on the workload you have to process, you might very well decide that additional allocations are well spent for your application\u0026#8217;s purposes.\n Increasing the allocation per request by a factor of ten in the described way quite likely does not fall into this category, though. At the very least, we should look into making the call against the User REST API more efficiently, either by setting up REST Assured in a more suitable way, or by looking for an alternative REST client. Of course the external API call just by itself adds to the request latency, which is something we might want to avoid.\n It\u0026#8217;s also worth examining the application\u0026#8217;s garbage collection behavior. In order to so, you can run the performance test method again, either enabling all the GC-related JFR event types, or by enabling a pre-existing JFR configuration (the JDK comes with two built-in JFR configurations, default and profile, but you can also create and export them via JMC):\n @Test @EnableConfiguration(\"profile\") public void retrieveTodo() throws Exception { // ... }   Note that the pre-defined configurations imply minimum durations for certain event types; e.g. the I/O events discussed in the next section will only be recorded if they have a duration of 20 ms or longer. Depending on your testing requirements, you may have to adjust and tweak the configuration to be used.\n Open the recording in JMC, and you\u0026#8217;ll see there\u0026#8217;s a substantial amount of GC activity happening:\n   The difference to the GC behavior before this code change is striking:\n   Pause times are worse, directly impacting the application\u0026#8217;s latency, and the largely increased GC volume means the production environment will be able to serve less concurrent requests when reaching its capacity limits, meaning you\u0026#8217;d have to provision another machine earlier on as your load increases.\n     Memory Leak in the JFR Event Streaming API The astute reader may have noticed that there is a memory leak before and after the code change, as indicated by the ever increased heap size post GC. After some exploration it turned out that this is a bug in the JFR event streaming API which holds on to a large number of RecordedEvent instances internally. Erik Gahlin from the OpenJDK team logged JDK-8257906 for tracking and hopefully fixing this in JDK 16.\n     Now such drastic increase of object allocation and thus potential impact on performance should hopefully be an exception rather than a regular situation. But the example shows how continuous performance unit tests on impacting metrics like memory allocation, using JfrUnit and JDK Flight Recorder and, can help to identify performance issues in an automated and reliable way, preventing such regression to sneak into production. Being able to identify this kind of issue by running tests locally on a developer laptop or a CI server can be a huge time-saver and productivity boost.\n   Case Study 2: Identifying Increased I/O With the Database Once you\u0026#8217;ve started to look at performance regression tests through the lense of JfrUnit, more and more possibilities pop up. Asserting a maximum number of garbage collections? Not a problem. Avoiding an unexpected amount of file system IO? The jdk.FileRead and jdk.FileWrite events are our friend. Examining and asserting the I/O done with the database? Easily doable. Assertions on application-specific JFR event types you\u0026#8217;ve defined yourself? Sure thing!\n You can find a complete list of all JFR event types by JDK version in this nice matrix created by Tom Schindl. The number of JFR event types is growing constantly; as of JDK 15, there are 157 different ones of them.\n Now let\u0026#8217;s take a look at assertions on database I/O, as the amount of data fetched from or written to the database often is a very impactful factor of an enterprise application\u0026#8217;s behavior. A regression here, e.g. fetching more data from the database than anticipated, may indicate that data is unnecessarily loaded. For instance it might be the case that a set of data is loaded only in order to filter it in the application subsequently, instead of doing so via SQL in the database itself, resulting in increased request durations.\n So how could such test look like for our GET /todo/{id} API call? The general approach is the same as before with memory allocations: first define a baseline of the bytes read and written by invoking the API under test for a given number of executions. Once that\u0026#8217;s done, you can implement the actual test, including an assertion on the expected number of bytes read or written:\n @Test @EnableEvent(value=\"jdk.SocketRead\", stackTrace=INCLUDED) (1) @EnableEvent(value=\"jdk.SocketWrite\", stackTrace=INCLUDED) public void retrieveTodo() throws Exception { Random r = new Random(); HttpClient client = HttpClient.newBuilder() .build(); for (int i = 1; i\u0026lt;= ITERATIONS; i++) { executeRequest(r, client); } jfrEvents.awaitEvents(); long count = jfrEvents.filter(this::isDatabaseIoEvent).count(); (2) assertThat(count / ITERATIONS).isEqualTo(4) .describedAs(\"write + read per statement, write + read per commit\"); long bytesReadOrWritten = jfrEvents.filter(this::isDatabaseIoEvent) .mapToLong(this::getBytesReadOrWritten) .sum(); assertThat(bytesReadOrWritten / ITERATIONS).isLessThan(250); (3) } private boolean isDatabaseIoEvent(RecordedEvent re) { (4) return ((re.getEventType().getName().equals(\"jdk.SocketRead\") || re.getEventType().getName().equals(\"jdk.SocketWrite\")) \u0026amp;\u0026amp; re.getInt(\"port\") == databasePort); } private long getBytesReadOrWritten(RecordedEvent re) { (5) return re.getEventType().getName().equals(\"jdk.SocketRead\") ? re.getLong(\"bytesRead\") : re.getLong(\"bytesWritten\"); }     1 Enable the jdk.SocketRead and jdk.SocketWrite event types; by default, those don\u0026#8217;t contain the stacktrace for the events, so that needs to be enabled explicitly   2 There should be four events per invocation of the API method   3 Less than 250 bytes I/O are expected per invocation   4 Only read and write events on the database port are relevant for this test, but e.g. not I/O on the web port of the application   5 Retrieve the value of the event\u0026#8217;s bytesRead or bytesWritten field, depending on the event type    Now let\u0026#8217;s again assume that after some time the test begins to fail. This time it\u0026#8217;s the assertion on the number of executed reads and writes:\n AssertionFailedError: Expecting: \u0026lt;18L\u0026gt; to be equal to: \u0026lt;4L\u0026gt; but was not.   Also the number of bytes read and written has substantially increased:\n java.lang.AssertionError: Expecting: \u0026lt;1117L\u0026gt; to be less than: \u0026lt;250L\u0026gt;   That\u0026#8217;s definitely something to look into. So let\u0026#8217;s open the recording of the failed test in Flight Recorder and take a look at the socket read and write events. Thanks to enabling stacktraces for the two JFR event types we can quite quickly identify the events asssociated to an invocation of the GET /todo/{id} API:\n   At this point, some familiarity with the application in question will come in handy to identify suspicous events. But even without that, we could compare previous recordings of successful test runs with the recording from the failing one in order to see where differences are. In the case at hand, the BlobInputStream and Hibernate\u0026#8217;s BlobTypeDescriptor in the call stack seem pretty unexpected, as our User entity didn\u0026#8217;t have any BLOB attribute before.\n In reality, comparing with the latest version and a look into the git history of that class could confirm that there\u0026#8217;s a new attribute storing an image (perhaps not a best practice to do so ;):\n @Entity public class Todo extends PanacheEntity { public String title; public int priority; public boolean completed; @Lob (1) public byte[] image; }     1 This looks suspicious!    We now would have to decide whether this image attribute actually should be loaded for this particular use case, (if so, we\u0026#8217;d have to adjust the test accordingly), or whether it would for instance make more sense to mark this property as a lazily loaded one and only retrieve it when actually required.\n Solely working with the raw socket read and write events can be a bit cumbersome, though. Wouldn\u0026#8217;t it be nice if we also had the actual SQL statement which caused this I/O? Glad you asked! Neither Hibernate nor the Postgres JDBC driver emit any JFR events at the moment (although well-informed sources are telling me that the Hibernate team wants to look into this). Therefore, in part two of this blog post series, we\u0026#8217;ll discuss how to instrument an existing library to emit events like this, using a Java agent, without modifying the library in question.\n   Discussion JfrUnit in conjunction with JDK Flight Recorder opens up a very interesting approach for identifying potential performance regressions in Java applications. Instead of directly measuring an application\u0026#8217;s performance metrics, most notably latency and throughput, the idea is to measure and assert metrics that impact the performance characteristics. This allows you to implement stable and reliable automated performance regression tests, whose outcome does not depend on the capabilities of the execution environment (e.g. number/size of CPUs), or other influential factors like concurrently running programs.\n Regressions in such impacting metrics, e.g. the amount of allocated memory, or bytes read from a database, are indicators that the application\u0026#8217;s performance may have degraded. This approach offers some interesting advantages over performance tests on actual latency and throughput themselves:\n   Hardware independent: You can identify potential regressions also when running tests on hardware which is different (e.g. less powerful) from the actual production hardware\n  Fast feedback cycle: Being able to run performance regression tests on developer laptops, even in the IDE, allows for fast identification of potential regressions right during development, instead of having to wait for the results of less frequently executed test runs in a traditional performance test lab environment\n  Robustness: Tests are robust and not prone to factors such as the load induced by parallel jobs of a CI server or a virtualized/containerized environment\n  Pro-active identification of performance issues: Asserting a metric like memory allocation can help to identify future performance problems before they actual materialize; while the additional allocation rate may make no difference with the system\u0026#8217;s load as of today, it may negatively impact latency and throughput as the system reaches its limits with increased load; being able to identify the increased allocation rate early on allows for a more efficient handling of the situation while working on the code, compared to when finding out about such regression only later on\n  Reduced need for warm-up: For traditional performance tests of Java-based applications, a thorough warm-up is mandatory, e.g. to ensure proper optimization of the JIT-compiled code. In comparison, metrics like file or database I/O are very stable for a defined workload, so that regressions can be identified also with just a single or a few executions\n   Needless to say, that you should be aware of the limitations of this approach, too:\n   No statement on user-visible performance metrics: Measuring and asserting performance-impacting factors doesn\u0026#8217;t tell you anything in terms of the user-visible performance characteristics themselves. While we can reason about guarantees like \"The system can handle 10K concurrent requests while the 99.9 percentile of requests has a latency of less than 250 ms\", that\u0026#8217;s not the case for metrics like memory allocation or I/O. What does it mean if an application allocates 100 KB of RAM for a particular use case? Is it a lot? Too much? Just fine?\n  Focused on identifying regressions: Somewhat related to the first point, this approach of testing is focused not on specific absolute values, but rather on identifying performance regressions. It\u0026#8217;s hard to tell whether 100 KB database I/O is good or bad for a particular web request, but a change from 100 KB to 200 KB might indicate that something is wrong\n  Focused on identifying potential regressions: A change in performance-impacting metrics does not necessarily imply an actual user-visible performance regression. For instance it might be acceptable for a specific request to allocate more RAM than it did before, if the production system generally isn\u0026#8217;t under high load and the additional GC effort doesn\u0026#8217;t matter in practice\n  Does not work for all performance-impacting metrics: Some performance metrics cannot be meaningfully asserted in plain unit tests; e.g. degraded throughput due to lock contention can typically only be identified with a reasonable number of concurrent requests\n  Only identifies regressions in the application itself: A traditional integrative performance test of an enterprise application will also capture issues in related components, such as the application\u0026#8217;s database. A query run with a sub-optimal execution plan won\u0026#8217;t be noticed with this testing approach\n  Volatile results for timer-based tasks: While metrics like object allocations should be stable e.g. for a specific web request, events which are timing-based, would yield more events on a slower environment than on a faster machine\n     Summary and Outlook JUnit tests based on performance-impacting factors can be a very useful part of the performance testing strategy for an application. They can help to identify potential performance regressions very early in the development lifecycle, when they can be fixed comparatively easy and cheap. Of course they are no silver bullet; you should consider them as complement for classic performance tests running on production-like hardware, not a replacement.\n The approach may feel a bit unfamiliar initially, and it may take some time to learn about the different metrics which can be measured with JFR and asserted via JfrUnit, as well as their implications on an application\u0026#8217;s performance characteristics. But once this hurdle is passed, continuous performance regression tests can be a valuable tool in the box of every software and performance engineer.\n JfrUnit is still in its infancy, and could evolve into a complete toolkit around automated test of JFR-based metrics. Ideas for future development include:\n   A more powerful \"built-in\" API which e.g. provides the functionality for calculating the total TLAB allocations of a given set of threads as a ready-to-use method\n  It could also be very interesting to run assertions against externally collected JFR recording files. This would allow to validate workloads which require more complex set-ups, e.g. running in a dedicated performance testing lab, or even from continuous recordings taken in production\n  The JFR event streaming API could be leveraged for streaming queries on live events streamed from a remote system\n  Another use case we haven\u0026#8217;t explored yet is the validation of resource consumption before and after a defined workload. E.g. after logging in and out a user 100 times, the system should roughly consume\u0026#8201;\u0026#8212;\u0026#8201;ignoring any initial growth after starting up\u0026#8201;\u0026#8212;\u0026#8201;the same amount of memory. A failure of such assertion would indicate a potential memory leak in the application\n  JfrUnit might automatically detect that certain metrics like object allocations are still undergoing some kind of warm-up phase and thus are not stable, and mark such tests as potentially incorrect or flaky\n  Keeping track of historical measurement data, e.g. allowing to identify regressions which got introduced step by step over a longer period of time, with one comparatively small change being the straw finally breaking the camel\u0026#8217;s back\n   Your feedback, feature requests, or even contributions to the project will be highly welcomed!\n Stay tuned for part two of this blog post, where we\u0026#8217;ll explore how to trace the SQL statements executed by an application using the JMC Agent and assert these query events using JfrUnit. This will come in very handy for instance for identifying common performance problems like N+1 SELECT statements.\n Many thanks to Hans-Peter Grahsl, John O\u0026#8217;Hara, Nitsan Wakart, and Sanne Grinovero for their extensive feedback while writing this blog post!\n  ","id":9,"publicationdate":"Dec 16, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eFunctional unit and integration tests are a standard tool of any software development organization,\nhelping not only to ensure correctness of newly implemented code,\nbut also to identify regressions\u0026#8201;\u0026#8212;\u0026#8201;bugs in existing functionality introduced by a code change.\nThe situation looks different though when it comes to regressions related to non-functional requirements, in particular performance-related ones:\nHow to detect increased response times in a web application?\nHow to identify decreased throughput?\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThese aspects are typically hard to test in an automated and reliable way in the development workflow,\nas they are dependent on the underlying hardware and the workload of an application.\nFor instance assertions on the duration of specific requests of a web application typically cannot be run in a meaningful way on a developer laptop,\nwhich differs from the actual production hardware\n(ironically, nowadays both is an option, the developer laptop being less or more powerful than the actual production environment).\nWhen run in a virtualized or containerized CI environment, such tests are prone to severe measurement distortions due to concurrent load of other applications and jobs.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThis post introduces the \u003ca href=\"https://github.com/gunnarmorling/jfrunit\"\u003eJfrUnit\u003c/a\u003e open-source project, which offers a fresh angle to this topic by supporting assertions not on metrics like latency/throughput themselves, but on \u003cem\u003eindirect metrics\u003c/em\u003e which may impact those.\nJfrUnit allows you define expected values for metrics such as memory allocation, database I/O, or number of executed SQL statements, for a given workload and asserts the actual metrics values\u0026#8201;\u0026#8212;\u0026#8201;which are obtained from \u003ca href=\"https://openjdk.java.net/jeps/328\"\u003eJDK Flight Recorder\u003c/a\u003e events\u0026#8201;\u0026#8212;\u0026#8201;against these expected values.\nStarting off from a defined base line, future failures of such assertions are an indicator for potential performance regressions in an application, as a code change may have introduced higher GC pressure,\nthe retrieval of unneccessary data from the database, or SQL problems commonly induced by ORM tools, like N+1 SELECT statements.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Towards Continuous Performance Regression Testing","uri":"https://www.morling.dev/blog/towards-continuous-performance-regression-testing/"},{"content":"A few months ago I wrote about how you could speed up your Java application\u0026#8217;s start-up times using application class data sharing (AppCDS), based on the example of a simple Quarkus application. Since then, quite some progress has been made in this area: Quarkus 1.6 brought built-in support for AppCDS, so that now you just need to provide the -Dquarkus.package.create-appcds=true option when building your project, and you\u0026#8217;ll find an AppCDS file in the target folder.\n Things get more challenging though when combining AppCDS with custom Java runtime images, as produced using the jlink tool added in Java 9. Combining custom runtime images with AppCDS is very attractive, in particular when looking at the deployment of Java applications via Linux containers. Instead of putting the full Java runtime into the container image, you only add those JDK modules which your application actually requires. (Parts of) what you save in image size by doing so, can be used for adding an AppCDS archive to your container image. The result will be a container image which still is smaller than before\u0026#8201;\u0026#8212;\u0026#8201;and thus is faster to push to a container registry, distribute to worker nodes in a Kubernetes cluster, etc.\u0026#8201;\u0026#8212;\u0026#8201;and which starts up significantly faster.\n A challenge though is that AppCDS archives must be created with exactly same Java runtime which later on is used to run the application. In the case of jlink this means the custom runtime image itself must be used to produce the AppCDS archive. In other words, the default archive produced by the Quarkus build unfortunately cannot be used with jlink images. The goal for this post is to explore\n   the steps required to create a custom runtime image for a simple Java CRUD application based on Quarkus,\n  how to build a Linux container image with this custom runtime image and the application itself,\n  how this approach compares to container images with the full Java runtime in terms of size and start-up time.\n   Creating a Modular Runtime Image for a Quarkus Application It\u0026#8217;s a common misbelief that only Java applications which have been fully ported to the Java module system (JPMS) would be able to benefit from jlink. But as explained by Simon Ritter in this blog post, this is not true actually; you don\u0026#8217;t need to fully modularize an application in order to run it via a custom runtime image.\n While indeed the creation of a runtime image is a bit easier when it only is comprised of proper Java modules, it also is possible to create a runtime image by explicitly stating which JDK (or other) modules it should contain. The application can then be run via the traditional classpath, just as you\u0026#8217;d do it with a full Java runtime. Which JDK modules to add though? To answer this question, the jdeps tool comes in handy. Via its --print-module-deps option it can determine for a given set of JARs which (JDK) modules they depend on, and which thus are the ones that need to go into the custom runtime image.\n Having built the example application from the previous blog post via mvn clean verify, let\u0026#8217;s try and invoke jdeps like so:\n jdeps --print-module-deps \\ --class-path target/lib/* \\ target/todo-manager-1.0.0-SNAPSHOT-runner.jar   This results in an error though:\n Error: com.sun.istack.istack-commons-runtime-3.0.10.jar is a multi-release jar file but --multi-release option is not set   Ok, we need to tell which code version to analyse for multi-release JARs; no problem:\n jdeps --print-module-deps \\ --multi-release 15 \\ --class-path target/lib/* \\ target/todo-manager-1.0.0-SNAPSHOT-runner.jar   Hum, some progress, but still an issue:\n Exception in thread \"main\" java.lang.module.FindException: Module java.xml.bind not found, required by java.ws.rs   This one is a bit odd; the file org.jboss.spec.javax.ws.rs.jboss-jaxrs-api_2.1_spec-2.0.1.Final.jar is an explicit module with a module-info.class descriptor, which references the module java.xml.bind, and this one is not found on the module path. It\u0026#8217;s not quite clear to me why this is flagged here, given that the JAX-RS API JAR is part of the class path and not the module path. But it\u0026#8217;s not a big problem, we simply can add the JAXB API (which also is provided on the class path) on the module path, too.\n The same issue arises for some other dependencies which are explicit modules already, so we end up with the following configuration:\n jdeps --print-module-deps \\ --multi-release 15 \\ --module-path target/lib/jakarta.activation.jakarta.activation-api-1.2.1.jar:target/lib/org.reactivestreams.reactive-streams-1.0.3.jar:target/lib/org.jboss.spec.javax.xml.bind.jboss-jaxb-api_2.3_spec-2.0.0.Final.jar \\ --class-path target/lib/* \\ target/todo-manager-1.0.0-SNAPSHOT-runner.jar   And another issue, now about some missing dependencies:\n ... org.postgresql.util.internal.Nullness -\u0026gt; org.checkerframework.dataflow.qual.Pure not found org.wildfly.common.wildfly-common-1.5.4.Final-format-001.jar org.wildfly.common.Substitutions$Target_Branch -\u0026gt; com.oracle.svm.core.annotate.AlwaysInline not found ...   After taking a closer look, these are either compile-time only dependencies (like annotations from the Checker framework), or dependencies of optional features which are not relevant for our case. These can be safely ignored using the --ignore-missing-deps switch, which leaves us with this jdeps invocation:\n jdeps --print-module-deps \\ --ignore-missing-deps \\ --multi-release 15 \\ --module-path target/lib/jakarta.activation.jakarta.activation-api-1.2.1.jar:target/lib/org.reactivestreams.reactive-streams-1.0.3.jar:target/lib/org.jboss.spec.javax.xml.bind.jboss-jaxb-api_2.3_spec-2.0.0.Final.jar \\ --class-path target/lib/* \\ target/todo-manager-1.0.0-SNAPSHOT-runner.jar   The required JDK modules are printed out finally:\n java.base,java.compiler,java.instrument,java.naming,java.rmi,java.security.jgss,java.security.sasl,java.sql,jdk.jconsole,jdk.unsupported   I.e. out of the nearly 60 modules which make up OpenJDK 15, only ten are required by this particular application. Building a custom runtime image containing only these modules should result in quite some space saving.\n     Why is a Particular Module Required? When looking at the module list, you might wonder why certain modules actually are needed. What is this application doing with jdk.jconsole for instance? To gain insight into this, jdeps can help, too. Run it again without the --print-module-deps switch, and you can grep for interesting module references:\n jdeps \u0026lt;...\u0026gt; | grep jconsole org.jboss.narayana.jta.narayana-jta-5.10.6.Final.jar -\u0026gt; jdk.jconsole com.arjuna.ats.arjuna.tools.stats -\u0026gt; com.sun.tools.jconsole jdk.jconsole   In this case, there\u0026#8217;s a single dependency to jconsole, from the Narayana transaction manager. Depending on the details, it might be an opportunity to reach out to the maintainers of such library and discuss, whether this dependency really is needed or whether it could be avoided (e.g. by moving the code in question to a separate module), resulting in a further decreased size of custom runtime images.\n     With the list of required modules, creating the actual runtime image is rather simple:\n $JAVA_HOME/bin/jlink \\ --add-modules java.base,java.compiler,java.instrument,java.naming,java.rmi,java.security.jgss,java.security.sasl,java.sql,jdk.jconsole,jdk.unsupported \\ --compress 2 --no-header-files --no-man-pages \\(1) --output target/runtime-image (2)     1 Compressing the runtime image as well as omitting header files and man pages helps to further reduce the size of the runtime image   2 Output location for creating the runtime image    In order to create a dynamic AppCDS archive for our application classes later on, we now need to add the class data archive for all of the classes of the image itself. Failing to do so results in this error message:\n Error occurred during initialization of VM DynamicDumpSharedSpaces is unsupported when base CDS archive is not loaded   This step isn\u0026#8217;t very well documented, and at this point I was somewhat stuck. But you always can count on the OpenJDK community: after asking about this on Twitter, Claes Redestad pointed me into the right direction:\n ./target/runtime-image/bin/java -Xshare:dump   Thanks, Claes! This creates the base class data archive under target/runtime-image/lib/server/classes.jsa, adding ~12 MB to the runtime image, which now has a size of ~63 MB; not too bad.\n   Adding an AppCDS Archive to a Custom Runtime Image Having created the custom Java runtime image, let\u0026#8217;s now add the AppCDS archive to it. Since the introduction of dynamic AppCDS archives in JDK 13, this is one simple step which only requires to run the application with the -XX:ArchiveClassesAtExit option:\n cd target (1) mkdir runtime-image/cds (2) (3) runtime-image/bin/java \\ -XX:ArchiveClassesAtExit=runtime-image/cds/app-cds.jsa \\ -jar todo-manager-1.0.0-SNAPSHOT-runner.jar cd ..     1 The class path used when running the application later on must be the same as (or rather a prefix of, to be precise) the class path used for building the AppCDS archive; hence changing to the target directory, so to run with -jar *-runner.jar, instead of with -jar target/*-runner.jar   2 Creating a folder for storing the AppCDS archive   3 Using the java binary of the runtime image to launch the application and create the AppCDS archive when exiting    This will create the CDS archive under target/runtime-image/cds/app-cds.jsa. In the next step this can be added to a Linux container image, built e.g. using Docker or podman. Note that while jlink supports cross-platform builds (so for instance you could build a custom runtime image for a Linux container on macOS), the same isn\u0026#8217;t the case for AppCDS. This means an AppCDS archive to be used by a containerized application needs to be built on Linux. When not running on Linux yourself, but on Windows or macOS, you could put the entire build process into a container for this purpose.\n   Creating a Linux Container Image At this point we have built our actual application, a custom Java runtime image with the required JDK modules, and an AppCDS archive for the application\u0026#8217;s classes. The final step is to put everything into a Linux container image, which is quickly done via a small Dockerfile:\n FROM registry.fedoraproject.org/fedora-minimal:33 COPY target/runtime-image /opt/todo-manager/jdk COPY target/lib/* /opt/todo-manager/lib/ COPY target/todo-manager-1.0.0-SNAPSHOT-runner.jar /opt/todo-manager COPY todo-manager.sh /opt/todo-manager ENTRYPOINT [ \"/opt/todo-manager/todo-manager.sh\" ]   This uses the Fedora minimal base image, which is a great foundation for container images. With a size of ~120 MB, it\u0026#8217;s small enough to be distributed efficiently, while still providing the flexibility of a complete Linux distribution, e.g. allowing for the installation of additional tools if needed.\n     Even Smaller Container Images If you wanted to shrink the image size further and felt adventureous, you could look into using Alpine Linux as a base image; the issue there though is that Alpine comes with musl instead of glibc (as used by the JDK) as its implementation of the ISO C and POSIX standard APIs. The OpenJDK Portola project aims at providing a port to Alpine and musl. But as of JDK 15, no GA build of this port exists yet. For JDK 16, an early access build of the Alpine/musl port is available.\n Another option for smaller images is to use jib, which also is supported by Quarkus out of the box. I haven\u0026#8217;t tried out yet though whether/how jib would work with custom runtime images and AppCDS.\n It\u0026#8217;s also worth pointing out that the size of base images doesn\u0026#8217;t matter too much in practice, as container images use a layered file system, which means that typically rather stable base image layers don\u0026#8217;t need to be redistributed too often when pushing or pulling a container image.\n     The container\u0026#8217;s entry point, todo-manager.sh, is a basic shell script, which starts the actual Java application via the Java runtime image:\n #!/bin/bash export PATH=\"/opt/todo-manager/jdk/bin:${PATH}\" cd /opt/todo-manager \u0026amp;\u0026amp; \\ (1) exec java -Xshare:on -XX:SharedArchiveFile=jdk/cds/app-cds.jsa -jar \\ (2) todo-manager-1.0.0-SNAPSHOT-runner.jar     1 Changing into the todo-manager directory, so to make sure the same JAR path is passed as when creating the CDS archive   2 Specifying the archive name; the -Xshare:on isn\u0026#8217;t strictly needed, it\u0026#8217;s used here though to ensure the process will fail if something is wrong with the CDS archive, instead of silently not using it      Let\u0026#8217;s See Some Numbers! Finally, let\u0026#8217;s compare some numbers: container image size, and start-up time for different ways of containerizing the todo manager application. I\u0026#8217;ve tried out four different aproaches:\n   OpenJDK 11 on the RHEL UBI 8.3 image (universal base image), as per the default Dockerfile created for new Quarkus applications\n  A full OpenJDK 15 on Fedora 33 (as there\u0026#8217;s no OpenJDK 15 package for the RHEL base image yet)\n  A custom runtime image for OpenJDK 15 on Fedora 33\n  A custome runtime image with AppCDS on Fedora 33\n   Here are the results, running on a Hetzner Cloud CX4 instance (4 vCPUs, 16 GB RAM), using Fedora 33 as the host OS:\n   As we can see, the container image size is significantly lower when adding a custom Java runtime image instead of the full JDK. In particular when comparing to the OpenJDK package of Fedora 33 which is a fair bit larger than the OpenJDK 11 package of the RHEL UBI 8.3 image, the difference is striking.\n The start-up times are as displayed by Quarkus, averaged over five runs. Numbers have improved by about 10% by going from OpenJDK 11 to 15, which is explained by multiple improvements in this area, most notably the introduction of default CDS archives for the JDK\u0026#8217;s own classes in JDK 12 (JEP 341). Using a custom runtime image by itself doesn\u0026#8217;t have any measurable impact on start-up time. The AppCDS archive improves the start-up time by a whopping 54%. Unless pure image size is the key factor for you (in which case you should look for alternative approaches anyways, see note \"Even Smaller Container Images\" above), I would say that the additional 40 MB for the AppCDS archive are more than worth it. In particular as the resulting container image still is way smaller than when adding the full JDK, be it with the Fedora base image or the RHEL UBI one.\n Based on those numbers, I think it\u0026#8217;s fair to say that custom Java runtime images created via jlink, combined with AppCDS archives are a great foundation for containerized Java applications. Adding a custom runtime image containing only those JDK modules actually needed by an application help to cut down image size signficantly. Parts of that saved space can be invested into adding an AppCDS archive, so you end up with a container image that\u0026#8217;s smaller and starts up faster. I.e. you can have this cake, and eat it, too!\n The one downside is the increased complexity of the build process for producing the runtime image as well as the AppCDS archive. This should be manageable though by means of scripting and automation; also I\u0026#8217;d expect tooling like the Quarkus Maven plug-in and others to further improve on this front. One tricky aspect is that you must not forget to rebuild the custom runtime image, in case you have added dependencies to your application which affect the set of required JDK modules. Automated tests of the application running via the runtime image should help to identify this situation.\n If you\u0026#8217;d like to give it a try yourself, or obtain numbers for the different deployment approaches on your own hardware, you can find all the required code and information in this GitHub repository.\n  ","id":10,"publicationdate":"Dec 13, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eA few months ago I \u003ca href=\"/blog/building-class-data-sharing-archives-with-apache-maven/\"\u003ewrote about\u003c/a\u003e how you could speed up your Java application\u0026#8217;s start-up times using application class data sharing (\u003ca href=\"http://openjdk.java.net/jeps/350\"\u003eAppCDS\u003c/a\u003e),\nbased on the example of a simple \u003ca href=\"https://quarkus.io/\"\u003eQuarkus\u003c/a\u003e application.\nSince then, quite some progress has been made in this area:\nQuarkus 1.6 brought \u003ca href=\"https://quarkus.io/guides/maven-tooling#quarkus-package-pkg-package-config_quarkus.package.create-appcds\"\u003ebuilt-in support for AppCDS\u003c/a\u003e,\nso that now you just need to provide the \u003cem\u003e-Dquarkus.package.create-appcds=true\u003c/em\u003e option when building your project,\nand you\u0026#8217;ll find an AppCDS file in the \u003cem\u003etarget\u003c/em\u003e folder.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThings get more challenging though when combining AppCDS with custom Java runtime images,\nas produced using the \u003ca href=\"https://docs.oracle.com/en/java/javase/15/docs/specs/man/jlink.html\"\u003ejlink\u003c/a\u003e tool added in Java 9.\nCombining custom runtime images with AppCDS is very attractive,\nin particular when looking at the deployment of Java applications via Linux containers.\nInstead of putting the full Java runtime into the container image, you only add those JDK modules which your application actually requires.\n(Parts of) what you save in image size by doing so,\ncan be used for adding an AppCDS archive to your container image.\nThe result will be a container image which still is smaller than before\u0026#8201;\u0026#8212;\u0026#8201;and thus is faster to push to a container registry, distribute to worker nodes in a Kubernetes cluster, etc.\u0026#8201;\u0026#8212;\u0026#8201;and which starts up significantly faster.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Smaller, Faster-starting Container Images With jlink and AppCDS","uri":"https://www.morling.dev/blog/smaller-faster-starting-container-images-with-jlink-and-appcds/"},{"content":"The Testcontainers project is invaluable for spinning up containerized resources during your (JUnit) tests, e.g. databases or Kafka clusters.\n For users of JUnit 5, the project provides the @Testcontainers extension, which controls the lifecycle of containers used by a test. When testing a Quarkus application though, this is at odds with Quarkus' own @QuarkusTest extension; it\u0026#8217;s a recommended best practice to avoid fixed ports for any containers started by Testcontainers. Instead, you should rely on Docker to automatically allocate random free ports. This avoids conflicts between concurrently running tests, e.g. amongst multiple Postgres containers, started up by several parallel job runs in a CI environment, all trying to allocate Postgres' default port 5432. Obtaining the randomly assigned port and passing it into the Quarkus bootstrap process isn\u0026#8217;t possible though when combining the two JUnit extensions.\n One work-around you can find described e.g. on StackOverflow is setting up the database container via a static class initializer block and then propagating the host and port to Quarkus through system properties. While this works, it\u0026#8217;s not ideal in terms of lifecycle control (e.g. how to make sure the container is started up once at the beginning of an entire test suite), and in general, it just feels a bit hack-ish.\n Luckily, there\u0026#8217;s a better alternative, which interestingly isn\u0026#8217;t discussed as much: using Quarkus' notion of test resources. There\u0026#8217;s just two steps involved. First, create an implementation of the QuarkusTestResourceLifecycleManager interface, which controls your resource\u0026#8217;s lifecycle. In case of a Postgres database, this could look like this:\n public class PostgresResource implements QuarkusTestResourceLifecycleManager { static PostgreSQLContainer\u0026lt;?\u0026gt; db = new PostgreSQLContainer\u0026lt;\u0026gt;(\"postgres:13\") (1) .withDatabaseName(\"tododb\") .withUsername(\"todouser\") .withPassword(\"todopw\"); @Override public Map\u0026lt;String, String\u0026gt; start() { (2) db.start(); return Collections.singletonMap( \"quarkus.datasource.url\", db.getJdbcUrl() ); } @Override public void stop() { (3) db.stop(); } }     1 Configure the database container, using the Postgres 13 container image, the given database name, and credentials   2 Start up the database; the returned map of configuration properties amends/overrides the configuration properties of the test; in this case the datasource URL will be overridden with the value obtained from Testcontainers, which contains the randomly allocated public port of the Postgres container   3 Shut down the database after all tests have been executed    All you then need to do is to reference that test resource from your test class using the @QuarkusTestResource annotation:\n @QuarkusTest @QuarkusTestResource(PostgresResource.class) (1) public class TodoResourceTest { @Test public void createTodoShouldYieldId() { given() .when() .contentType(ContentType.JSON) .body(\"\"\" { \"title\" : \"Learn Quarkus\", \"priority\" : 1, } \"\"\") .then() .statusCode(201) .body( matchesJson( \"\"\" { \"id\" : 1, \"title\" : \"Learn Quarkus\", \"priority\" : 1, \"completed\" : false, } \"\"\")); } }     1 Ensures the Postgres database is started up    And that\u0026#8217;s it! Note that all the test resources of the test module are detected and started up, before starting the first test.\n Bonus: Schema Creation One other subtle issue is the creation of the database schema for the test. E.g. for my Todo example application, I\u0026#8217;d like to use a schema named \"todo\" in the Postgres database:\n create schema todo;   Quarkus supports SQL load scripts for executing SQL scripts when Hibernate ORM starts. But this will be executed only after Hibernate ORM has set up all the database objects, such as tables, sequences, indexes etc. (I\u0026#8217;m using the drop-and-create database generation mode during testing). This means that while a load script is great for inserting test data, it\u0026#8217;s executed too late for defining the actual database schema itself.\n Luckily, most database container images themselves support the execution of load scripts right upon database start-up; The Postgres image is no exception, so it\u0026#8217;s just a matter of exposing that script via Testcontainers. All it needs for that is a bit of tweaking of the Quarkus test resource for Postgres:\n static PostgreSQLContainer\u0026lt;?\u0026gt; db = new PostgreSQLContainer\u0026lt;\u0026gt;(\"postgres:13\") .withDatabaseName(\"tododb\") .withUsername(\"todouser\") .withPassword(\"todopw\") .withClasspathResourceMapping(\"init.sql\", (1) \"/docker-entrypoint-initdb.d/init.sql\", BindMode.READ_ONLY);     1 Expose the file src/main/resources/init.sql as /docker-entrypoint-initdb.d/init.sql within the container    With that in place, Postgres will start up and the \"todo\" schema will be created in the database, before Quarkus boots Hibernate ORM, which will populate the schema, and finally, all tests can run.\n You can find the complete source code of this test and the Postgres test resource on GitHub.\n Many thanks to Sergei Egorov for his feedback while writing this blog post!\n  ","id":11,"publicationdate":"Nov 28, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.testcontainers.org/\"\u003eTestcontainers\u003c/a\u003e project is invaluable for spinning up containerized resources during your (JUnit) tests,\ne.g. databases or Kafka clusters.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eFor users of JUnit 5, the project provides the \u003ca href=\"https://www.testcontainers.org/quickstart/junit_5_quickstart/\"\u003e\u003ccode\u003e@Testcontainers\u003c/code\u003e\u003c/a\u003e extension, which controls the lifecycle of containers used by a test.\nWhen testing a \u003ca href=\"https://quarkus.io/\"\u003eQuarkus\u003c/a\u003e application though, this is at odds with Quarkus' own \u003ca href=\"https://quarkus.io/guides/getting-started-testing#recap-of-http-based-testing-in-jvm-mode\"\u003e\u003ccode\u003e@QuarkusTest\u003c/code\u003e\u003c/a\u003e extension;\nit\u0026#8217;s a recommended \u003ca href=\"https://bsideup.github.io/posts/testcontainers_fixed_ports/\"\u003ebest practice\u003c/a\u003e to avoid fixed ports for any containers started by Testcontainers.\nInstead, you should rely on Docker to automatically allocate random free ports.\nThis avoids conflicts between concurrently running tests,\ne.g. amongst multiple Postgres containers,\nstarted up by several parallel job runs in a CI environment, all trying to allocate Postgres' default port 5432.\nObtaining the randomly assigned port and passing it into the Quarkus bootstrap process isn\u0026#8217;t possible though when combining the two JUnit extensions.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Quarkus and Testcontainers","uri":"https://www.morling.dev/blog/quarkus-and-testcontainers/"},{"content":"Layers are sort of the secret sauce of the Java platform module system (JPMS): by providing fine-grained control over how individual JPMS modules and their classes are loaded by the JVM, they enable advanced usages like loading multiple versions of a given module, or dynamically adding and removing modules at application runtime.\n The Layrry API and launcher provides a small plug-in API based on top of layers, which for instance can be used to dynamically add plug-ins contributing new views and widgets to a running JavaFX application. If such plug-in gets removed from the application again, all its classes need to be unloaded by the JVM, avoiding an ever-increasing memory consumption if for instance a plug-in gets updated multiple times.\n In this blog post I\u0026#8217;m going to explore how to ensure classes from removed plug-in layers are unloaded in a timely manner, and how to find the culprit in case some class fails to be unloaded.\n Do We Really Need Plug-ins? Before diving into the details of class unloading, let\u0026#8217;s spend some time to think about the use cases for dynamic plug-ins in Java applications to begin with. I would argue that for typical backend applications this need mostly has diminished. At large, the industry is moving away from application servers and their model around \"deploying\" applications (which you could consider as some kind of \"plug-in\") into a running server process. Instead, there\u0026#8217;s a strong trend towards immutable application packages, based on stacks like Quarkus or Spring Boot, embedding the web server, the application as well as its dependencies, often-times deployed as container images.\n The advantages of this approach centered around immutable images manifold, e.g. in terms of security (no interface for deploying applications is needed) and governance (it\u0026#8217;s always exactly clear which version of the application is running). Updates\u0026#8201;\u0026#8212;\u0026#8201;i.e. the deployment of a new revision of the container image\u0026#8201;\u0026#8212;\u0026#8201;can be put in place e.g. with help of a proxy in front of a cluster of application nodes, which are updated in a rolling manner. That way, there\u0026#8217;s no downtime of the service that\u0026#8217;ll impact the user. Also techniques like canary releases and A/B testing, as well as rolling back to specific earlier versions of an application become a breeze that way.\n The situation is different though when it comes to client applications. When thinking of your favourite editor, IDE or web browser for instance, requiring a restart when installing or updating a plug-in is not desirable. Instead, it should be possible to add plug-ins (or new plug-in versions) to a running application instance and be usable immediately, without interrupting the flow of the user. The same applies for many IoT scenarios, where e.g. an application consuming sensor measurements should be updateable without any downtime.\n   Plug-ins in Layered Java Applications JPMS addresses this requirement via the notion of module layers:\n  A layer is created from a graph of modules in a Configuration and a function that maps each module to a ClassLoader. Creating a layer informs the Java virtual machine about the classes that may be loaded from the modules so that the Java virtual machine knows which module that each class is a member of.\n   Layers are the perfect means of adding new code into a running Java application: they can be added and removed dynamically, and code in an already running layer can invoke functionality from a dynamically added layer in different ways, e.g. via reflection or by using the service loader API. Layrry exposes this functionality via a very basic plug-in API:\n public interface PluginLifecycleListener { void pluginAdded(PluginDescriptor plugin); void pluginRemoved(PluginDescriptor plugin); }   public class PluginDescriptor { public String getName() { ... } public ModuleLayer getModuleLayer() { ... } }   A plug-in in this context is a JPMS layer containing one or more modules (either explicit or automatic) which all are loaded via a single class loader. A Layrry-based application can implement the PluginLifecycleListener service contract in order to be notified whenever a plug-in is added or removed. Plug-ins are loaded from configured directories in the file system which are monitored by Layrry (other means of (un-)installing plug-ins may be added in future versions of Layrry).\n Installing a plug-in is as easy as copying its JAR(s) into a sub-folder of such monitored directory. Layrry will copy the plug-in contents to a temporary directory, create a layer with all the plug-ins JARs, and notify any registered plug-in listeners about the new layer. These will typically use the service loader API then to interact with application-specific services which model its extension points, e.g. to contribute visual UI components in case of a desktop application.\n The reverse process happens when a plug-in gets un-installed: the user removes a plug-in\u0026#8217;s directory, and all listeners will be notified by the Layrry about the removal. They should release all references to any classes from the removed plug-in, rendering it avaible for garbage collection.\n   Class Unloading in Practice There is no API in the Java platform for explicitly unloading a given class. Instead, \"a class or interface may be unloaded if and only if its defining class loader may be reclaimed by the garbage collector\" (JLS, chapter 12.7). This means in a layered Java application any classes in a layer that got removed can be unloaded as soon as the layer\u0026#8217;s class loader is subject to GC. Most importantly, no class in a still running layer must keep a (strong) reference to any class of the removed layer; otherwise this class would hinder collecting the removed layer\u0026#8217;s loader and its classes.\n As an example, let\u0026#8217;s look at the modular-tiles demo, a JavaFX application which uses the Layrry plug-in API for dynamically adding and removing tiles with different widgets like clocks and gauges to its graphical UI. The tiles themselves are implemented using the fabulous TilesFX project by Gerrit Grundwald.\n If you want to follow along, check out the source code of the demo and build it as per the instructions in the README file. Then run the Layrry launcher with the -Xlog:class+unload=info option, so to be notified about any unloaded classes in the system output:\n java -Xlog:class+unload=info \\ -jar path/to/layrry-launcher-1.0-SNAPSHOT-all.jar \\ --layers-config staging/layers.toml \\ --properties staging/versions.properties   Now add and remove some tiles plug-ins a few times:\n cp -r staging/plugins-prepared/* staging/plugins rm -rf staging/plugins/*   The widgets will show up and disappear in the JavaFX UI, but what about class unloading in the logs? In all likelyhood, nothing! This is because without any further configuration, the G1 garbage collector (which is used by the JDK by default since Java 9) will unload classes only during a full garbage collection, which may only run after a long time (if at all), if there\u0026#8217;s no substantial object allocation happening.\n     JEP 158: Unified JVM Logging The -Xlog option has been defined by JEP 158, added to the JDK with Java 9, which provides a \"common logging system for all components of the JVM\". The new unified options should be preferred over the legacy options like -XX:+TraceClassLoading and -XX:+TraceClassUnloading. Usage of -Xlog is described in detail in the java man page; also Nicolai Parlog discusses JEP 158 in great depth in this blog post.\n     So at this point you could trigger a GC explicitly, e.g. via jcmd:\n jcmd \u0026lt;pid\u0026gt; GC.run   But of course that\u0026#8217;s not too desirable when running things in production. Instead, if you\u0026#8217;re on JDK 12 or later, you can use the new G1PeriodicGCInterval option for triggering a periodic GC:\n java -Xlog:class+unload=info \\ -XX:G1PeriodicGCInterval=5000 \\ -jar path/to/layrry-launcher-1.0-SNAPSHOT-all.jar \\ --layers-config staging/layers.toml \\ --properties staging/versions.properties   Introduced via JEP 346 (\"Promptly Return Unused Committed Memory from G1\"), this will periodically initiate a concurrent GC cycle (or optionally even a full GC). Add and remove some plug-ins again, and after some time you should see messages about the unloaded classes in the log:\n ... [138.912s][info][class,unload] unloading class org.kordamp.tiles.sparkline.SparklineTilePlugin 0x0000000800de1840 [138.912s][info][class,unload] unloading class org.kordamp.tiles.gauge.GaugeTilePlugin 0x0000000800de2040 [138.913s][info][class,unload] unloading class org.kordamp.tiles.clock.ClockTilePlugin 0x0000000800de2840 ...   From what I observed, class unloading doesn\u0026#8217;t happen on every concurrent GC cycle; it might take a few cycles after a plug-in has been removed until its classes are unloaded. If you\u0026#8217;re not using G1, but the new low-pause concurrent collectors Shenandoah or ZGC, they\u0026#8217;ll be able to concurrently unload classes without any special configuration needed. Note that class unloading is not a mandatory operation which would have to be provided by every GC implementation. E.g. initial ZGC releases did not support class unloading, which would have rendered them unsuitable for this use case.\n     JEP 371: Hidden Classes As mentioned above, regular classes can only be unloaded if their defining class loader become subject to garbage collection. This can be an issue for frameworks and libraries which generate lots of classes dynamically at runtime, e.g. script language implementations or solutions like Presto, which generates a class for each query.\n The traditional workaround is to generate each class using its own dedicated class loader, which then can be discarded specifically. This solves the GC issue, but it isn\u0026#8217;t ideal in terms of overall memory consumption and speed of class generation. Hence, JDK 15 defines a notion of Hidden Classes (JEP 371), which are not created by class loaders and thus can be unloaded eagerly: \"when all instances of the hidden class are reclaimed and the hidden class is no longer reachable, it may be unloaded even though its notional defining loader is still reachable\".\n You can find some more information on hidden classes in this tweet thread and this code example on GitHub.\n     But who wants to stare at logs in the system output, that\u0026#8217;s so 2010! So let\u0026#8217;s fire up JDK Mission Control and trigger a recording via the JDK Flight Recorder (JFR) to observe what\u0026#8217;s going on in more depth.\n JFR can capture class unloading events, you need to make sure though to enable this event type, which is not the case by default. In order to do so, start a recording, then go to the Template Manager, edit or create a flight recording template and check the Enabled box for the events under Java Virtual Machine \u0026#8594; Class Loading. With the recorder running, add and remove some tiles plug-ins to the running application.\n Once the recording is finished, you should see class unloading events under JVM Internals \u0026#8594; Class Loading:\n   In this case, the classes from a set of plug-ins were unloaded at 16:48:11, which correlates to the periodic GC cycle running at that time and spending a slightly increased time for cleaning up class loader data:\n   As a good Java citizen, Layrry itself also emits JFR events whenever a plug-in layer is added or removed, which helps to track the need for classes to be unloaded:\n     If Things Go Wrong Now let\u0026#8217;s look at the situation where some class failed to unload after its plug-in layer was removed. Common reasons for that include remaining references from classes in a still running layer to classes in the removed layer, threads started by a class in the removed layer which were not stopped, and JVM shutdown hooks registered by code in the removed layer.\n This is known as a class loader leak and is problematic as it means more and more memory will be consumed and cannot be freed as plug-ins are added and removed, which eventually may lead to an OutOfMemoryError. So how could you detect and analyse this situation? An OutOfMemoryError in production would surely be an indicator that there must be a memory or class loader leak somewhere. It\u0026#8217;s also a good idea to regularly examine JFR recording files (e.g. in your testing or staging environment): the absence of any class unloading event despite the removal of plug-ins should trigger an investigation.\n As far as analysing the situation is concerned, examining a heap dump of the application will typically yield insight into the cause rather quickly. Take a heap dump using jcmd as shown above, then load the dump into a tool such as Eclipse MAT. In Eclipse MAT, the \"Duplicate Classes\" action is a great starting point. If one class has been loaded by multiple class loaders, but failed to unload, it\u0026#8217;s a pretty strong indicator that something is wrong:\n   The next step is to analyse the shortest path from the involved class loaders to a GC root:\n   Some object on that path must hold on to a reference to a class or the class loader of the removed plug-in, preventing the loader to be GC-ed. In the case at hand, it\u0026#8217;s the leakingPlugins field in the PluginRegistry class, to which each plug-in is added upon addition of the layer, but then apparently its coffee-deprived author forgot to remove the plug-in from that collection within the pluginRemoved() event handler ;)\n As a quick side note, there\u0026#8217;s a really cool plug-in for Eclipse MAT written by Vladimir Sitnikov, which allows you to query heap dumps using SQL. It maps each class to its own \"table\", so that e.g. classes loaded more than once could be selected using the following SQL query on the java.lang.Class class:\n select c.name, listagg(toString(c.\"@classLoader\")) as 'loaders', count(*) as 'count' from \"java.lang.Class\" c where c.name \u0026lt;\u0026gt; '' group by c.name having count(*) \u0026gt; 1   Resulting in the same list of classes as above:\n   This could come in very handy for more advanced heap dump analyses, which cannot be done using Eclipse MAT\u0026#8217;s built-in query capabilities.\n   Learning More Via module layers, JPMS provides the foundation for dynamic plug-in architectures, as demonstrated by Layrry. Removing layers at runtime requires some care and consideration, so to avoid class loader leaks which eventually may lead to OutOfMemoryErrors. As so often, JDK Mission Control, JFR, and Eclipse MAT prove to be invaluable tools in the box of every Java developer, helping to ensure class unloading in your layered applications is done correctly, and if it is not, helping to understand and fix the underlying issue.\n Here are some more resources about class unloading and analysing class loader leaks:\n   Shenandoah GC in JDK 14, Part 2: Concurrent roots and class unloading: A blog post touching on class unloading in Shenandoah by Roman Kennke\n  ZGC Concurrent Class Unloading: A conference talk by Erik Österlund\n  class loader leaks: A series of blog posts by Mattias Jiderhamn\n  ClassLoader \u0026amp; memory leaks: a Java love story: A post about heap dump analysis by Aloïs Micard\n   Lastly, if you\u0026#8217;d like to explore the dynamic addition and removal of JPMS layers to a running application yourself, the modular-tiles demo app is a great starting point. Its source code can be found on GitHub.\n  ","id":12,"publicationdate":"Oct 14, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eLayers are sort of the secret sauce of the Java platform module system (JPMS):\nby providing fine-grained control over how individual JPMS modules and their classes are loaded by the JVM,\nthey enable advanced usages like loading multiple versions of a given module, or dynamically adding and removing modules at application runtime.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe \u003ca href=\"/blog/introducing-layrry-runner-and-api-for-modularized-java-applications/\"\u003eLayrry\u003c/a\u003e API and launcher provides a small plug-in API based on top of layers,\nwhich for instance can be used to dynamically add plug-ins contributing new views and widgets to a running JavaFX application.\nIf such plug-in gets removed from the application again,\nall its classes need to be unloaded by the JVM, avoiding an ever-increasing memory consumption if for instance a plug-in gets updated multiple times.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIn this blog post I\u0026#8217;m going to explore how to ensure classes from removed plug-in layers are unloaded in a timely manner,\nand how to find the culprit in case some class fails to be unloaded.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Class Unloading in Layered Java Applications","uri":"https://www.morling.dev/blog/class-unloading-in-layered-java-applications/"},{"content":"Lately I\u0026#8217;ve been fascinated by the possibility to analyse the assembly code emitted by the Java JIT (just-in-time) compiler. So far I had only looked only into Java class files using javap; diving into the world of assembly code feels a bit like Alice must have felt when falling down the rabbit whole into wonderland.\n My motivation for this exploration was trying to understand what is faster in Java: a switch statement over strings, or a lookup in a hash map. Solely looking at Java bytecode isn\u0026#8217;t going far enough to answer this question, as the difference lies in the actual assembly statements executed on the CPU. I\u0026#8217;ll keep the details around that for another time; in this post I\u0026#8217;m just going quickly to share what I learned in regards to building a tool needed for this exercise, hsdis.\n hsdis is a disassembler library which can be used with the java runtime as well as tools such as JitWatch to analyse the code produced by the Java JIT compiler. For licensing reasons though it doesn\u0026#8217;t come as a binary with the JDK. Instead, you need it to build yourself from source. Instructions for doing so are spread across a few different places, but I couldn\u0026#8217;t find any 100% current information, in particular as OpenJDK has moved to git and GitHub just recently.\n So here is what you need to do in order to build hsdis for OpenJDK 15; in my case I\u0026#8217;m running on macOS, slightly different steps may apply for other platforms. First, get the OpenJDK source code and check out the version for which you want to build hsdis:\n git clone git@github.com:openjdk/jdk.git git checkout jdk-15+36 # Current stable JDK 15 build   The source location of hsdis has changed with the move from Mercurial to git:\n cd src/utils/hsdis   In order to build hsdis, you\u0026#8217;ll need the GNU Binutils, a collection of several binary tools:\n wget https://ftp.gnu.org/gnu/binutils/binutils-2.35.tar.gz tar xvf binutils-2.35.tar.gz   Then run the actual hsdis build (macOS comes with all the required tools like make):\n make BINUTILS=binutils-2.35 ARCH=amd64   This will take a few minutes; if all goes well, there\u0026#8217;ll be hsdis binary in the build directory, in my case this is build/macosx-amd64/hsdis-amd64.dylib. Copy the library to lib/server of our JDK:\n sudo cp build/macosx-amd64/hsdis-amd64.dylib $JAVA_HOME/lib/server       If you\u0026#8217;re on Linux, you also can provide the hsdis tool via the LD_LIBRARY_PATH environment variable:\n export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:path/to/hsdis/build/linux-amd64   Note this won\u0026#8217;t work on current macOS versions unfortunately due to its System Integrity Protection feature (SIP). Thanks to Brice Dutheil for this tip!\n     Congrats! You now can use the XX:+PrintAssembly flag of the java command to examine the assembly code of your Java program. Let\u0026#8217;s give it a try. Create a Java source file with the following contents:\n public class PrintAssemblyTest { public static void main(String... args) { PrintAssemblyTest hello = new PrintAssemblyTest(); for(int i = 0; i \u0026lt;= 10_000_000; i++) { hello.hello(i); } } private void hello(int i) { if (i % 1_000_000 == 0) { System.out.println(\"Hello, \" + i); } } }   Compile and run it like so:\n javac PrintAssemblyTest.java java -XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly \\ -XX:+TraceClassLoading -XX:+LogCompilation \\ PrintAssemblyTest   You should then find the assembly code of the hello() method somewhere in the output:\n ============================= C2-compiled nmethod ============================== ----------------------------------- Assembly ----------------------------------- Compiled method (c2) 1409 106 4 PrintAssemblyTest::hello (20 bytes) total in heap [0x000000011e3fce90,0x000000011e3fd148] = 696 relocation [0x000000011e3fcfe8,0x000000011e3fcff8] = 16 main code [0x000000011e3fd000,0x000000011e3fd080] = 128 stub code [0x000000011e3fd080,0x000000011e3fd098] = 24 oops [0x000000011e3fd098,0x000000011e3fd0a0] = 8 metadata [0x000000011e3fd0a0,0x000000011e3fd0a8] = 8 scopes data [0x000000011e3fd0a8,0x000000011e3fd0d0] = 40 scopes pcs [0x000000011e3fd0d0,0x000000011e3fd140] = 112 dependencies [0x000000011e3fd140,0x000000011e3fd148] = 8 -------------------------------------------------------------------------------- [Constant Pool (empty)] -------------------------------------------------------------------------------- [Entry Point] # {method} {0x000000010d74c4b0} 'hello' '(I)V' in 'PrintAssemblyTest' # this: rsi:rsi = 'PrintAssemblyTest' # parm0: rdx = int # [sp+0x30] (sp of caller) 0x000000011e3fd000: mov 0x8(%rsi),%r10d 0x000000011e3fd004: shl $0x3,%r10 0x000000011e3fd008: movabs $0x800000000,%r11 0x000000011e3fd012: add %r11,%r10 0x000000011e3fd015: cmp %r10,%rax 0x000000011e3fd018: jne 0x0000000116977100 ; {runtime_call ic_miss_stub} 0x000000011e3fd01e: xchg %ax,%ax [Verified Entry Point] 0x000000011e3fd020: mov %eax,-0x14000(%rsp) 0x000000011e3fd027: push %rbp 0x000000011e3fd028: sub $0x20,%rsp ;*synchronization entry ; - PrintAssemblyTest::hello@-1 (line 10) 0x000000011e3fd02c: movslq %edx,%r10 0x000000011e3fd02f: mov %edx,%r11d 0x000000011e3fd032: sar $0x1f,%r11d 0x000000011e3fd036: imul $0x431bde83,%r10,%r10 0x000000011e3fd03d: sar $0x32,%r10 0x000000011e3fd041: mov %r10d,%r10d 0x000000011e3fd044: sub %r11d,%r10d 0x000000011e3fd047: imul $0xf4240,%r10d,%r10d ;*irem {reexecute=0 rethrow=0 return_oop=0} ; - PrintAssemblyTest::hello@3 (line 10) 0x000000011e3fd04e: cmp %r10d,%edx 0x000000011e3fd051: je 0x000000011e3fd063 ;*ifne {reexecute=0 rethrow=0 return_oop=0} ; - PrintAssemblyTest::hello@4 (line 10) 0x000000011e3fd053: add $0x20,%rsp 0x000000011e3fd057: pop %rbp 0x000000011e3fd058: mov 0x110(%r15),%r10 0x000000011e3fd05f: test %eax,(%r10) ; {poll_return} 0x000000011e3fd062: retq 0x000000011e3fd063: mov %edx,%ebp 0x000000011e3fd065: sub %r10d,%ebp ;*irem {reexecute=0 rethrow=0 return_oop=0} ; - PrintAssemblyTest::hello@3 (line 10) 0x000000011e3fd068: mov $0xffffff45,%esi 0x000000011e3fd06d: mov %edx,(%rsp) 0x000000011e3fd070: data16 xchg %ax,%ax 0x000000011e3fd073: callq 0x0000000116979080 ; ImmutableOopMap {} ;*ifne {reexecute=1 rethrow=0 return_oop=0} ; - (reexecute) PrintAssemblyTest::hello@4 (line 10) ; {runtime_call UncommonTrapBlob} 0x000000011e3fd078: hlt 0x000000011e3fd079: hlt 0x000000011e3fd07a: hlt 0x000000011e3fd07b: hlt 0x000000011e3fd07c: hlt 0x000000011e3fd07d: hlt 0x000000011e3fd07e: hlt 0x000000011e3fd07f: hlt [Exception Handler] 0x000000011e3fd080: jmpq 0x0000000116a22d80 ; {no_reloc} [Deopt Handler Code] 0x000000011e3fd085: callq 0x000000011e3fd08a 0x000000011e3fd08a: subq $0x5,(%rsp) 0x000000011e3fd08f: jmpq 0x0000000116978ca0 ; {runtime_call DeoptimizationBlob} 0x000000011e3fd094: hlt 0x000000011e3fd095: hlt 0x000000011e3fd096: hlt 0x000000011e3fd097: hlt --------------------------------------------------------------------------------   Interpreting the output is left as an exercise for the astute reader ;-) A great resource for getting started doing so is the post PrintAssembly output explained! by Jean-Philippe Bempel.\n With hsdis in place, you also can use the excellent JitWatch tool for analysing the assembly code, which e.g. not only provides an easy way to navigate from source code to byte code to assembly code, but also comes with helpful tooltips explaining the meaning of the different assembly mnemonics.\n","id":13,"publicationdate":"Oct 5, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eLately I\u0026#8217;ve been fascinated by the possibility to analyse the assembly code emitted by the Java JIT (just-in-time) compiler.\nSo far I had only looked only into Java class files using \u003cem\u003ejavap\u003c/em\u003e;\ndiving into the world of assembly code feels a bit like Alice must have felt when falling down the rabbit whole into wonderland.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Building hsdis for OpenJDK 15","uri":"https://www.morling.dev/blog/building-hsdis-for-openjdk-15/"},{"content":"I\u0026#8217;m excited to share the news about an open-source utility I\u0026#8217;ve been working on lately: JmFrX, a tool for capturing JMX data with JDK Flight Recorder.\n When using JMX (Java Management Extensions), The Java platform\u0026#8217;s standard for monitoring and managing applications, JmFrX allows you to periodically record the attributes from any JMX MBean into JDK Flight Recorder (JFR) files, which you then can analyse using JDK Mission Control (JMC).\n This is useful for a number of reasons:\n   You can track changes to the values of JMX MBean attributes over time without resorting to external monitoring tools\n  You can analyze JMX data from offline JFR recording files in cases where you cannot directly connect to the running application\n  You can export JMX data as live data streams using the JFR event streaming API introduced in Java 14\n   In this blog post I\u0026#8217;m going to explain how to use JmFrX for recording JMX data in your applications, point out some interesting JmFrX implemention details, and lastly will discuss some potential steps for future development of the tool.\n Why JmFrX? JDK Flight Recorder is a \"low-overhead data collection framework for troubleshooting Java applications and the HotSpot JVM\". In combination with the JDK Mission Control client application it allows to gain deep insights into the performance characteristics of Java applications.\n In addition to the built-in metrics and event types, JFR also allows to define and emit custom event types. JFR got open-sourced in JDK 11; since then, developers in the Java eco-system began to support this, enabling users to work with JFR and JMC for analyzing the runtime behavior of 3rd party libraries and frameworks. For instance, JUnit 5.7 produces JFR events related to the execution lifecycle of unit tests.\n At the same time, many library authors are not (yet) in a position where they could easily emit JFR events from their tools, as for instance they might wish to keep compatibility with older Java versions. They might already expose JMX MBeans though which often provide fine-grained information about the execution state of Java applications. This is where JmFrX comes in: by periodically capturing the attribute values from a given set of JMX MBeans, it allows to capture this information in JFR recordings.\n JmFrX isn\u0026#8217;t the first effort that seeks to bridge JMX and JFR; JDK Mission Control project lead Marcus Hirt discusses a similar project in a blog post in 2016. But unlike the implementation described by Marcus in this post, JmFrX is based on the public and supported APIs for defining, configuring and emitting JFR events, as available since OpenJDK 11.\n   How To Use JmFrX In order to use JmFrX, make sure to run OpenJDK 11 or newer. OpenJDK 8 also contains the open-sourced Flight Recorder bits as of release 8u262 (from July this year); so this should work, too, but I haven\u0026#8217;t tested it yet.\n Until a stable release will be provided, you can obtain JmFrX snapshot builds via JitPack. For that, add the JitPack repository to your pom.xml when using Apache Maven (or apply equivalent configuration for your preferred build tool):\n ... \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;jitpack.io\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://jitpack.io\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; ...   Then add the JmFrX dependency:\n ... \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.github.gunnarmorling\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jmfrx\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;master-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; ...   The next step is registering the JmFrX event type with JFR in the start-up routine of your program. This could for instance be done in the main() method, the static initializer of a class loaded early on, an eagerly initialized Spring or CDI bean, etc. A Java agent for this purpose will be provided as part of this project soon.\n When building applications with Quarkus, you could use an application start-up event like so:\n @ApplicationScoped public class EventRegisterer { public void registerEvent(@Observes StartupEvent se) { Jmfrx.getInstance().register(); } public void unregisterEvent(@Observes ShutdownEvent se) { Jmfrx.getInstance().unregister(); } }   Now start your application and create a JFR configuration file which enables the JmFrX event type. To do so, open JDK Mission Control, and choose your running application in the JVM Browser. Then perform these steps:\n   Right-click the target JVM \u0026#8594; Select Start Flight Recording\u0026#8230;\u0026#8203;\n  Click on Template Manager\n  Copy the Continuous setting and click Edit for modifying this copy\n  Expand the JMX and JMX Dump nodes\n  Make sure the JMX Dump event type is Enabled; choose a period for dumping the chosen JMX MBeans (by default 60 s) and specify the MBeans whose data should be captured; that\u0026#8217;s done by means of a regular expression, which matches one or more JMX object names, for instance .*OperatingSystem.*:\n       Close the two last dialogues by clicking OK and OK\n  Important: Make sure that the template you edited is selected under Event settings\n  Click Finish to begin the recording\n   Once the recording is complete, open the recording file in JDK Mission Control and go to the Event Browser. You should see periodic events corresponding to the selected MBeans under the JMX node:\n   When not using JDK Mission Control to initiate recordings, but the jcmd utility on the command line, also follow the same steps as above for creating a configuration as described above. But then, instead of starting the recording, export the configuration file from the template manager and specify its name to jcmd via the settings=/path/to/settings.jfc parameter.\n Now using JmFrX to observe JMX data from for the java.lang MBeans like Runtime and OperatingSystem in JFR isn\u0026#8217;t too exciting yet, as there\u0026#8217;s dedicated JFR event types which contain most of that information. But things get more interesting when capturing data from custom MBean types, as e.g. here for the stream threads metrics from a Kafka Streams application:\n     Customizing Event Formats By default, JmFrX will propagate the raw attribute values from a JMX MBean to the corresponding JFR event. This makes sure that all the information can be retrieved from recordings, but the data format can be a bit unwieldy, e.g. when it comes to data amounts in bytes, or time periods in milli-seconds since epoch.\n To address this, JFR supports a range of metadata annotations such as @DataAmount, @Timespan, or @Percentage, which allow to format event attributes. This information then is used by JMC for instance when displaying events in the browser (see event Properties to the left in the screenshot above).\n JmFrX integrates with this metadata facility via the notion of event profiles, which describe the data format of one MBean type and its attributes. When creating an event for a given JMX MBean, JmFrX will look for a corresponding event profile and apply its settings. Event profiles are defined by implementing the EventProfileContributor SPI. As an example here\u0026#8217;s a subset of the the built-in profile definition for the OperatingSystem MBean:\n public class JavaLangEventProfileContributor implements EventProfileContributor { @Override public void contributeProfiles(EventProfileBuilder builder) { builder.addEventProfile(\"java.lang:type=OperatingSystem\") (1) .addAttributeProfile(\"TotalSwapSpaceSize\", long.class, new AnnotationElement(DataAmount.class, DataAmount.BYTES), (2) v -\u0026gt; v) .addAttributeProfile(\"FreeSwapSpaceSize\", long.class, new AnnotationElement(DataAmount.class, DataAmount.BYTES), v -\u0026gt; v) (3) .addAttributeProfile(\"CpuLoad\", double.class, new AnnotationElement(Percentage.class), v -\u0026gt; v) .addAttributeProfile(\"ProcessCpuLoad\", double.class, new AnnotationElement(Percentage.class), v -\u0026gt; v) .addAttributeProfile(\"SystemCpuLoad\", double.class, new AnnotationElement(Percentage.class), v -\u0026gt; v) .addAttributeProfile(\"ProcessCpuTime\", long.class, new AnnotationElement(Timespan.class, Timespan.NANOSECONDS), v -\u0026gt; v ); } }     1 Profiles are linked via the MBean name   2 The atribute type is specified via an AnnotationElement for one of the JFR type metadata annotations   3 If needed, the actual value can be modified too, e.g. to convert it into another data type, or to shift its value into an expected range (for instance 0 to 1 for percentage values)    Once you\u0026#8217;ve defined the event profiles for your MBean type(s), don\u0026#8217;t forget to register the contributor type either as a service implementation in your module-info.java descriptor (when building a modular Java application):\n module com.example { requires jdk.jfr; requires dev.morling.jmfrx; provides dev.morling.jmfrx.spi.EventProfileContributor with com.example.MyEventProfileContributor; }   When building an application using the traditional classpath, register the names of all profile contributors in the META-INF/services/dev.morling.jmfrx.spi.EventProfileContributor file.\n There\u0026#8217;s a small (yet hopefully growing) set of event profiles built into JmFrX. But as event profile contributors are discovered using the Java service loader mechanism, you can also easily plug in event profiles for other MBean types, e.g. for the JMX MBeans of Apache Kafka or Kafka Connect, or application servers like WildFly.\n Also your pull requests for contributing event profiles for common JMX applications to JmFrX itself will be very welcomed!\n   How It Works If you solely want to use JmFrX, you can pretty much stop reading this post at this point. But if you\u0026#8217;re curious about how it is working internally, stay with me for a bit longer: JmFrX uses two lesser known JFR features which also might be interesting for your own application-specific event types, periodic JFR events and dynamic event types.\n Unlike most JFR event types which are emitted when some specific JVM or application functionality is executed, periodic events are produced in a regular interval. The default interval (which can be overridden by the user) is specified using the @Period annotation on the event type definition:\n @Name(JmxDumpEvent.NAME) @Label(\"JMX Dump\") @Category(\"JMX\") @Description(\"Periodically dumps specific JMX MBeans\") @StackTrace(false) @Period(\"60 s\") public class JmxDumpEvent extends Event { public static final String NAME = \"dev.morling.jmfrx.JmxDumpEvent\"; // event implementation ... }   Upon application start-up, JmFrX registers this event type with the JFR environment:\n ... private Runnable hook; public void register() { hook = () -\u0026gt; { (1) JmxDumpEvent dumpEvent = new JmxDumpEvent(); if (!dumpEvent.isEnabled()) { return; } dumpEvent.begin(); // retrieve data from matching MBean(s) and create event(s) ... dumpEvent.commit(); }; FlightRecorder.addPeriodicEvent(JmxDumpEvent.class, hook); (2) } public void unregister() { FlightRecorder.removePeriodicEvent(hook); (3) } ...     1 The event hook implementation   2 Register the periodic event   3 Unregister the periodic event    The regular expression for specifying the MBean name(s) is passed to the event type as a SettingControl. You can learn more about event settings in my post on custom JFR event types.\n When the periodic event hook runs, it must create one event for each captured MBean. As JmFrX cannot know which MBean(s) you\u0026#8217;re interested in, it\u0026#8217;s not an option to pre-define these event types and their structure.\n This is where dynamic JFR event types come in: Using the EventFactory class, event types can be defined at runtime. Under the covers, JFR will create a corresponding Event sub-class dynamically using the ASM API. Here\u0026#8217;s the relevant JmFrX code which defines the event type for a given MBean:\n ... public static EventDescriptor getDescriptorFor(String mBeanName) { MBeanServer mbeanServer = ManagementFactory.getPlatformMBeanServer(); try { ObjectName objectName = new ObjectName(mBeanName); MBeanInfo mBeanInfo = mbeanServer.getMBeanInfo(objectName); List\u0026lt;AnnotationElement\u0026gt; eventAnnotations = Arrays.asList( (1) new AnnotationElement(Category.class, getCategory(objectName)), new AnnotationElement(StackTrace.class, false), new AnnotationElement(Name.class, getName(objectName)), new AnnotationElement(Label.class, getLabel(objectName)), new AnnotationElement(Description.class, mBeanInfo.getDescription()) ); List\u0026lt;AttributeDescriptor\u0026gt; fields = getFields(objectName, mBeanInfo); List\u0026lt;ValueDescriptor\u0026gt; valueDescriptors = fields.stream() (2) .map(AttributeDescriptor::getValueDescriptor) .collect(Collectors.toList()); return new EventDescriptor(EventFactory.create(eventAnnotations, valueDescriptors), fields); } catch (Exception e) { throw new RuntimeException(e); } } ...     1 Define event metadata like name, label, category etc. via the JFR metadata annotations   2 For each MBean attribute, an attribute is added to the event type; its definition is based on the information in the corresponding event profile, if present    The actual implemention is slightly more complex, as it deals with integrating metadata from JmFrX event profiles and more. You can find the complete code in the EventProfile class.\n   Takeaways JmFrX is a small utility which allows you to capture JMX data with JDK Flight Recorder. It\u0026#8217;s open-source (Apache License, version 2), you can find the source code on GitHub. With the wide usage of JMX for application monitoring in the Java world, JmFrX can help to bring that information into JFR recordings, making it available for offline investigations and analyses.\n Potential next steps for JmFrX include more meaningful handling of tabular and composite JMX data, adding a Java agent for registering the event type, providing some more built-in event profiles and publishing a stable release on Maven Central. Eventually, the JmFrX project might move over to the rh-jmc-team GitHub organization, which is is managed by Red Hat\u0026#8217;s OpenJDK team and contains many other very useful projects around JDK Flight Recorder and Mission Control.\n Your feedback on and contributions to JmFrX will be very welcomed!\n  ","id":14,"publicationdate":"Aug 18, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eI\u0026#8217;m excited to share the news about an open-source utility I\u0026#8217;ve been working on lately:\n\u003ca href=\"https://github.com/gunnarmorling/jmfrx\"\u003eJmFrX\u003c/a\u003e,\na tool for capturing JMX data with JDK Flight Recorder.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eWhen using JMX (\u003ca href=\"https://en.wikipedia.org/wiki/Java_Management_Extensions\"\u003eJava Management Extensions\u003c/a\u003e), The Java platform\u0026#8217;s standard for monitoring and managing applications,\nJmFrX allows you to periodically record the attributes from any JMX MBean into \u003ca href=\"https://openjdk.java.net/jeps/328\"\u003eJDK Flight Recorder\u003c/a\u003e (JFR) files,\nwhich you then can analyse using \u003ca href=\"https://openjdk.java.net/projects/jmc/\"\u003eJDK Mission Control\u003c/a\u003e (JMC).\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Introducing JmFrX: A Bridge From JMX to JDK Flight Recorder","uri":"https://www.morling.dev/blog/introducing-jmfrx-a-bridge-from-jmx-to-jdk-flight-recorder/"},{"content":"I have built a custom search functionality for this blog, based on Java and the Apache Lucene full-text search library, compiled into a native binary using the Quarkus framework and GraalVM. It is deployed as a Serverless application running on AWS Lambda, providing search results without any significant cold start delay. If you thought Java wouldn\u0026#8217;t be the right language for this job, keep reading; in this post I\u0026#8217;m going to give an overview over the implementation of this feature and my learnings along the way.\n Having a search functionality for my blog has been on my mind for quite some time; I\u0026#8217;d like to give users the opportunity to find specific contents on this blog right here on this site, without having to use an external search engine. That\u0026#8217;s not only nice in terms of user experience, but also having insight into the kind of information readers look for on this blog should help me to identify interesting things to write about in the future.\n Now this blog is a static site\u0026#8201;\u0026#8212;\u0026#8201;generated using Hugo, hosted on GitHub Pages\u0026#8201;\u0026#8212;\u0026#8201;which makes this an interesting challenge. I didn\u0026#8217;t want to rely on an external search service (see \"Why No External Search Service\" below for the reasoning), and also a purely client-side solution as described in this excellent blog post didn\u0026#8217;t seem ideal. While technically fascinating, I didn\u0026#8217;t like the fact that it requires shipping the entire search index to the client for executing search queries. Also things like result highlighting, customized result scoring, word stemming, fuzzy search and more seemed a bit more than I\u0026#8217;d be willing to implement on the client.\n All these issues have largely been solved on the server-side by libraries such as Apache Lucene for quite some time. Using a library like Lucene means implementing a custom server-side process, though. How to deploy such service? Operating a VM 24/7 with my search backend for what\u0026#8217;s likely going to be not more than a few dozen queries per month seemed a bit like overkill.\n So after some consideration I decided to implement my own search functionality, based on the highly popular Apache Lucene library, deployed as a Serverless application, which is started on-demand if a user runs a query on my website. In the remainder of this post I\u0026#8217;m going to describe the solution I came up with and how it works.\n If you like, you can try it out right now, this post is about this little search input control at the top right of this page!\n     Why No External Search Service? When tweeting about my serverless search experiment, one of the questions was \"What\u0026#8217;s wrong with Algolia?\". To be very clear, there\u0026#8217;s nothing wrong with it at all. External search services like Algolia, Google Custom Search, or an Elasticsearch provider such as Bonsai promise an easy-to-use, turn-key search functionality which can be a great choice for your specific use case.\n However, I felt that none of these options would provide me the degree of control and customizability I was after. I also ruled out any \"free\" options, as they\u0026#8217;d either mean having ads or paying for the service with the data of myself or that of my readers. And to be honest, I also just fancied the prospect of solving the problem by myself, instead of relying on an off-the-shelf solution.\n     Why Serverless? First of all, let\u0026#8217;s discuss why I opted for a Serverless solution. It boils down to three reasons:\n   Security: While it\u0026#8217;d only cost a few EUR per month to set up a VM with a cloud provider like Digital Ocean or Hetzner, having to manage a full operating system installation would require too much of my attention; I don\u0026#8217;t want someone to mine bitcoins or doing other nasty things on a box I run, just because I failed to apply some security patch\n  Cost: Serverless does not only promise to scale-out (and let\u0026#8217;s be honest, there likely won\u0026#8217;t be millions of search queries on my blog every month), but also scale-to-zero. As Serverless is pay-per-use and there are free tiers in place e.g. for AWS Lambda, this service ideally should cost me just a few cents per month\n  Learning Opportunity: Last but not least, this also should be a nice occasion for me to dive into the world of Serverless, by means of designing, developing and running a solution for a real-world problem, exploring how Java as my preferred programming language can be used for this task\n     Solution Overview The overall idea is quite simple: there\u0026#8217;s a simple HTTP service which takes a query string, runs the query against a Lucene index with my blog\u0026#8217;s contents and returns the search results to the caller. This service gets invoked via JavaScript from my static blog pages, where results are shown to the user.\n The Lucene search index is read-only and gets rebuilt whenever I update the blog. It\u0026#8217;s baked into the search service deployment package, which that way becomes fully immutable. This reduces complexities and the attack surface at runtime. Surely that\u0026#8217;s not an approach that\u0026#8217;s viable for more dynamic use cases, but for a blog that\u0026#8217;s updated every few weeks, it\u0026#8217;s perfect. Here\u0026#8217;s a visualization of the overall flow:\n   The search service is deployed as a Serverless function on AWS Lambda. One important design goal for me is to avoid lock-in to any specific cloud provider: the solution should be portable and also be usable with container-based Serverless approaches like Knative.\n Relying on a Serverless architecture means its start-up time must be a matter of milli-seconds rather than seconds, so to not have a user wait for a noticeable amount of time in case of a cold start. While substantial improvements have been made in recent Java versions to improve start-up times, it\u0026#8217;s still not ideal for this kind of use case. Therefore, the application is compiled into a native binary via Quarkus and GraalVM, which results in a start-up time of ~30 ms on my laptop, and ~180 ms when deployed to AWS Lambda. With that we\u0026#8217;re in a range where a cold start won\u0026#8217;t impact the user experience in any significant way.\n The Lambda function is exposed to callers via the AWS API Gateway, which takes incoming HTTP requests, maps them to calls of the function and converts its response into an HTTP response which is sent back to the caller.\n Now let\u0026#8217;s dive down a bit more into the specific parts of the solution. Overall, there are four steps involved:\n   Data extraction: The blog contents to be indexed must be extracted and converted into an easy-to-process data format\n  Search backend implementation: A small HTTP service is needed which exposes the search functionality of Apache Lucene, which in particular requires some steps to enable Lucene being used in a native GraalVM binary\n  Integration with the website: The search service must be integrated into the static site on GitHub Pages\n  Deployment: Finally, the search service needs to be deployed to AWS API Gateway and Lambda\n     Data Extraction The first step was to obtain the contents of my blog in an easily processable format. Instead of requiring something like a real search engine\u0026#8217;s crawler, I essentially only needed to have a single file in a structured format which then can be passed on to the Lucene indexer.\n This task proved rather easy with Hugo; by means of a custom output format it\u0026#8217;s straight-forward to produce a JSON file which contains the text of all my blog pages. In my config.toml I declared the new output format and activate it for the homepage (largely inspired by this write-up):\n [outputFormats.SearchIndex] mediaType = \"application/json\" baseName = \"searchindex\" isPlainText = true notAlternative = true [outputs] home = [\"HTML\",\"RSS\", \"SearchIndex\"]   The template in layouts/_default/list.searchindex.json isn\u0026#8217;t too complex either:\n {{- $.Scratch.Add \"searchindex\" slice -}} {{- range $index, $element := .Site.Pages -}} {{- $.Scratch.Add \"searchindex\" (dict \"id\" $index \"title\" $element.Title \"uri\" $element.Permalink \"tags\" $element.Params.tags \"section\" $element.Section \"content\" $element.Plain \"summary\" $element.Summary \"publicationdate\" ($element.Date.Format \"Jan 2, 2006\")) -}} {{- end -}} {{- $.Scratch.Get \"searchindex\" | jsonify -}}   The result is this JSON file:\n [...{\"content\":\"The JDK Flight Recorder (JFR) is an invaluable tool...\",\"id\":12,\"publicationdate\":\"Jan 29, 2020\",\"section\":\"blog\",\"summary\":\"\\u003cdiv class=\\\"paragraph\\\"\\u003e\\n\\u003cp\\u003eThe \\u003ca href=\\\"https://openjdk.java.net/jeps/328\\\"\\u003eJDK Flight Recorder\\u003c/a\\u003e (JFR) is an invaluable tool...\",\"tags\":[\"java\",\"monitoring\",\"microprofile\",\"jakartaee\",\"quarkus\"],\"title\":\"Monitoring REST APIs with Custom JDK Flight Recorder Events\",\"uri\":\"https://www.morling.dev/blog/rest-api-monitoring-with-custom-jdk-flight-recorder-events/\"},...]   This file gets automatically updated whenever I republish the blog.\n   Search Backend Implementation My stack of choice for this kind of application is Quarkus. As a contributor, I am of course biased, but Quarkus is ideal for the task at hand: built and optimized from the ground up for implementing fast-starting and memory-efficient cloud-native and Serverless applications, it makes building HTTP services, e.g. based on JAX-RS, running on GraalVM a trivial effort.\n Now typically a Java library such as Lucene will not run in a GraalVM native binary out-of-the-box. Things like reflection or JNI usage require specific configuration, while other Java features like method handles are only supported partly or not at all.\n Apache Lucene in a GraalVM Native Binary Quarkus enables a wide range of popular Java libraries to be used with GraalVM, but at this point there\u0026#8217;s no extension yet which would take care of Lucene. So I set out to implement a small Quarkus extension for Lucene. Depending on the implementation details of the library in question, this can be a more or less complex and time-consuming endeavor. The workflow is like so:\n   compile down an application using the library into a native image\n  run into some sort of exception, e.g. due to types accessed via Java reflection (which causes the GraalVM compiler to miss them during call flow analysis so that they are missing from the generated binary image)\n  fix the issue e.g. by registering the types in question for reflection\n  rinse and repeat\n   The good thing there is that the list of Quarkus extensions is constantly growing, so that you hopefully don\u0026#8217;t have to go through this by yourself. Or if you do, consider publishing your extension via the Quarkus platform, saving others from the same work.\n For my particular usage of Lucene, I ran luckily into two issues only. The first is the usage of method handles in the AttributeFactory class for dynamically instantiating sub-classes of the AttributeImpl type, which isn\u0026#8217;t supported in that form by GraalVM. One way for dealing with this is to define substitutions, custom methods or classes which will override a specific original implementation. As an example, here\u0026#8217;s one of the substitution classes I had to create:\n @TargetClass(className = \"org.apache.lucene.util.AttributeFactory$DefaultAttributeFactory\") public final class DefaultAttributeFactorySubstitution { public DefaultAttributeFactorySubstitution() {} @Substitute public AttributeImpl createAttributeInstance(Class\u0026lt;? extends Attribute\u0026gt; attClass) { if (attClass == BoostAttribute.class) { return new BoostAttributeImpl(); } else if (attClass == CharTermAttribute.class) { return new CharTermAttributeImpl(); } else if (...) { ... } throw new UnsupportedOperationException(\"Unknown attribute class: \" + attClass); } }   During native image creation, the GraalVM compiler will discover all substitute classes and apply their code instead of the original ones.\n The other problem I ran into was the usage of method handles in the MMapDirectory class, which will be used by Lucene by default on Linux when obtaining a file-system backed index directory. I didn\u0026#8217;t explore how to circumvent that, instead I opted for using the SimpleFSDirectory implementation which proved to work fine in my native GraalVM binary.\n While this was enough in order to get Lucene going in a native image, you might run into different issues when using other libraries with GraalVM native binaries. Quarkus comes with a rich set of so-called build items which extension authors can use in order to enable external dependencies on GraalVM, e.g. for registering classes for reflective access or JNI, adding additional resources to the image, and much more. I recommend you take a look at the extension author guide in order to learn more.\n Besides enabling Lucene on GraalVM, that Quarkus extension also does two more things:\n   Parse the previously extracted JSON file, build a Lucene index from that and store that index in the file system; that\u0026#8217;s fairly standard Lucene procedure without anything noteworthy; I only had to make sure that the index fields are stored in their original form in the search index, so that they can be accessed at runtime when displaying fragments with the query hits\n  Register a CDI bean, which allows to obtain the index at runtime via @Inject dependency injection from within the HTTP endpoint class\n   A downside of creating binaries via GraalVM is the increased build time: creating a native binary for macOS via a locally installed GraalVM SDK takes about two minutes on my laptop. For creating a Linux binary to be used with AWS Lambda, I need to run the build in a Linux container, which takes about five minutes. But typically this task is only done once when actually deploying the application, whereas locally I\u0026#8217;d work either with the Quarkus Dev Mode (which does a live reload of the application as its code changes) or test on the JVM. In any case it\u0026#8217;s a price worth paying: only with start-up times in the range of milli-seconds on-demand Serverless cold starts with the user waiting for a response become an option.\n  The Search HTTP Service The actual HTTP service implementation for running queries is rather unspectacular; It\u0026#8217;s based on JAX-RS and exposes as simple endpoint which can be invoked with a given query like so:\n http \"https://my-search-service/search?q=java\" HTTP/1.1 200 OK Connection: keep-alive Content-Length: 4930 Content-Type: application/json Date: Tue, 21 Jul 2020 17:05:00 GMT { \"message\": \"ok\", \"results\": [ { \"fragment\": \"...plug-ins. In this post I\u0026amp;#8217;m going to explore how the \u0026lt;b\u0026gt;Java\u0026lt;/b\u0026gt; Platform Module System's notion of module layers can be leveraged for implementing plug-in architectures on the JVM. We\u0026amp;#8217;ll also discuss how Layrry, a launcher and runtime for layered \u0026lt;b\u0026gt;Java\u0026lt;/b\u0026gt; applications, can help with this task. A key requirement...\", \"publicationdate\": \"Apr 21, 2020\", \"title\": \"Plug-in Architectures With Layrry and the \u0026lt;b\u0026gt;Java\u0026lt;/b\u0026gt; Module System\", \"uri\": \"https://www.morling.dev/blog/plugin-architectures-with-layrry-and-the-java-module-system/\" }, { \"fragment\": \"...the current behavior indeed is not intended (see JDK-8236597) and in a future \u0026lt;b\u0026gt;Java\u0026lt;/b\u0026gt; version the shorter version of the code shown above should work. Wrap-Up In this blog post we\u0026amp;#8217;ve explored how invariants on \u0026lt;b\u0026gt;Java\u0026lt;/b\u0026gt; 14 record types can be enforced using the Bean Validation API. With just a bit...\", \"publicationdate\": \"Jan 20, 2020\", \"title\": \"Enforcing \u0026lt;b\u0026gt;Java\u0026lt;/b\u0026gt; Record Invariants With Bean Validation\", \"uri\": \"https://www.morling.dev/blog/enforcing-java-record-invariants-with-bean-validation/\" }, ... ] }   Internally it\u0026#8217;s using Lucene\u0026#8217;s MultiFieldQueryParser for parsing the query and running it against the \"title\" and \"content\" fields of the index. It is set to combine multiple terms using the logical AND operator by default (who ever would want the default of OR?), it supports phrase queries given in quotes, and a number of other query operators.\n Query hits are highlighted using the FastVectorHighlighter highlighter and SimpleHTMLFormatter as a fallback (not all kinds of queries can be processed by FastVectorHighlighter). The highlighter wraps the matched search terms in the returned fragment in \u0026lt;b\u0026gt; tags, which are styled appropriately in my website\u0026#8217;s CSS. I was prepared to do some adjustments to result scoring, but this wasn\u0026#8217;t necessary so far. Title matches are implicitly ranked higher than content matches due to the shorter length of the title field values.\n Implementing the service using a standard HTTP interface instead of relying on specific AWS Lambda contracts is great in terms of local testing as well as portability: I can work on the service using the Quarkus Dev Mode and invoke it locally, without having to deploy it into some kind of Lambda test environment. It also means that should the need arise, I can take this service and run it elsewhere, without requiring any code changes. As I\u0026#8217;ll discuss in a bit, Quarkus takes care of making this HTTP service runnable within the Lambda environment by means of a single dependency configuration.\n    Wiring Things Up Now it was time to hook up the search service into my blog. I wouldn\u0026#8217;t want to have the user navigate to the URL of the AWS API Gateway in their browser; this means that the form with the search text input field cannot actually be submitted. Instead, the default form handling must be disabled, and the search string be sent via JavaScript to the API Gateway URL.\n This means the search feature won\u0026#8217;t work for users who have JavaScript disabled in their browser. I deemed this an acceptable limitation; in order to avoid unnecessary confusion and frustration, the search text input field is hidden in that case via CSS:\n \u0026lt;noscript\u0026gt; \u0026lt;style type=\"text/css\"\u0026gt; .search-input { display:none; } \u0026lt;/style\u0026gt; \u0026lt;/noscript\u0026gt;   The implementation of the backend call is fairly standard JavaScript business using the XMLHttpRequest API, so I\u0026#8217;ll spare you the details here. You can find the complete implementation in my GitHub repo.\n There\u0026#8217;s one interesting detail to share though in terms of improving the user experience after a cold start. As mentioned above, the Quarkus application itself starts up on Lambda in about ~180 ms. Together with the initialization of the Lambda execution environment I typically see ~370 ms for a cold start. Add to that the network round-trip times, and a user will feel a slight delay. Nothing dramatical, but it doesn\u0026#8217;t have that snappy instant feeling you get when executing the search with a warm environment.\n Thinking about the typical user interaction though, the situation can be nicely improved: if a visitor puts the focus onto the search text input field, it\u0026#8217;s highly likely that they will submit a query shortly thereafter. We can take advantage of that and have the website send a small \"ping\" request right at the point when the input field obtains the focus. This gives us enough headstart to have the Lambda function being started before the actual query comes in. Here\u0026#8217;s the request flow of a typical interaction (the \"Other\" requests are CORS preflight requests):\n   Note how the search call is issued only a few hundred ms after the ping. Now you could beat this e.g. when navigating to the text field using your keyboard and if you were typing really fast. But most users will use their mouse or touchpad to put the cursor into the input, and then change to the keyboard to enter the query, which is time enough for this little trick to work.\n The analysis of the logs confirms that essentially all executed queries hit a warmed up Lambda function, making cold starts a non-issue. To avoid any unneeded warm-up calls, they are only done when entering the input field for the first time after loading the page, or when staying on the page for long enough, so that the Lambda might have shut down again due to lack of activity.\n Of course you\u0026#8217;ll be charged for the additional ping requests, but for the volume I expect, this makes no relevant difference whatsoever.\n   Deployment to AWS Lambda The last part of my journey towards a Serverless search function was deployment to AWS Lambda. I was exploring Heroku and Google Cloud Run as alternatives, too. Both allow you to deploy regular container images, which then are automatically scaled on demand. This results in great portability, as things hardly can get any more standard than plain Linux containers.\n With Heroku, cold start times proved problematic, though: I observed 5 - 6 seconds, which completely ruling it out. This wasn\u0026#8217;t a problem with Cloud Run, and it\u0026#8217;d surely work very well overall. In the end I went for AWS Lambda, as its entire package of service runtime, API Gateway and web application firewall seemed more complete and mature to me.\n With AWS Lambda, I observed cold start times of less than 0.4 sec for my actual Lambda function, plus the actual request round trip. Together with the warm-up trick described above, this means that a user practically never will get a cold start when executing the search.\n You shouldn\u0026#8217;t under-estimate the time needed though to get familiar with Lambda itself, the API Gateway which is needed for routing HTTP requests to your function and the interplay of the two.\n To get started, I configured some playground Lambda and API in the web console, but eventually I needed something along the lines of infrastructure-as-code, means of reproducible and automated steps for configuring and setting up all the required components. My usual go-to solution in this area is Terraform, but here I settled for the AWS Serverless Application Model (SAM), which is tailored specifically to setting up Serverless apps via Lambda and API Gateway and thus promised to be a bit easier to use.\n Building Quarkus Applications for AWS Lambda Quarkus supports multiple approaches for building Lambda-based applications:\n   You can directly implement Lambda\u0026#8217;s APIs like RequestHandler, which I wanted to avoid though for the sake of portability between different environments and cloud providers\n  You can use the Quarkus Funqy API for building portable functions which e.g. can be deployed to AWS, Azure Functions and Google Cloud Functions; the API is really straight-forward and it\u0026#8217;s a very attractive option, but right now there\u0026#8217;s no way to use Funqy for implementing an HTTP GET API with request parameters, which ruled out this option for my purposes\n  You can implement your Lambda function using the existing and well-known HTTP APIs of Vert.x, RESTEasy (JAX-RS) and Undertow; in this case Quarkus will take care of mapping the incoming function call to the matching HTTP endpoint of the application\n   Used together with the proxy feature of the AWS API Gateway, the third option is exactly what I was looking for. I can implement the search endpoint using the JAX-RS API I\u0026#8217;m familiar with, and the API Gateway proxy integration together with Quarkus' glue code will take care of everything else for running this. This is also great in terms of portability: I only need to add the io.quarkus:quarkus-amazon-lambda-http dependency to my project, and the Quarkus build will emit a function.zip file which can be deployed to AWS Lambda. I\u0026#8217;ve put this into a separate Maven build profile, so I can easily switch between creating the Lambda function deployment package and a regular container image with my REST endpoint which I can deploy to Knative and environments like OpenShift Serverless, without requiring any code changes whatsoever.\n The Quarkus Lambda extension also produces templates for the AWS SAM tool for deploying my stack. They are a good starting point which just needs a little bit of massaging; For the purposes of cost control (see further below), I added an API usage plan and API key. I also enabled CORS so that the API can be called from my static website. This made it necessary to disable the configuration of binary media types which the generated template contains by default. Lastly, I used a specific pre-configured execution role instead of the default AWSLambdaBasicExecutionRole.\n With the SAM descriptor in place, re-building and publishing the search service becomes a procedure of three steps:\n mvn clean package -Pnative,lambda -DskipTests=true \\ -Dquarkus.native.container-build=true sam package --template-file sam.native.yaml \\ --output-template-file packaged.yaml \\ --s3-bucket \u0026lt;my S3 bucket\u0026gt; sam deploy --template-file packaged.yaml \\ --capabilities CAPABILITY_IAM \\ --stack-name \u0026lt;my stack name\u0026gt;   The lambda profile takes care of adding the Quarkus Lambda HTTP extension, while the native profile makes sure that a native binary is built instead of a JAR to be run on the JVM. As I need to build a Linux binary for the Lambda function while running on macOS locally, I\u0026#8217;m using the -Dquarkus.native.container-build=true option, which will make the Quarkus build running in a container itself, producing a Linux binary no matter which platform this build itself is executed on.\n The function.zip file produced by the Quarkus build has a size of ~15 MB, i.e. it\u0026#8217;s uploaded and deployed to Lambda in a few seconds. Currently it also contains the Lucene search index, meaning I need to run the time-consuming GraalVM build whenever I want to update the index. As an optimization I might at some point extract the index into a separate Lambda layer, which then could be deployed by itself, if there were no code changes to the search service otherwise.\n  Identity and Access Management A big pain point for me was identity and access management (IAM) for the AWS API Gateway and Lambda. While the AWS IAM is really powerful and flexible, there\u0026#8217;s unfortunately no documentation, which would describe the minimum set of required permissions in order to deploy a stack like my search using SAM.\n Things work nicely if you use a highly-privileged account, but I\u0026#8217;m a strong believer into running things with only the least privileges needed for the job. For instance I don\u0026#8217;t want my Lambda deployer to set up the execution role, but rather have it using one I pre-defined. The same goes for other resources like the S3 bucket used for uploading the deployment package.\n Identifying the set of privileges actually needed is a rather soul-crushing experience of trial and error (please let me know in the comments below if there\u0026#8217;s a better way to do this), which gets complicated by the fact that different resources in the AWS stack expose insufficient privileges in inconsistent ways, or sometimes in no really meaningful way at all when configured via SAM. I spent hours identifying a lacking S3 privilege when trying to deploy a Lambda layer from the Serverless Application Repository.\n Hoping to spare others from this tedious work, here\u0026#8217;s the policy for my deployment role I came up with:\n {\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":[\"s3:PutObject\",\"s3:GetObject\"],\"Resource\":[\"arn:aws:s3:::\u0026lt;deployment-bucket\u0026gt;\",\"arn:aws:s3:::\u0026lt;deployment-bucket\u0026gt;/*\"]},{\"Effect\":\"Allow\",\"Action\":[\"lambda:CreateFunction\",\"lambda:GetFunction\",\"lambda:GetFunctionConfiguration\",\"lambda:AddPermission\",\"lambda:UpdateFunctionCode\",\"lambda:ListTags\",\"lambda:TagResource\",\"lambda:UntagResource\"],\"Resource\":[\"arn:aws:lambda:eu-central-1:\u0026lt;account-id\u0026gt;:function:search-morling-dev-SearchMorlingDev-*\"]},{\"Effect\":\"Allow\",\"Action\":[\"iam:PassRole\"],\"Resource\":[\"arn:aws:iam::\u0026lt;account-id\u0026gt;:role/\u0026lt;execution-role\u0026gt;\"]},{\"Effect\":\"Allow\",\"Action\":[\"cloudformation:DescribeStacks\",\"cloudformation:DescribeStackEvents\",\"cloudformation:CreateChangeSet\",\"cloudformation:ExecuteChangeSet\",\"cloudformation:DescribeChangeSet\",\"cloudformation:GetTemplateSummary\"],\"Resource\":[\"arn:aws:cloudformation:eu-central-1:\u0026lt;account-id\u0026gt;:stack/search-morling-dev/*\",\"arn:aws:cloudformation:eu-central-1:aws:transform/Serverless-2016-10-31\"]},{\"Effect\":\"Allow\",\"Action\":[\"apigateway:POST\",\"apigateway:PATCH\",\"apigateway:GET\"],\"Resource\":[\"arn:aws:apigateway:eu-central-1::/restapis\",\"arn:aws:apigateway:eu-central-1::/restapis/*\"]},{\"Effect\":\"Allow\",\"Action\":[\"apigateway:POST\",\"apigateway:GET\"],\"Resource\":[\"arn:aws:apigateway:eu-central-1::/usageplans\",\"arn:aws:apigateway:eu-central-1::/usageplans/*\",\"arn:aws:apigateway:eu-central-1::/apikeys\",\"arn:aws:apigateway:eu-central-1::/apikeys/search-morling-dev-apikey\"]}]}   Perhaps this could be trimmed down some more, but I felt it\u0026#8217;s good enough for my purposes.\n  Performance At this point I haven\u0026#8217;t conducted any systematic performance testing yet. There\u0026#8217;s definitely a significant difference in terms of latency between running things locally on my (not exactly new) laptop and on AWS Lambda. Where the app starts up in ~30 ms locally, it\u0026#8217;s ~180 ms when deployed to Lambda. Note this is only the number reported by Quarkus itself, the entire cold start duration of the application on Lambda, i.e. including the time required for fetching the code to the execution environment and starting the container, is ~370 ms (with 256 MB RAM assigned). Due to the little trick described above, though, a visitor is very unlikely to ever experience this delay when executing a query.\n Similarly, there\u0026#8217;s a substantial difference in terms of request execution duration. Still, when running a quick test of the deployed service via Siege, the vast majority of Lambda executions clocked in well below 100 ms (depending on the number of query hits which need result highlighting), putting them into the lowest bracket of billed Lambda execution time. As I learned, Lambda allocates CPU resources proportionally to assigned RAM, meaning assigning twice as much RAM should speed up execution, also if my application actually does not need that much memory. Indeed, with 512 MB RAM assigned, Lambda execution is down to ~30 - 40 ms after some warm-up, which is more than good enough for my purposes.\n Raw Lambda execution of course is only one part of the overall request duration, on top of that some time is spent in the API Gateway and on the wire to the user; The service is deployed in the AWS eu-central-1 region (Frankfurt, Germany), yielding roundtrip times for me, living a few hundred km away, between 50 - 70 ms (again with 512 MB RAM). With longer distances, network latencies outweigh the Lambda execution time: My good friend Eric Murphy from Seattle in the US reported a roundtrip time of ~240 ms when searching for \"Java\", which I think is still quite good, given the long distance.\n  Cost Control The biggest issue for me as a hobbyist when using pay-per-use services like AWS Lambda and API Gateway is cost control. Unlike typical enterprise scenarios where you might be willing to accept higher cost for your service in case of growing demand, in my case I\u0026#8217;d rather set up a fixed spending limit and shut down my search service for the rest of the month, once that has been reached. I absolutely cannot have an attacker doing millions and millions of calls against my API which could cost me a substantial amount of money.\n Unfortunately, there\u0026#8217;s no easy way on AWS for setting up a maximum spending after which all service consumption would be stopped. Merely setting up a budget alert won\u0026#8217;t cut it either, as this won\u0026#8217;t help me while sitting on a plane for 12h (whenever that will be possible again\u0026#8230;\u0026#8203;) or being on vacation for three weeks. And needless to say, I don\u0026#8217;t have an ops team monitoring my blog infrastructure 24/7 either.\n So what to do to keep costs under control? An API usage plan is the first part of the answer. It allows you to set up a quota (maximum number of calls in a given time frame) which is pretty much what I need. Any calls beyond the quota are rejected by the API Gateway and not charged.\n There\u0026#8217;s one caveat though: a usage plan is tied to an API key, which the caller needs to pass using the X-API-Key HTTP request header. The idea being that different usage plans can be put in place for different clients of an API. Any calls without the API key are not charged either. Unfortunately though this doesn\u0026#8217;t play well with CORS preflight requests as needed in my particular use case. Such requests will be sent by the browser before the actual GET calls to validate that the server actually allows for that cross-origin request. CORS preflight requests cannot have any custom request headers, though, meaning they cannot be part of a usage plan. The AWS docs are unclear whether those preflight requests are charged or not, and in a way it seems unfair if they were charged given there\u0026#8217;s no way to prevent this situation. But at this point it is fair to assume they are charged and we need a way to prevent having to pay for a gazillion preflight calls by a malicious actor.\n In good software developer\u0026#8217;s tradition I turned to Stack Overflow for finding help, and indeed I received a nice idea: A budget alert can be linked with an SNS topic, to which a message will be sent once the alert triggers. Then another Lambda function can be used to set the allowed rate of API invocations to 0, effectively disabling the API, preventing any further cost to pile up. A bit more complex than I was hoping for, but it does the trick. Thanks a lot to Harish for providing this nice answer on Stack Overflow and his blog! I implemented this solution and sleep much better now.\n Note that you should set the alert to a lower value than what you\u0026#8217;re actually willing to spend, as billing happens asynchronously and requests might come in some more time until the alert triggers: as per Corey Quinn, there\u0026#8217;s an \"8-48 hour lag between 'you incur the charge' and 'it shows up in the billing system where an alert can see it and thus fire'\". It\u0026#8217;s therefore also a good idea to reduce the allowed request rate. E.g. in my case I\u0026#8217;m not expecting really that there\u0026#8217;d be more than let\u0026#8217;s say 25 concurrent requests (unless this post hits the Hackernews front page of course), so setting the allowed rate to that value helps to at least slow down the spending until the alert triggers.\n With these measures in place, there should (hopefully!) be no bad surprises at the end of the month. Assuming a (very generously estimated) number of 10K search queries per month, each returning a payload of 5 KB, I\u0026#8217;d be looking at an invoice over EUR 0.04 for the API Gateway, while the Lambda executions would be fully covered by the AWS free tier. That seems manageable :)\n    Wrap-Up and Outlook Having rolled out the search feature for this blog a few days ago, I\u0026#8217;m really happy with the outcome. It was a significant amount of work to put everything together, but I think a custom search is a great addition to this site which hopefully proves helpful to my readers. Serverless is a perfect architecture and deployment option for this use case, being very cost-efficient for the expected low volume of requests, and providing a largely hands-off operations experience for myself.\n With AOT compilation down to native binaries and enabling frameworks like Quarkus, Java definitely is in the game for building Serverless apps. Its huge eco-system of libraries such as Apache Lucene, sophisticated tooling and solid performance make it a very attractive implementation choice. Basing the application on Quarkus makes it a matter of configuration to switch between creating a deployment package for Lambda and a regular container image, avoiding any kind of lock-in into a specific platform.\n Enabling libraries for being used in native binaries can be a daunting task, but over time I\u0026#8217;d expect either library authors themselves to do the required adjustment to smoothen that experience, and of course the growing number of Quarkus extensions also helps to use more and more Java libraries in native apps. I\u0026#8217;m also looking forward to Project Leyden, which aims at making AOT compilation a part of the Java core platform.\n The deployment to AWS Lambda and API Gateway was definitely more involved than I had anticipated; things like IAM and budget control are more complex than I think they could and should be. That there is no way to set up a hard spend capping is a severe shortcoming; hobbyists like myself should be able to explore this platform without having to fear any surprise AWS bills. It\u0026#8217;s particular bothersome that API usage plans are no 100% safe way to enforce API quotas, as they cannot be applied to unauthorized CORS pref-flight requests and custom scripting is needed in order to close this loophole.\n But then this experiment also was an interesting learning experience for me; working on libraries and integration solutions most of the time during my day job, I sincerely enjoyed the experience of designing a service from the ground-up and rolling it out into \"production\", if I may dare to use that term here.\n While the search functionality is rolled out on my blog, ready for you to use, there\u0026#8217;s a few things I\u0026#8217;d like to improve and expand going forward:\n   CI pipeline: Automatically re-building and deploying the search service after changes to the contents of my blog; this should hopefully be quite easy using GitHub Actions\n  Performance improvements: While the performance of the query service definitely is good enough, I\u0026#8217;d like to see whether and how it could be tuned here and there. Tooling might be challenging there; where I\u0026#8217;d use JDK Flight Recorder and Mission Control with a JVM based application, I\u0026#8217;m much less familiar with equivalent tooling for native binaries. One option I\u0026#8217;d like to explore in particular is taking advantage of Quarkus bytecode recording capability: bytecode instructions for creating the in-memory data structure of the Lucene index could be recorded at build time and then just be executed at application start-up; this might be the fastest option for loading the index in my special use case of a read-only index\n  Serverless comments: Currently I\u0026#8217;m using Disqus for the commenting feature of the blog. It\u0026#8217;s not ideal in terms of privacy and page loading speed, which is why I\u0026#8217;m looking for alternatives. One idea could be a custom Serverless commenting functionality, which would be very interesting to explore, in particular as it shifts the focus from a purely immutable application to a stateful service that\u0026#8217;ll require some means of modifiable, persistent storage\n   In the meantime, you can find the source code of the Serverless search feature on GitHub. Feel free to take the code and deploy it to your own website!\n Many thanks to Hans-Peter Grahsl and Eric Murphy for their feedback while writing this post!\n  ","id":15,"publicationdate":"Jul 29, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003e\u003cem\u003eI have built a custom search functionality for this blog,\nbased on Java and the Apache Lucene full-text search library,\ncompiled into a native binary using the Quarkus framework and GraalVM.\nIt is deployed as a Serverless application running on AWS Lambda,\nproviding search results without any significant cold start delay.\nIf you thought Java wouldn\u0026#8217;t be the right language for this job, keep reading;\nin this post I\u0026#8217;m going to give an overview over the implementation of this feature and my learnings along the way.\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"How I Built a Serverless Search for My Blog","uri":"https://www.morling.dev/blog/how-i-built-a-serverless-search-for-my-blog/"},{"content":"Ahead-of-time compilation (AOT) is the big topic in the Java ecosystem lately: by compiling Java code to native binaries, developers and users benefit from vastly improved start-up times and reduced memory usage. The GraalVM project made huge progress towards AOT-compiled Java applications, and Project Leyden promises to standardize AOT in a future version of the Java platform.\n This makes it easy to miss out on significant performance improvements which have been made on the JVM in recent Java versions, in particular when it comes to faster start-up times. Besides a range of improvements related to class loading, linking and bytecode verification, substantial work has been done around class data sharing (CDS). Faster start-ups are beneficial in many ways: shorter turnaround times during development, quicker time-to-first-response for users in coldstart scenarios, cost savings when billed by CPU time in the cloud.\n With CDS, class metadata is persisted in an archive file, which during subsequent application starts is mapped into memory. This is faster than loading the actual class files, resulting in reduced start-up times. When starting multiple JVM processes on the same host, read-only archives of class metadata can also be shared between the VMs, so that less memory is consumed overall.\n Originally a partially commercial feature of the Oracle JDK, CDS was completely open-sourced in JDK 10 and got incrementally improved since then in a series of Java improvement proposals:\n   JEP 310, Application Class-Data Sharing (AppCDS), in JDK 10: \"To improve startup and footprint, extend the existing [CDS] feature to allow application classes to be placed in the shared archive\"\n  JEP 341, Default CDS Archives, in JDK 12: \"Enhance the JDK build process to generate a class data-sharing (CDS) archive, using the default class list, on 64-bit platforms\"\n  JEP 350, Dynamic CDS Archives, in JDK 13: \"Extend application class-data sharing to allow the dynamic archiving of classes at the end of Java application execution. The archived classes will include all loaded application classes and library classes that are not present in the default, base-layer CDS archive\"\n   In the remainder of this blog post we\u0026#8217;ll discuss how to automatically create AppCDS archives as part of your (Maven) project build, based on the improvements made with JEP 350. I.e. Java 13 or later is a prerequisite for this. To learn more about using CDS with the current LTS release JDK 11 and about CDS in general, refer to the excellent blog post on everything CDS by Nicolai Parlog.\n Manually Creating CDS Archives At first let\u0026#8217;s see what\u0026#8217;s needed to manually create and use an AppCDS archive (note I\u0026#8217;m going to use \"AppCDS\" and \"CDS\" somewhat interchangeably for the sake of brevity). Subsequently, we\u0026#8217;ll discuss how the task can be automated in a Maven project build.\n To have an example to work with which goes beyond a plain \"Hello World\", I\u0026#8217;ve created a small web application for managing personal to-dos, using the Quarkus stack. If you\u0026#8217;d like to follow along, clone the repo and build the project:\n git clone git@github.com:gunnarmorling/quarkus-cds.git cd quarkus-cds mvn clean verify -DskipTests=true   The application uses a Postgres database for persisting the to-dos; fire it up via Docker:\n cd compose docker run -d -p 5432:5432 --name pgdemodb \\ -v $(pwd)/init.sql:/docker-entrypoint-initdb.d/init.sql \\ -e POSTGRES_USER=todouser \\ -e POSTGRES_PASSWORD=todopw \\ -e POSTGRES_DB=tododb postgres:11   The next step is to run the application and create the CDS archive file. Do so by passing the -XX:ArchiveClassesAtExit option:\n java -XX:ArchiveClassesAtExit=target/app-cds.jsa \\ (1) -jar target/todo-manager-1.0.0-SNAPSHOT-runner.jar     1 Triggers creation of a CDS archive at the given location upon application shutdown    Only loaded classes will be added to the archive. As classloading on the JVM happens lazily, you must invoke some functionality in your application in order to cause all the relevant classes to be loaded. For that to happen, open the application\u0026#8217;s API endpoint in a browser or invoke it via curl, httpie or similar:\n http localhost:8080/api   Stop the application by hitting Ctrl+C. This will create the CDS archive under target/app-cds.jsa. In our case it should have a size of about 41 MB. Also observe the log messages about classes which were skipped from archiving:\n ... [190.220s][warning][cds] Skipping java/lang/invoke/LambdaForm$MH+0x0000000800bd0c40: Hidden or Unsafe anonymous class [190.220s][warning][cds] Skipping java/lang/invoke/LambdaForm$DMH+0x0000000800fdc840: Hidden or Unsafe anonymous class [190.220s][warning][cds] Pre JDK 6 class not supported by CDS: 46.0 antlr/TokenStreamIOException ...   Mostly this is about hidden or anonymous classes which cannot be archived; there\u0026#8217;s not so much you can do about that (apart from using less Lambda expressions perhaps\u0026#8230;\u0026#8203;).\n The hint on old classfile versions is more actionable: only classes using classfile format 50 (= JDK 1.6) or newer are supported by CDS. In the case at hand, the classes from Antlr 2.7.7 are using classfile format 46 (which was introduced in Java 1.2) and thus cannot be added to the CDS archive. Note this also applies to any subclasses, even if they themselves use a newer classfile format version.\n It\u0026#8217;s thus a good idea to check whether you can upgrade to newer versions of your dependencies, as this may result in more classes becoming available for CDS, resulting in better start-up times in turn.\n   Using the CDS Archive Now let\u0026#8217;s run the application again, this time using the previously created CDS archive:\n java -XX:SharedArchiveFile=target/app-cds.jsa \\ (1) -Xlog:class+load:file=target/classload.log \\ (2) -Xshare:on \\ (3) -jar target/todo-manager-1.0.0-SNAPSHOT-runner.jar     1 The path to the CDS archive   2 classloading logging allows to verify whether the CDS archive gets applied as expected   3 While class data sharing is enabled by default on JDK 12 and newer, explicitely enforcing it will ensure an error is raised if something is wrong, e.g. a mismatch of Java versions between building and using the archive    When examining the classload.log file, you should see how most class metadata is obtained from the CDS archive (\"source: shared object file\"), while some classes such as the ancient Antlr classes are loaded just as usual from the corresponding JAR:\n [0.016s][info][class,load] java.lang.Object source: shared objects file [0.016s][info][class,load] java.io.Serializable source: shared objects file [0.016s][info][class,load] java.lang.Comparable source: shared objects file [0.016s][info][class,load] java.lang.CharSequence source: shared objects file ... [2.555s][info][class,load] antlr.Parser source: file:/.../antlr.antlr-2.7.7.jar ...   Note it is vital that the exact same Java version is used as when creating the archive, otherwise an error will be raised. Unfortunately, this also means that AppCDS archives cannot be built cross-platform. This would be very useful, e.g. when building a Java application on macOS or Windows, which should be packaged in a Linux container. If you are aware of a way for doing so, please let me know in the comments below.\n     CDS and the Java Module System Beginning with Java 11, not only classes from the classpath can be added to CDS archives, but also classes from the module path of a modularized Java application. One important detail to consider there is that the --upgrade-module-path and --patch-module options will cause CDS to be disabled or disallowed (with -Xshare:on) is specified. This is to avoid a mismatch of class metadata in the CDS archive and classes brought in by a newer module version.\n       Creating CDS Archives in Your Maven Build Manually creating a CDS archive is not very efficient nor reliable, so let\u0026#8217;s see how the task can be automated as part of your project build. The following shows the required configuration when using Apache Maven, but of course the same approach could be implemented with Gradle or any other build system.\n The basic idea is the follow the same steps as before, but executed as part of the Maven build:\n  start up the application with the -XX:ArchiveClassesAtExit option\n  invoke some application functionality to initiate the loading of all relevant classes\n  stop the application\n       It might appear as a compelling idea to produce the CDS archive as part of regular test execution, e.g. via JUnit. This will not work though, as the classpath at the time of using the CDS archive must be not miss any entries from the classpath at the time of creating it. As during test execution all the test-scoped dependencies will be part of the classpath, any CDS archive created that way couldn\u0026#8217;t be used when running the application later on without those test dependencies.\n     Steps 1. and 3. can be automated with help of the Process-Exec Maven plug-in, binding it to the pre-integration-test and post-integration-test build phases, respectively. While I was thinking of using the more widely known Exec plug-in initially, this turned out to not be viable as there\u0026#8217;s no way for stopping any forked process in a later build phase.\n Here\u0026#8217;s the relevant configuration:\n ... \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;com.bazaarvoice.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;process-exec-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.9\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; (1) \u0026lt;id\u0026gt;app-cds-creation\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;pre-integration-test\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;start\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;name\u0026gt;todo-manager\u0026lt;/name\u0026gt; \u0026lt;healthcheckUrl\u0026gt;http://localhost:8080/\u0026lt;/healthcheckUrl\u0026gt; (2) \u0026lt;arguments\u0026gt; \u0026lt;argument\u0026gt;java\u0026lt;/argument\u0026gt; (3) \u0026lt;argument\u0026gt;-XX:ArchiveClassesAtExit=app-cds.jsa\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;-jar\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt; ${project.build.directory}/${project.artifactId}-${project.version}-runner.jar \u0026lt;/argument\u0026gt; \u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;execution\u0026gt; (4) \u0026lt;id\u0026gt;stop-all\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;post-integration-test\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;stop-all\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; ...     1 Start up the application in the pre-integration-test build phase   2 The health-check URL is used to await application start-up before proceeding with the next build phase   3 Assemble the java invocation   4 Stop the application in the post-integration-test build phase    What remains to be done is the automation of step 2, the invocation of the required application logic so to trigger the loading of all relevant classes. This can be done with help of the Maven Surefire plug-in. A simple \"integration test\" via REST Assured does the trick:\n public class ExampleResourceAppCds { @Test public void getAll() { given() .when() .get(\"/api\") .then() .statusCode(200); } }   We just need to configure a specific execution of the plug-in, which only picks up any test classes whose names end with *AppCds.java, so to keep them apart from actual integration tests:\n ... \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-failsafe-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0-M4\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;integration-test\u0026lt;/goal\u0026gt; \u0026lt;goal\u0026gt;verify\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;includes\u0026gt; \u0026lt;include\u0026gt;**/*AppCds.java\u0026lt;/include\u0026gt; \u0026lt;/includes\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; ...   And that\u0026#8217;s all we need; when now building the project via mvn clean verify, a CDS archive will be created at target/app-cds.jsa. You can find the complete example project and steps for building/running it on GitHub.\n   What Do You Gain? Creating a CDS archive is nice, but is it also worth the effort? In order to answer this question, I\u0026#8217;ve done some measurements of the \"time-to-first-response\" metric, following the Quarkus guide on measuring performance. I.e. instead of awaiting some rather meaningless \"start-up complete\" status, which could arbitrarily be tweaked by means of lazy initialization, this measures the time until the application is actually ready to handle the first incoming request after start-up.\n I\u0026#8217;ve done measurements on OpenJDK 1.8.0_252 (AdoptOpenJDK build), OpenJDK 14.0.1 (upstream build, without and with AppCDS), and OpenJDK 15-ea-b26 (upstream build, with AppCDS). Please see the README file of the example repo for the exact steps.\n Here are the numbers, averaged over ten runs each:\n   Update, June 12th: I had originally classload logging enabled for the OpenJDK 14 AppCDS runs, which added an unneccessary overhead (thanks a lot to Claes Redestad for pointing this out!). The numbers and chart have been updated accordingly. I\u0026#8217;ve also added numbers for OpenJDK 15-ea.\n Time-to-first-response values are 2s 267ms, 2s 162ms, 1s 669ms 1s 483ms, and 1s 279ms. I.e. on my machine (2014 MacBook Pro), with this specific workload, there\u0026#8217;s an improvement of ~100ms just by upgrading to the current JDK, and of another ~500ms ~700ms by using AppCDS.\n With OpenJDK 15 things will further improve. The latest EA build at the time of writing (b26) shortens time-to-first-response by another ~200ms. The upcoming EA build 27 should bring another improvement, as Lambda proxy classes will be added to AppCDS archives then.\n That all is definitely a nice improvement, in particular as we get it essentially for free, without any changes to the actual application itself. You should contrast this with the additional size of the application distribution, though. E.g. when obtaining the application as a container image from a remote container registry, downloading the additional ~40 MB might take longer than the time saved during application start-up. Typically, this will only affect the first start-up of on a particular node, though, after which the image will be cached locally.\n As always when it comes to any kinds of performance numbers, please take these numbers with a grain of salt, do your own measurements, using your own applications and in your own environment.\n     Addressing Different Workload Profiles If your application supports different \"work modes\", e.g. \"online\" and \"batch\", which work with a largely differing set of classes, you also might consider to create different CDS archives for the specific workloads. This might give you a good balance between additional size and realized improvements of start-up times, when for instance dealing with at large monolithic application instead of more fine-grained microservices.\n       Wrap-Up AppCDS provides Java developers with a useful tool for reducing start-up times of their applications, without requiring any code changes. For the example discussed, we could observe an improvement of the time-to-first-response metric by about 30% when running with OpenJDK 14. Other users reported even bigger improvements.\n We didn\u0026#8217;t discuss any potential memory improvements due to CDS when sharing class metadata between multiple JVMs on one host. In containerized server applications, with each JVM being packaged in its own container image, this won\u0026#8217;t play a role. It could make a difference on desktop systems, though. For instance multiple instances of the Java language server, as leveraged by VSCode and other editors, could benefit from that.\n That all being said, when raw start-up time is your primary concern, e.g. in a serverless or Function-based setting, you should look at AOT compilation with GraalVM (or Project Leyden in the future). This will bring down start-up times to a completely different level; for example the todo manager application would return a first response within a few 10s of milliseconds when executed as a native image via GraalVM.\n But AOT is not always an option, nor does it always make sense: the JVM may offer a better latency than native binaries, external dependencies migh not be ready for usage in AOT-compiled native images yet, or you simply might want to be able to benefit from all the JVM goodness, like familiar debugging tools, the JDK Flight Recorder, or JMX. In that case, CDS can give you a nice start-up time improvement, solely by means of adding a few steps to your build process.\n Besides class data sharing in OpenJDK, there are some other related techniques for improving start-up times which are worth exploring:\n   Eclipse OpenJ9 has its own implementation of class data sharing\n  Alibaba\u0026#8217;s Dragonwell distribution of the OpenJDK comes with JWarmUp, a tool for speeding up initial JIT compilations\n   To learn more about AppCDS, a long yet insightful post is this one by Vladimir Plizga. Volker Simonis did another interesting write-up. Also take a look at the CDS documentation in the reference docs of the java command.\n Lastly, the Quarkus team is working on out-of-the-box support for CDS archives. This could fully automate the creation of an archive for all required classes without any further configuration, making it even easier to benefit from the start-up time improvements promised by CDS.\n  ","id":16,"publicationdate":"Jun 11, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eAhead-of-time compilation (AOT) is \u003cem\u003ethe\u003c/em\u003e big topic in the Java ecosystem lately:\nby compiling Java code to native binaries, developers and users benefit from vastly improved start-up times and reduced memory usage.\nThe \u003ca href=\"https://www.graalvm.org/\"\u003eGraalVM\u003c/a\u003e project made huge progress towards AOT-compiled Java applications,\nand \u003ca href=\"https://mail.openjdk.java.net/pipermail/discuss/2020-April/005429.html\"\u003eProject Leyden\u003c/a\u003e promises to standardize AOT in a future version of the Java platform.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThis makes it easy to miss out on significant performance improvements which have been made on the JVM in recent Java versions,\nin particular when it comes to \u003ca href=\"https://cl4es.github.io/2019/11/20/OpenJDK-Startup-Update.html\"\u003efaster start-up times\u003c/a\u003e.\nBesides a range of improvements related to class loading, linking and bytecode verification,\nsubstantial work has been done around \u003ca href=\"https://docs.oracle.com/en/java/javase/14/vm/class-data-sharing.html\"\u003eclass data sharing\u003c/a\u003e (CDS).\nFaster start-ups are beneficial in many ways:\nshorter turnaround times during development,\nquicker time-to-first-response for users in coldstart scenarios,\ncost savings when billed by CPU time in the cloud.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eWith CDS, class metadata is persisted in an archive file,\nwhich during subsequent application starts is mapped into memory.\nThis is faster than loading the actual class files, resulting in reduced start-up times.\nWhen starting multiple JVM processes on the same host, read-only archives of class metadata can also be shared between the VMs, so that less memory is consumed overall.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Building Class Data Sharing Archives with Apache Maven","uri":"https://www.morling.dev/blog/building-class-data-sharing-archives-with-apache-maven/"},{"content":"Do you remember Angus \"Mac\" MacGyver? The always creative protagonist of the popular 80ies/90ies TV show, who could solve about any problem with nothing more than a Swiss Army knife, duct tape, shoe strings and a paper clip?\n The single message transformations (SMTs) of Kafka Connect are almost as versatile as MacGyver\u0026#8217;s Swiss Army knife:\n   How to change the timezone or format of date/time message fields?\n  How to change the topic a specific message gets sent to?\n  How to filter out specific records?\n   SMTs can be the answer to these and many other questions that come up in the context of Kafka Connect. Applied to source or sink connectors, SMTs allow to modify Kafka records before they are sent to Kafka, or after they are consumed from a topic, respectively.\n In this post I\u0026#8217;d like to focus on some interesting (hopefully anyways) usages of SMTs. Those use cases are mostly based on my experiences from using Kafka Connect with Debezium, an open-source platform for change data capture (CDC). I also got some great pointers on interesting SMT usages when asking the community about this on Twitter some time ago:\n    I definitely recommend to check out the thread; thanks a lot to all who replied! In order to learn more about SMTs in general, how to configure them etc., refer to the resources given towards the end of this post.\n For each category of use cases, I\u0026#8217;ve also asked our sympathetic TV hero for his opinion on the usefulness of SMTs for the task at hand. You can find his rating at the end of each section, ranging from 📎 (poor fit) to 📎📎📎📎📎 (perfect fit).\n Format Conversions Probably the most common application of SMTs is format conversion, i.e. adjustments to type, format and representation of data. This may apply to entire messages, or to specific message attributes. Let\u0026#8217;s first look at a few examples for converting individual message attribute formats:\n   Timestamps: Different systems tend to have different assumptions of how timestamps should be typed and formatted. Debezium for instance represents most temporal column types as milli-seconds since epoch. Change event consumers on the other hand might expect such date and time values using Kafka Connect\u0026#8217;s Date type, or as an ISO-8601 formatted string, potentially using a specific timezone\n  Value masking: Sensitive data might have be to masked or truncated, or specific fields should even be removed altogether; the org.apache.kafka.connect.transforms.MaskField and ReplaceField SMTs shipping with Kafka Connect out of the box come in handy for that\n  Numeric types: Similar to timestamps, requirements around the representation of (decimal) numbers may differ between systems; e.g. Kafka Connect\u0026#8217;s Decimal type allows to convey arbitrary-precision decimals, but its binary representation of numbers might not be supported by all sink connectors and consumers\n  Name adjustments: Depending on the chosen serialization formats, specific field names might be unsupported; when working with Apache Avro for instance, field names must not start with a number\n   In all these cases, either existing, ready-made SMTs or bespoke implementations can be used to apply the required attribute type and/or format conversions.\n When using Kafka Connect for integrating legacy services and databases with newly built microservices, such format conversions can play an important role for creating an anti-corruption layer: by using better field names, choosing more suitable data types or by removing unneeded fields, SMTs can help to shield a new service\u0026#8217;s model from the oddities and quirks of the legacy world.\n But SMTs cannot only modify the representation of single fields, also the format and structure of entire messages can be adjusted. E.g. Kafka Connect\u0026#8217;s ExtractField transformation allows to extract a single field from a message and propagate that one. A related SMT is Debezium\u0026#8217;s SMT for change event flattening. It can be used to convert the complex Debezium change event structure with old and new row state, metadata and more, into a flat row representation, which can be consumed by many existing sink connectors.\n SMTs also allow to fine-tune schema namespaces; that can be of interest when working with a schema registry for managing schemas and their versions, and specific schema namespaces should be enforced for the messages on given topics. Two more, very useful examples of SMTs in this category are kafka-connect-transform-xml and kafka-connect-json-schema by Jeremy Custenborder, which will take XML or text and produce a typed Kafka Connect Struct, based on a given XML schema or JSON schema, respectively.\n Lastly, as a special kind of format conversion, SMTs can be used to modify or set the key of Kafka records. This may be desirable if a source connector doesn\u0026#8217;t produce any meaningful key, but one can be extracted from the record value. Also changing the message key can be useful, when considering subsequent stream processing. Choosing matching keys right at the source side e.g. allows for joining multiple topics via Kafka Streams, without the need for re-keying records.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎📎\u0026nbsp;\u0026nbsp; SMTs are the perfect tool for format conversions of Kafka Connect records\n   Ensuring Backwards Compatibility Changes to the schema of Kafka records can potentially be disruptive for consumers. If for instance a record field gets renamed, a consumer must be adapted accordingly, reading the value using the new field name. In case a field gets dropped altogether, consumers must not expect this field any longer.\n Message transformations can help with such transition from one schema version to the next, thus reducing the coupling of the lifecycles of message producers and consumers. In case of a renamed field, an SMT could add the field another time, using the original name. That\u0026#8217;ll allow consumers to continue reading the field using the old name and to be upgraded to use the new name at their own pace. After some time, once all consumers have been adjusted, the SMT can be removed again, only exposing the new field name going forward. Similarly, a field that got removed from a message schema could be re-added, e.g. using some sort of constant placeholder value. In other cases it might be possible to derive the field value from other, still existing fields. Again consumers could then be updated at their own pace to not expect and access that field any longer.\n It should be said though that there are limits for this usage: e.g. when changing the type of a field, things quickly become tricky. One option could be a multi-step approach where at first a separate field with the new type is added, before renaming it again as described above.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎\u0026nbsp;\u0026nbsp; SMTs can primarily help to address basic compatibility concerns around schema evolution\n   Filtering and Routing When applied on the source side, SMTs allow to filter out specific records produced by the connector. They also can be used for controlling the Kafka topic a record gets sent to. That\u0026#8217;s in particular interesting when filtering and routing is based on the actual record contents. In an IoT scenario for instance where Kafka Connect is used to ingest data from some kind of sensors, an SMT might be used to filter out all sensor measurements below a certain threshold, or route measurement events above a threshold to a special topic.\n Debezium provides a range of SMTs for record filtering and routing:\n   The logical topic routing SMT allows to send change events originating from multiple tables to the same Kafka topic, which can be useful when working with partition tables in Postgres, or with data that is sharded into multiple tables\n  The Filter and ContentBasedRouter SMTs let you use script expressions in languages such as Groovy or JavaScript for filtering and routing change events based on their contents; such script-based approach can be an interesting middleground between ease-of-use (no Java code must be compiled and deployed to Kafka Connect) and expressiveness; e.g. here is how the routing SMT could be used with GraalVM\u0026#8217;s JavaScript engine for routing change events from a table with purchase orders to different topics in Kafka, based on the order type:\n... transforms=route transforms.route.type=io.debezium.transforms.ContentBasedRouter transforms.route.topic.regex=.*purchaseorders transforms.route.language=jsr223.graal.js transforms.route.topic.expression= value.after.ordertype == 'B2B' ? 'b2b_orders' : 'b2c_orders' ...     The outbox event router comes in handy when implementing the transactional outbox pattern for data propagation between microservices: it can be used to send events originating from a single outbox table to a specific Kafka topic per aggregate (when thinking of domain driven design) or event type\n   There are also two SMTs for routing purposes in Kafka Connect itself: RegexRouter which allows to re-route records two different topics based on regular expressions, and TimestampRouter for determining topic names based on the record\u0026#8217;s timestamp.\n While routing SMTs usually are applied to source connectors (defining the Kafka topic a record gets sent to), it can also make sense to use them with sink connectors. That\u0026#8217;s the case when a sink connector derives the name of downstream table names, index names or similar from the topic name.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎📎\u0026nbsp;\u0026nbsp; Message filtering and topic routing\u0026#8201;\u0026#8212;\u0026#8201;no problem for SMTs\n   Tombstone Handling Tombstone records are Kafka records with a null value. They carry special semantics when working with compacted topics: during log compaction, all records with the same key as a tombstone record will be removed from the topic.\n Tombstones will be retained on a topic for a configurable time before compaction happens (controlled via delete.retention.ms topic setting), which means that also Kafka Connect sink connectors need to handle them. Unfortunately though, not all connectors are prepared for records with a null value, typically resulting in NullPointerExceptions and similar. A filtering SMT such as the one above can be used to drop tombstone records in such case.\n But also the exact opposite\u0026#8201;\u0026#8212;\u0026#8201;producing tombstone records\u0026#8201;\u0026#8212;\u0026#8201;can be useful: some sink connectors use tombstone records as the indicator to delete corresponding rows from a downstream datastore. Now when using a CDC connector like Debezium to capture changes from a database where \"soft deletes\" are used (i.e. records are not physically deleted, but a logically deleted flag is set to true when deleting a record), those change events will be exported as update events (which they technically are). A bespoke SMT can be used to translate these update events into tombstone records, triggering the deletion of corresponding records in downstream datastores.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎\u0026nbsp;\u0026nbsp; SMTs work well to discard tombstones or convert soft delete events into tombstones. What\u0026#8217;s not possible though is to keep the original event and produce an additional tombstone record at the same time\n   Externalizing Large Payloads Even some advanced enterprise application patterns can be implemented with the help of SMTs, one example being the claim check pattern. This pattern comes in handy in situations like this:\n  A message may contain a set of data items that may be needed later in the message flow, but that are not necessary for all intermediate processing steps. We may not want to carry all this information through each processing step because it may cause performance degradation and makes debugging harder because we carry so much extra data.\n \u0026#8201;\u0026#8212;\u0026#8201;Gregor Hohpe, Bobby Woolf; Enterprise Application Patterns\n   A specific example could again be a CDC connector that captures changes from a database table Users, with a BLOB column that contains the user\u0026#8217;s profile picture (surely not a best practice, still not that uncommon in reality\u0026#8230;\u0026#8203;).\n     Apache Kafka and Large Messages Apache Kafka isn\u0026#8217;t meant for large messages. The maximum message size is 1 MB by default, and while this can be increased, benchmarks are showing best throughput for much smaller messages. Strategies like chunking and externalizing large payloads can thus be vital in order to ensure a satisfying performance.\n     When propagating change data events from that table to Apache Kafka, adding the picture data to each event poses a significant overhead. In particular, if the picture BLOB hasn\u0026#8217;t changed between two events at all.\n Using an SMT, the BLOB data could be externalized to some other storage. On the source side, the SMT could extract the image data from the original record and e.g. write it to a network file system or an Amazon S3 bucket. The corresponding field in the record would be updated so it just contains the unique address of the externalised payload, such as the S3 bucket name and file path:\n   As an optimization, it could be avoided to re-upload unchanged file contents another time by comparing earlier and current hash of the externalized file.\n A corresponding SMT instance applied to sink connectors would retrieve the identifier of the externalized files from the incoming record, obtain the contents from the external storage and put it back into the record before passing it on to the connector.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎\u0026nbsp;\u0026nbsp; SMTs can help to externalize payloads, avoiding large Kafka records. Relying on another service increases overall complexity, though\n   Limitations As we\u0026#8217;ve seen, single message transformations can help to address quite a few requirements that commonly come up for users of Kafka Connect. But there are limitations, too; Like MacGyver, who sometimes has to reach for some other tool than his beloved Swiss Army knife, you shouldn\u0026#8217;t think of SMTs as the perfect solution all the time.\n The biggest shortcoming is already hinted at in their name: SMTs only can be used to process single records, one at a time. E.g. you cannot split up a record into multiple ones using an SMT, as they only can return (at most) one record. Also any kind of stateful processing, like aggregating data from multiple records, or correlating records from several topics is off limits for SMTs. For such use cases, you should be looking at stream processing technologies like Kafka Streams and Apache Flink; also integration technologies like Apache Camel can be of great use here.\n One thing to be aware of when working with SMTs is configuration complexity; when using generic, highly configurable SMTs, you might end up with lengthy configuration that\u0026#8217;s hard to grasp and debug. You might be better off implementing a bespoke SMT which is focussing on one particular task, leveraging the full capabilities of the Java programming language.\n     SMT Testing Whether you use ready-made SMTs by means of configuration, or you implement custom SMTs in Java, testing your work is essential.\n While unit tests are a viable option for basic testing of bespoke SMT implementations, integration tests running against Kafka Connect connectors are recommended for testing SMT configurations. That way you\u0026#8217;ll be sure that the SMT can process actual messages and it has been configured the way you intended to.\n Testcontainers and the Debezium support for Testcontainers are a great foundation for setting up all the required components such as Apache Kafka, Kafka Connect, connectors and the SMTs to test.\n     A specific feature I wished for every now and then is the ability to apply SMTs only to a specific sub-set of the topics created or consumed by a connector. In particular if connectors create different kinds of topics (like an actual data topic and another one with with metadata), it can be desirable to apply SMTs only to the topics of one group but not the other. This requirement is captured in KIP-585 (\"Filter and Conditional SMTs\"), please join the discussion on that one if you got requirements or feedback related to that.\n   Learning More There are several great presentations and blog posts out there which describe in depth what SMTs are, how you can implement your own one, how they are configured etc.\n Here are a few resources I found particularly helpful:\n   KIP-66: The original KIP (Kafka Improvement Proposal) that introduced SMTs\n  Singe Message Transforms are not the Transformations You\u0026#8217;re Looking For: A great overview on SMTs, their capabilities as well as limitations, by Ewen Cheslack-Postava\n  A hands-on experience with Kafka Connect SMTs: In-depth blog post on SMT use cases, things to be aware of and more, by Gian D\u0026#8217;Uia\n   Now, considering this wide range of use cases for SMTs, would MacGyver like and use them for implementing various tasks around Kafka Connect? I would certainly think so. But as always, the right tool for the job must be chosen: sometimes an SMT may be a great fit, another time a more flexible (and complex) stream processing solution might be preferable.\n Just as MacGyver, you got to make a call when to use your Swiss Army knife, duct tape or a paper clip.\n Many thanks to Hans-Peter Grahsl for his feedback while writing this blog post!\n  ","id":17,"publicationdate":"May 14, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eDo you remember Angus \"Mac\" MacGyver?\nThe always creative protagonist of the popular 80ies/90ies TV show, who could solve about any problem with nothing more than a Swiss Army knife, duct tape, shoe strings and a paper clip?\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe single message transformations (SMTs) of Kafka Connect are almost as versatile as MacGyver\u0026#8217;s Swiss Army knife:\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"ulist\"\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eHow to change the timezone or format of date/time message fields?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow to change the topic a specific message gets sent to?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow to filter out specific records?\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eSMTs can be the answer to these and many other questions that come up in the context of Kafka Connect.\nApplied to source or sink connectors,\nSMTs allow to modify Kafka records before they are sent to Kafka, or after they are consumed from a topic, respectively.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Single Message Transformations - The Swiss Army Knife of Kafka Connect","uri":"https://www.morling.dev/blog/single-message-transforms-swiss-army-knife-of-kafka-connect/"},{"content":"For libraries and frameworks it\u0026#8217;s a common requirement to make specific aspects customizeable via service provider interfaces (SPIs): contracts to be implemented by the application developer, which then are invoked by framework code, adding new or replacing existing functionality.\n Often times, the method implementations of such an SPI need to return value(s) to the framework. An alternative to return values are \"emitter parameters\": passed by the framework to the SPI method, they offer an API for receiving value(s) via method calls. Certainly not revolutionary or even a new idea, I find myself using emitter parameters more and more in libraries and frameworks I work on. Hence I\u0026#8217;d like to discuss some advantages I perceive about the emitter parameter pattern.\n An Example As an example, let\u0026#8217;s consider a blogging platform which provides an SPI for extracting categories and tags from given blog posts. Application developers can plug in custom implementations of that SPI, e.g. based on the latest and greatest algorithms in information retrieval and machine learning. Here\u0026#8217;s how a basic SPI contract for this use case could look like, using regular method return values:\n 1 2 3 4 5 public interface BlogPostDataExtractor { Set\u0026lt;String\u0026gt; extractCategories(String contents); Set\u0026lt;String\u0026gt; extractTags(String contents); }    This probably would get the job done, but there are a few problems: any implementation will have to do two passes on the given blog post contents, once in each method\u0026#8201;\u0026#8212;\u0026#8201;not ideal. Also let\u0026#8217;s assume that most blog posts only belong to exactly one category. Implementations still would have to allocate a set for the single returned category.\n While there\u0026#8217;s not much we can do about the second issue with a return value based design, the former problem could be addressed by combining the two methods:\n 1 2 3 4 public interface BlogPostDataExtractor { CategoriesAndTags extractCategoriesAndTags(String contents); }    Now an implementation can retrieve both categories and tags at once. But it\u0026#8217;s worth thinking about how an SPI implementation would instantiate the return type.\n Exposing a concrete class to be instantiated by implementors poses a challenge for future evolution of the SPI: following the best practice and making the return object type immutable, all its properties must be passed to its constructor. Now if an additional attribute should be extracted from blog posts, such as a teaser, the existing constructor cannot be modified, so to not break existing user code. Instead, we\u0026#8217;d have to introduce new constructors whenever adding further attributes. Dealing with all these constructors could become quite inconvenient, in particular if a specific SPI implementation is only interested in producing some of the attributes.\n All in all, for SPIs it\u0026#8217;s often a good idea to only expose interfaces, but no concrete classes. So we could make the return type an interface and leave it to SPI implementors to create an implementation class, but that\u0026#8217;d be rather tedious.\n   The Emitter Parameter Pattern Or, we could provide some sort of builder object which can be used to construct CategoriesAndTags objects. But then why even return an object at all, instead of simply mutating the state of a builder that is provided through a method parameter? And that\u0026#8217;s essentially what the emitter parameter pattern is about: passing in an object which can be used to emit the values which should be \"returned\" by the method.\n     I\u0026#8217;m not aware of any specific name for this pattern, so I came up with \"emitter parameter pattern\" (the notion of callback parameters is related, yet different). And hey, perhaps I\u0026#8217;ll become famous for coining a design pattern name ;) Please let me know in the comments below if you know this pattern under a different name.\n     Here\u0026#8217;s how the extractor SPI could look like when designed with an emitter parameter:\n 1 2 3 4 5 6 7 8 9 10 public interface BlogPostDataExtractor { void extractData(String contents, BlogPostDataReceiver data); (1) interface BlogPostDataReceiver { (2) void addCategory(String category); void addTag(String tag); } }      1 SPI method with input parameter and emitter parameter   2 Emitter parameter type    An implementation would emit the retrieved information by invoking the methods on the data parameter:\n 1 2 3 4 5 6 7 8 9 10 public class MyBlogPostDataExtractor implements BlogPostDataExtractor { public void extractData(String contents, BlogPostDataReceiver data) { String category = ...; Stream\u0026lt;String\u0026gt; tags = ...; data.addCatgory(category); tags.forEach(data::addTag); } }    This approach nicely avoids all the issues with the return value based design:\n   Single and multiple value case handled uniformly: an implementation can call addCategory() just once, or multiple times; either way, it doesn\u0026#8217;t have to deal with the creation of a set, list, or other container for the produced value(s)\n  Flexible evolution of the SPI contract: new methods such as addTeaser(), or addTags(String\u0026#8230;\u0026#8203; tags) can be added to the emitter parameter type, avoiding the creation of more and more return type constructors; as the passed BlogPostDataReceiver instance is controlled by the framework itself, we also could add methods which provide more context required for the task at hand\n  No need for exposing concrete types on the SPI surface: as no return value needs to be instantiated by SPI implementations, the solution works solely with interfaces on the SPI surface; this provides more control to the framework, e.g. the emitter object could be re-used etc.\n  Flexible implementation choices: by not requiring SPI implementations to allocate any return objects, the platform gains a lot of flexibility for how it\u0026#8217;s processing the emitted values: while it could collect the values in a set or list, it also has the option to not allocate any intermediary collections, but process and pass on values one-by-one in a streaming-based way, without any of this impacting SPI implementors\n   Now, are there some downsides to this approach, too? I can see two: if a method only ever should yield a single value, the emitter API might be misleading. We could raise an exception though if an emitter method is called more than once. Also an implementation might hold on to the emitter object and invoke its methods after the call flow has returned from the SPI method, which typically isn\u0026#8217;t desirable. Again that\u0026#8217;s something that can be prevented by invalidating the emitter object after the SPI method returned, raising an exception in case of further method invocations.\n Overall, I think the emitter parameter pattern is a valuable tool in the box of library and framework authors; it provides flexibility for implementation choices and future evolution when designing SPIs. Real-world examples include the ValueExtractor SPI in Bean Validation 2.0 (where it was chosen to provide a uniform value of extracting single and multiple values from container objects) and the ChangeRecordEmitter contract in Debezium\u0026#8217;s SPI.\n Many thanks to Hans-Peter Grahsl and Nils Hartmann for reviewing an early version of this blog post.\n  ","id":18,"publicationdate":"May 4, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eFor libraries and frameworks it\u0026#8217;s a common requirement to make specific aspects customizeable via \u003ca href=\"https://en.wikipedia.org/wiki/Service_provider_interface\"\u003eservice provider interfaces\u003c/a\u003e (SPIs):\ncontracts to be implemented by the application developer, which then are invoked by framework code,\nadding new or replacing existing functionality.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eOften times, the method implementations of such an SPI need to return value(s) to the framework.\nAn alternative to return values are \"emitter parameters\":\npassed by the framework to the SPI method, they offer an \u003cem\u003eAPI\u003c/em\u003e for receiving value(s) via method calls.\nCertainly not revolutionary or even a new idea,\nI find myself using emitter parameters more and more in libraries and frameworks I work on.\nHence I\u0026#8217;d like to discuss some advantages I perceive about the emitter parameter pattern.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"The Emitter Parameter Pattern for Flexible SPI Contracts","uri":"https://www.morling.dev/blog/emitter-parameter-pattern-for-flexible-spis/"},{"content":"Making applications extensible with some form of plug-ins is a very common pattern in software design: based on well-defined APIs provided by the application core, plug-ins can customize an application\u0026#8217;s behavior and provide new functionality. Examples include desktop applications like IDEs or web browsers, build tools such as Apache Maven or Gradle, as well as server-side applications such as Apache Kafka Connect, a runtime for Kafka connectors plug-ins.\n In this post I\u0026#8217;m going to explore how the Java Platform Module System's notion of module layers can be leveraged for implementing plug-in architectures on the JVM. We\u0026#8217;ll also discuss how Layrry, a launcher and runtime for layered Java applications, can help with this task.\n A key requirement for any plug-in architecture is strong isolation between different plug-ins: their state, classes and dependencies should be encapsulated and independent of each other. E.g. package declarations in two plug-ins should not collide, also they should be able to use different versions of another 3rd party dependency. This is why the default module path of Java (specified using the --module-path option) is not enough for this purpose: it doesn\u0026#8217;t support more than one version of a given module.\n The module system\u0026#8217;s answer are module layers: by organizing an application and its plug-ins into multiple layers, the required isolation between plug-ins can be achieved.\n     With the module system, each Java application always contains at least one layer, the boot layer. It contains the platform modules and the modules provided on the module path.\n     An Example: The Greeter CLI App To make things more tangible, let\u0026#8217;s consider a specific example; The \"Greeter\" app is a little CLI utility, that can produce greetings in different languages.\n In order to not limit the number of supported languages, it provides a plug-in API, which allows to add additional greeting implementations, without the need to rebuild the core application. Here is the Greeter contract, which is to be implemented by each language plug-in:\n 1 2 3 4 5 package com.example.greeter.api; public interface Greeter { String greet(String name); }    Greeters are instantiated via accompanying implementations of GreeterFactory:\n 1 2 3 4 5 public interface GreeterFactory { String getLanguage(); (1) String getFlag(); Greeter getGreeter(); (2) }      1 The getLanguage() and getFlag() methods are used to show a description of all available greeters in the CLI application   2 The getGreeter() method returns a new instance of the corresponding Greeter type    Here\u0026#8217;s the overall architecture of the Greeter application, with three different language implementations:\n   The application is made up of five different layers:\n   greeter-platform: contains the Greeter and GreeterFactory contracts\n  greeter-en, greeter-de and greeter-fr: greeter implementations for different languages; note how each one is depending on a different version of some greeter-date module. As they are isolated in different layers, they can co-exist within the application\n  greeter-app: the \"shell\" of the application which loads all the greeter implementations and makes them accessible as a simple CLI application\n   Now let\u0026#8217;s see how this application structure can be assembled using Layrry.\n   Application Plug-ins With Layrry In a previous blog post we\u0026#8217;ve explored how applications can be cut into layers, described in Layrry\u0026#8217;s layers.yml configuration file. A simple static layer definition would defeat the purpose of a plug-in architecture, though: not all possible plug-ins are known when assembling the application.\n Layrry addresses this requirement by allowing to source different layers from directories on the file system:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 layers: platform: (1) modules: - \"com.example.greeter:greeter-api:1.0.0\" plugins: (2) parents: - \"api\" directory: path/to/plugins app: (3) parents: - \"plugins\" modules: - \"com.example.greeter:greeter-app:1.0.0\" main: module: com.example.greeter.app class: com.example.greeter.app.App      1 The platform layer with the API module   2 The plug-in layer(s)   3 The application layer with the \"application shell\"    Whereas the platform and app layers are statically defined, using the Maven GAV coordinates of the modules to include, the plugins part of the configuration describes an open-ended set of layers. Each sub-directory of the given directory represents its own layer. All modules within this sub-directory will be added to the layer, and the API layer will be the parent of each of the plug-in layers. The app layer has all the plug-in layers as its ancestors, allowing it to retrieve plug-in implementations from these layers.\n More greeter plug-ins can be added to the application by simply creating a sub-directory with the required module(s).\n   Finding Plug-in Implementations With the Java Service Loader Structuring the application into different layers isn\u0026#8217;t all we need for building a plug-in architecture; we also need a way for detecting and loading the actual plug-in implementations. The service loader mechanism of the Java platform comes in handy for that. If you have never worked with the service loader API, it\u0026#8217;s definitely recommended to study its extensive JavaDoc description:\n  A service is a well-known interface or class for which zero, one, or many service providers exist. A service provider (or just provider) is a class that implements or subclasses the well-known interface or class. A ServiceLoader is an object that locates and loads service providers deployed in the run time environment at a time of an application\u0026#8217;s choosing.   Having been a supported feature of Java since version 6, the service loader API has been been reworked and refined to work within modular environments when the Java Module System was introduced in JDK 9.\n In order to retrieve service implementations via the service loader, a consuming module must declare the use of the service in its module descriptor. For our purposes, the GreeterFactory contract is a perfect examplification of the service idea. Here\u0026#8217;s the descriptor of the Greeter application\u0026#8217;s app module, declaring its usage of this service:\n 1 2 3 4 5 module com.example.greeter.app { exports com.example.greeter.app; requires com.example.greeter.api; uses com.example.greeter.api.GreeterFactory; }    The module descriptor of each greeter plug-in must declare the service implementation(s) which it provides. E.g. here is the module descriptor of the English greeter implementation:\n 1 2 3 4 5 6 module com.example.greeter.en { requires com.example.greeter.api; requires com.example.greeter.dateutil; provides com.example.greeter.api.GreeterFactory with com.example.greeter.en.EnglishGreeterFactory; }    From within the app module, the service implementations can be retrieved via the java.util.ServiceLoader class.\n When using the service loader in layered applications, there\u0026#8217;s one potential pitfall though, which mostly will affect existing applications which are migrated: in order to access service implementations located in a different layer (specifically, in an ancestor layer of the loading layer), the method load(ModuleLayer, Class\u0026lt;?\u0026gt;) must be used. When using other overloaded variants of load(), e.g. the commonly used load(Class\u0026lt;?\u0026gt;), those implementations won\u0026#8217;t be found.\n Hence the code for loading the greeter implementations from within the app layer could look like this:\n 1 2 3 4 5 6 7 8 9 10 private static List\u0026lt;GreeterFactory\u0026gt; getGreeterFactories() { ModuleLayer appLayer = App.class.getModule().getLayer(); return ServiceLoader.load(appLayer, GreeterFactory.class) .stream() .map(p -\u0026gt; p.get()) .sorted((gf1, gf2) -\u0026gt; gf1.getLanguage().compareTo( gf2.getLanguage())) .collect(Collectors.toList()); }    Having loaded the list of greeter factories, it doesn\u0026#8217;t take too much code to display a list with all available implementations, expect a choice by the user and invoke the greeter for the chosen language. This code which isn\u0026#8217;t too interesting is omitted here for the sake of brevity and can be found in the accompanying example source code repo.\n     JDK 9 brought some more nice improvements for the service loader API. E.g. the type of service implementations can be examined without actually instantiating them. This allows for interesting alternatives for providing service meta-data and choosing an implementation based on some criteria. For instance, greeter metadata like the language name and flag could be given using an annotation:\n 1 2 3 4 @GreeterDefinition(lang=\"English\", flag=\"🇬🇧\") public class EnglishGreeterFactory implements GreeterFactory { Greeter getGreeter(); }    Then the method ServiceLoader.Provider#type() can be used to obtain the annotation and return a greeter factory for a given language:\n 1 2 3 4 5 6 7 8 9 10 11 private Optional\u0026lt;GreeterFactory\u0026gt; getGreeterFactoryForLanguage( String language) { ModuleLayer layer = App.class.getModule().getLayer(); return ServiceLoader.load(layer, GreeterFactory.class) .stream() .filter(gf -\u0026gt; gf.type().getAnnotation( GreeterDefinition.class).lang().equals(language)) .map(gf -\u0026gt; gf.get()) .findFirst(); }          Seeing it in Action Lastly, let\u0026#8217;s take a look at the complete Greeter application in action. Here it is, initially with two, and then with three greeter implementations:\n   The layers configuration file is adjusted to load greeter plug-ins from the plugins directory; initially, two greeters for English and French exist. Then the German greeter implementation gets picked up by the application after adding it to the plug-in directory, without requiring any changes to the application tiself.\n The complete source code, including the logic for displaying all the available greeters and prompting for input, is available in the Layrry repository on GitHub.\n And there you have it, a basic plug-in architecture using Layrry and the Java Module System. Going forward, this might evolve in a few ways. E.g. it might be desirable to detect additional plug-ins without having to restart the application, e.g. when thinking of desktop application use cases. While loading additional plug-ins in new layers should be comparatively easy, unloading already loaded layers, e.g. when updating a plug-in to a newer version, could potentially be quite tricky. In particular, there\u0026#8217;s no way to actively unload layers, so we\u0026#8217;d have to rely on the garbage collector to clean up unused layers, making sure no references to any of their classes are kept in other, active layers.\n One also could think of an event bus, allowing different plug-ins to communicate in a safe, yet loosely coupled way. What requirements would you have for plug-in centered applications running on the Java Module System? Let\u0026#8217;s exchange in the comments below!\n  ","id":19,"publicationdate":"Apr 21, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eMaking applications extensible with some form of plug-ins is a very common pattern in software design:\nbased on well-defined APIs provided by the application core, plug-ins can customize an application\u0026#8217;s behavior and provide new functionality.\nExamples include desktop applications like IDEs or web browsers, build tools such as Apache Maven or Gradle, as well as server-side applications such as Apache Kafka Connect,\na runtime for Kafka connectors plug-ins.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIn this post I\u0026#8217;m going to explore how the \u003ca href=\"https://www.jcp.org/en/jsr/detail?id=376\"\u003eJava Platform Module System\u003c/a\u003e's notion of module layers can be leveraged for implementing plug-in architectures on the JVM.\nWe\u0026#8217;ll also discuss how \u003ca href=\"https://github.com/moditect/layrry\"\u003eLayrry\u003c/a\u003e, a launcher and runtime for layered Java applications, can help with this task.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Plug-in Architectures With Layrry and the Java Module System","uri":"https://www.morling.dev/blog/plugin-architectures-with-layrry-and-the-java-module-system/"},{"content":"One of the biggest changes in recent Java versions has been the introduction of the module system in Java 9. It allows to organize Java applications and their dependencies in strongly encapsulated modules, utilizing explicit and well-defined module APIs and relationships.\n In this post I\u0026#8217;m going to introduce the Layrry open-source project, a launcher and Java API for executing modularized Java applications. Layrry helps Java developers to assemble modularized applications from dependencies using their Maven coordinates and execute them using module layers. Layers go beyond the capabilities of the \"flat\" module path specified via the --module-path parameter of the java command, e.g. allowing to use multiple versions of one module within one and the same application.\n Why Layrry? The Java Module System doesn\u0026#8217;t define any means of mapping between modules (e.g. com.acme.crm) and JARs providing such module (e.g. acme-crm-1.0.0.Final.jar), or retrieving modules from remote repositories using unique identifiers (e.g. com.acme:acme-crm:1.0.0.Final). Instead, it\u0026#8217;s the responsibility of the user to obtain all required JARs of a modularized application and provide them via the --module-path parameter.\n Furthermore, the module system doesn\u0026#8217;t define any means of module versioning; i.e. it\u0026#8217;s the responsibility of the user to obtain all modules in the right version. Using the --module-path option, it\u0026#8217;s not possible, though, to assemble an application that uses multiple versions of one and the same module. This may be desirable for transitive dependencies of an application, which might be required in different versions by two separate direct dependencies.\n This is where Layrry comes in (pronounced \"Larry\"): it provides a declarative approach as well as an API for assembling modularized applications. The (modular) JARs to be included are described using Maven GAV (group id, artifact id, version) coordinates, solving the issue of retrieving all required JARs from a remote repository, in the right version.\n With Layrry, applications are organized in module layers, which allows to use different versions of one and the same module in different layers of an application (as long as they are not exposed in a conflicting way on module API boundaries).\n   An Example As an example, let\u0026#8217;s consider an application made up of the following modules:\n   The application\u0026#8217;s main module, com.example:app, depends on two others, com.example:foo and com.example:bar. They in turn depend on the Log4j API and another module, com.example:greeter. The latter is used in two different versions, though.\n Let\u0026#8217;s take a closer look at the Greeter class in these modules. Here is the version in com.example:greeter@1.0.0, as used by com.example:foo:\n 1 2 3 4 5 6 public class Greeter { public String greet(String name, String from) { return \"Hello, \" + name + \" from \" + from + \" (Greeter 1.0.0)\"; } }    And this is how it looks in com.example:greeter@2.0.0, as used by com.example:bar:\n 1 2 3 4 5 6 7 8 9 10 11 public class Greeter { public String hello(String name, String from) { return \"Hello, \" + name + \" from \" + from + \" (Greeter 2.0.0)\"; } public String goodBye(String name, String from) { return \"Good bye, \" + name + \" from \" + from + \" (Greeter 2.0.0)\"; } }    The Greeter API has evolved in a backwards-incompatible way, i.e. it\u0026#8217;s not possible for the foo and bar modules to use the same version.\n With a \"flat\" module path (or classpath), there\u0026#8217;s no way for dealing with this situation. You\u0026#8217;d inevitably end up with a NoSuchMethodError, as either foo or bar would be linked at runtime against a version of the class different from the version it has been compiled against.\n The lack of support for using multiple module versions when working with the --module-path option might be surprising at first, but it\u0026#8217;s an explicit non-requirement of the module system to support multiple module versions or even deal with selecting matching module versions at all.\n This means that the module descriptors of both foo and bar require the greeter module without any version information:\n 1 2 3 4 5 module com.example.foo { exports com.example.foo; requires org.apache.logging.log4j; requires com.example.greeter; }    1 2 3 4 5 module com.example.bar { exports com.example.bar; requires org.apache.logging.log4j; requires com.example.greeter; }      Module Layers to the Rescue While only one version of a given module is supported when running applications via java --module-path=\u0026#8230;\u0026#8203;, there\u0026#8217;s a lesser known feature of the module system which provides a way out: module layers.\n A module layer \"is created from a graph of modules in a Configuration and a function that maps each module to a ClassLoader.\" Using the module layer API, multiple versions of a module can be loaded in different layers, thus using different classloaders.\n Note the layers API doesn\u0026#8217;t concern itself with obtaining JARs or modules from remote locations such as the Maven Central repository; instead, any modules must be provided as Path objects. Here is how a layer with the foo and greeter:1.0.0 modules could be assembled:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ModuleLayer boot = ModuleLayer.boot(); ClassLoader scl = ClassLoader.getSystemClassLoader(); Path foo = Paths.get(\"path/to/foo-1.0.0.jar\"); (1) Path greeter10 = Paths.get(\"path/to/greeter-1.0.0.jar\"); (2) ModuleFinder fooFinder = ModuleFinder.of(foo, greeter10); Configuration fooConfig = boot.configuration() (3) .resolve( fooFinder, ModuleFinder.of(), Set.of(\"com.example.foo\", \"com.example.greeter\") ); ModuleLayer fooLayer = boot.defineModulesWithOneLoader( fooConfig, scl); (4)      1 obtain foo-1.0.0.jar   2 obtain greeter-1.0.0.jar   3 Create a configuration derived from the \"boot\" module of the JVM, providing a ModuleFinder for the two JARs obtained before, and resolving the two modules   4 Create a module layer using the configuration, loading all contained modules with a single classloader    Similarly, you could create a layer for bar and greeter:2.0.0, as well as layers for log4j and the main application module. The layers API is very flexible, e.g. you could load each module in its own classloader and more. But all this flexibility can make using the API direcly a daunting task.\n Also using an API might not be what you want in the first place: wouldn\u0026#8217;t it be nice if there was a CLI tool, akin to using java --module-path=\u0026#8230;\u0026#8203;, but with the additional powers of module layers?\n   The Layrry Launcher This is where Layrry comes in: it is a CLI tool which takes a configuration of a layered application (defined in a YAML file) and executes it. The layer descriptor for the example above looks like so:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 layers: log: (1) modules: (2) - \"org.apache.logging.log4j:log4j-api:jar:2.13.1\" - \"org.apache.logging.log4j:log4j-core:jar:2.13.1\" - \"com.example:logconfig:1.0.0\" foo: parents: (3) - \"log\" modules: - \"com.example:greeter:1.0.0\" - \"com.example:foo:1.0.0\" bar: parents: - \"log\" modules: - \"com.example:greeter:2.0.0\" - \"com.example:bar:1.0.0\" app: parents: - \"foo\" - \"bar\" modules: - \"com.example:app:1.0.0\" main: (4) module: com.example.app class: com.example.app.App      1 Each layer has a unique name   2 The modules element lists all the modules contained in the layer, using Maven coordinates (group id, artifact id, version), unambigously referencing a (modular) JAR in a specific version   3 A layer can have one or more parent layers, whose modules it can access; if no parent is given, the JVM\u0026#8217;s \"boot\" layer is the implicit parent of a layer   4 The given main module and class is the one that will be executed by Layrry    The configuration above describes four layers, log, foo, bar and app, with the modules they contain and the parent/child relationships between these layers. Note how the versions 1.0.0 and 2.0.0 of the greeter module are used in foo and bar. The file also specifies the main class to execute when running this application.\n Using Layrry, a modular application is executed like this:\n 1 2 3 4 5 6 7 java -jar layrry-1.0-SNAPSHOT-jar-with-dependencies.jar \\ --layers-config layers.yml \\ Alice 20:58:01.451 [main] INFO com.example.foo.Foo - Hello, Alice from Foo (Greeter 1.0.0) 20:58:01.472 [main] INFO com.example.bar.Bar - Hello, Alice from Bar (Greeter 2.0.0) 20:58:01.473 [main] INFO com.example.bar.Bar - Good bye, Alice from Bar (Greeter 2.0.0)    The log messages show how the two versions of greeter are used by foo and bar, respectively. Layrry will download all referenced JARs using the Maven resolver API, i.e. you don\u0026#8217;t have to deal with manually obtaining all the JARs and providing them to the java runtime.\n   Using the Layrry API In addition to the YAML-based launcher, Layrry provides also a Java API for assembling and running layered applications. This can be used in cases where the structure of layers is only known at runtime, or for implementing plug-in architectures.\n In order to use Layrry programmatically, add the following dependency to your pom.xml:\n 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.moditect.layrry\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;layrry\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;    Then, the Layrry Java API can be used like this (showing the same example as above):\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Layers layers = Layers.layer(\"log\") .withModule(\"org.apache.logging.log4j:log4j-api:jar:2.13.1\") .withModule(\"org.apache.logging.log4j:log4j-core:jar:2.13.1\") .withModule(\"com.example:logconfig:1.0.0\") .layer(\"foo\") .withParent(\"log\") .withModule(\"com.example:greeter:1.0.0\") .withModule(\"com.example:foo:1.0.0\") .layer(\"bar\") .withParent(\"log\") .withModule(\"com.example:greeter:2.0.0\") .withModule(\"com.example:bar:1.0.0\") .layer(\"app\") .withParent(\"foo\") .withParent(\"bar\") .withModule(\"com.example:app:1.0.0\") .build(); layers.run(\"com.example.app/com.example.app.App\", \"Alice\");      Next Steps The Layrry project is still in its infancy. Nevertheless it can be a useful tool for application developers wishing to leverage the Java Module System. Obtaining modular JARs via Maven coordinates and providing an easy-to-use mechanism for organizing modules in layers enables usages which cannot be addressed using the plain java --module-path \u0026#8230;\u0026#8203; approach.\n Layrry is open-source (under the Apache License version 2.0). The source code is hosted on GitHub, and your contributions are very welcomed.\n Please let me know about your ideas and requirements in the comments below or by opening up issues on GitHub. Planned enhancements include support for creating modular runtime images (jlink) based on the modules referenced in a layers.yml file, and visualization of module layers and their modules via GraphViz.\n  ","id":20,"publicationdate":"Mar 29, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eOne of the biggest changes in recent Java versions has been the introduction of the \u003ca href=\"http://openjdk.java.net/projects/jigsaw/spec/\"\u003emodule system\u003c/a\u003e in Java 9.\nIt allows to organize Java applications and their dependencies in strongly encapsulated modules, utilizing explicit and well-defined module APIs and relationships.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIn this post I\u0026#8217;m going to introduce the \u003ca href=\"https://github.com/moditect/layrry\"\u003eLayrry\u003c/a\u003e open-source project, a launcher and Java API for executing modularized Java applications.\nLayrry helps Java developers to assemble modularized applications from dependencies using their Maven coordinates and execute them using module layers.\nLayers go beyond the capabilities of the \"flat\" module path specified via the \u003cem\u003e--module-path\u003c/em\u003e parameter of the \u003cem\u003ejava\u003c/em\u003e command,\ne.g. allowing to use multiple versions of one module within one and the same application.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Introducing Layrry: A Launcher and API for Modularized Java Applications","uri":"https://www.morling.dev/blog/introducing-layrry-runner-and-api-for-modularized-java-applications/"},{"content":"Within Debezium, the project I\u0026#8217;m working on at Red Hat, we recently encountered an \"interesting\" situation where we had to resolve a rather difficult merge conflict. As others where interested in how we addressed the issue, and also for our own future reference, I\u0026#8217;m going to give a quick run down of the problem we encountered and how we solved it.\n The Problem Ideally, we\u0026#8217;d only ever work on a single branch and would never have to deal with porting changes between the master and other branches. Oftentimes we cannot get around this, though: specific versions of a software may have to be maintained for some time, requiring to backport bugfixes from the current development branch to the branch corresponding to the maintained version.\n In our specific case we had to deal with backporting changes to our project documentation. To complicate things, this documentation (written in AsciiDoc) has been largely re-organized between master and the targeted older branch, 1.0. What used to be one large AsciiDoc file for each of the Debezium connectors, got split up into multiple smaller files on master now. This split was meant to be applied to 1.0 too, but due to some miscommunication in the team (these things happen, right) this wasn\u0026#8217;t done, whereas an asorted set of documentation changes had been backported already to the larger, monolithic AsciiDoc files.\n So the situation we faced was this:\n   large, monolithic AsciiDoc files on the 1.0 branch\n  smaller, modularized AsciiDoc files on master\n  Documentation updates applied on master, of which only a subset is relevant for 1.0 (new features shouldn\u0026#8217;t be added to the Debezium 1.0 documentation)\n  Some of the documentation updates relevant for the 1.0 branch already had been backported from master, while others had not\n   All in all, a rather convoluted situation; the full diff of the documentation sub-directory between the two branches was about 13K lines.\n So what should we do? Cherry-picking individual commits from master was not really an option, as there were a few hundred commits on master since 1.0 had been forked off. Also many commits would contain documentation and code changes. The latter had already been backported successfully before.\n Realizing that resolving that merge conflict was next to impossible, the next idea was to essentially start from scratch and re-apply all relevant documentation changes to the 1.0 branch. Our initial idea was to create a patch with the difference of the documentation directory between the two branches. But editing that patch file with 13K lines turned out to be not manageable, either.\n   The Solution This is when we were reminded of the possibilities of git filter-branch: using this command it should be possible to isolate all the documentation changes done on master since Debezium 1.0 and apply the required sub-set of these changes to the 1.0 branch.\n To start with a clean slate, we created a new temporary branch based on 1.0:\n git checkout -b docs_backport 1.0   We then reset the contents of the documentation directory to its state as of the 1.0.0.Final release, as that\u0026#8217;s where the 1.0 and master branches diverged.\n rm -rf documentation git add documentation git checkout v1.0.0.Final documentation git commit -m \"Resetting documentation dir to v1.0.0.Final\" # This should yield no differences git diff v1.0.0.Final..docs_backport documentation   The next step was to filter all commits on master so to only keep any changes to the documentation directory. This was done on a new branch, docs_filtered. The --subdirectory-filter option comes in handy for that:\n git checkout -b docs_filtered master git filter-branch -f --prune-empty \\ --subdirectory-filter documentation \\ v1.0.0.Final..docs_filtered   This leaves us with a branch docs_filtered which only contains the commits since the v1.0.0.Final tag that modified the documentation directory.\n The --subdirectory-filter option also moves the contents of the given directory to the root of the repo, though. That\u0026#8217;s not exactly what we need. But another option, --tree-filter, lets us restore the original directory layout. It allows to run a set of commands against each of the filtered commits. We can use this to move the contents of documentation back to that directory:\n git filter-branch -f \\ --tree-filter 'mkdir -p documentation; \\ mv antora.yml documentation 1\u0026gt;/dev/null 2\u0026gt;/dev/null; \\ mv modules documentation 1\u0026gt;/dev/null 2\u0026gt;/dev/null;' \\ v1.0.0.Final..docs_filtered   Examining the history now, we can see that the commits on the docs_filtered apply the changes to the documentation directory, as expected.\n One problem still remains, though: by means of the --subdirectory-filter option, the very first commit removes all contents besides the documentation directory. This can be fixed by doing an interactive rebase of the current branch, beginning at the v1.0.0.Final tag:\n git rebase -i v1.0.0.Final   We need to edit the very first commit; all changes besides those to the documentation directory need to be reverted from that commit. There might be a better way of doing so, I simply ran git checkout for all the other resources:\n git checkout v1.0.0.Final debezium-connector-mongodb git checkout v1.0.0.Final debezium-connector-mysql ...   At this point the filtered branch still is based off of the v1.0.0.Final tag, whereas it should be based off of the docs_backport branch. git rebase --onto to the rescue:\n git rebase --onto docs_backport v1.0.0.Final docs_filtered   This rebases all the commits from the docs_filtered branch onto the docs_backport branch. Now we have a state where where all the documention changes have been cleanly applied to the 1.0 code base, i.e. the following should yield no differences:\n git diff docs_filtered..master documentation   The last and missing step is to do another rebase of all the documentation commits, discarding those that apply to any features that didn\u0026#8217;t get backported to 1.0.\n Thankfully, my partner-in-crime Jiri Pechanec stepped in here: as he had done the original feature backport, it didn\u0026#8217;t take him too long to go through the list of documentation commits and identify those which were relevant for the 1.0 code base. After one more interactive rebase for applying those we finally were in a state, where all the required documentation changes had been backported.\n Looking at the 1.0 history, you\u0026#8217;d still see some partial documentation changes up to the point, where we decided to start all over and revert these. Theoretically we could do another git filter run to exclude those, but we decided against that, as we already had done releases off of the 1.0 branch and didn\u0026#8217;t want to alter the commit history of a released branch after the fact.\n  ","id":21,"publicationdate":"Mar 16, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eWithin \u003ca href=\"https://debezium.io/\"\u003eDebezium\u003c/a\u003e, the project I\u0026#8217;m working on at Red Hat, we recently encountered an \"interesting\" situation where we had to resolve a rather difficult merge conflict.\nAs others where interested in how we addressed the issue, and also for our own future reference,\nI\u0026#8217;m going to give a quick run down of the problem we encountered and how we solved it.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Reworking Git Branches with git filter-branch","uri":"https://www.morling.dev/blog/reworking-git-branches-with-git-filter-branch/"},{"content":"","id":22,"publicationdate":"Jan 29, 2020","section":"tags","summary":"","tags":null,"title":"jakartaee","uri":"https://www.morling.dev/tags/jakartaee/"},{"content":"","id":23,"publicationdate":"Jan 29, 2020","section":"tags","summary":"","tags":null,"title":"java","uri":"https://www.morling.dev/tags/java/"},{"content":"","id":24,"publicationdate":"Jan 29, 2020","section":"tags","summary":"","tags":null,"title":"microprofile","uri":"https://www.morling.dev/tags/microprofile/"},{"content":"","id":25,"publicationdate":"Jan 29, 2020","section":"tags","summary":"","tags":null,"title":"monitoring","uri":"https://www.morling.dev/tags/monitoring/"},{"content":"The JDK Flight Recorder (JFR) is an invaluable tool for gaining deep insights into the performance characteristics of Java applications. Open-sourced in JDK 11, JFR provides a low-overhead framework for collecting events from Java applications, the JVM and the operating system.\n In this blog post we\u0026#8217;re going to explore how custom, application-specific JFR events can be used to monitor a REST API, allowing to track request counts, identify long-running requests and more. We\u0026#8217;ll also discuss how the JFR Event Streaming API new in Java 14 can be used to export live events, making them available for monitoring and alerting via tools such as Prometheus and Grafana.\n JFR and its companion tool JDK Mission Control (JMC) for analyzing JFR recordings have come a long way; originally developed at BEA and part of the JRockit VM, they were later on commercial features of the Oracle JDK. As of Java 11, JFR got open-sourced and is part of OpenJDK distributions. JMC is also open-source, but it\u0026#8217;s an independent tool under the OpenJDK umbrella, which must be downloaded separately.\n Using the combination of JFR and JMC, you can get all kinds of information about your Java application, such as events on garbage collection, compilation, classloading, memory allocation, file and socket IO, method profiling data, and much more. To learn more about Flight Recorder and Mission Control in general, have a look at the Code One 2019 presentation Introduction to JDK Mission Control \u0026amp; JDK Flight Recorder by Marcus Hirt and Klara Ward. You can find some more links to related useful resources towards the end of this post.\n Custom Flight Recorder Events One thing that\u0026#8217;s really great about JFR and JMC is that you\u0026#8217;re not limited to the events and data baked into the JVM and platform libraries: JFR also provides an API for implementing custom events. That way you can use the low-overhead event recording infrastructure (its goal is to add at most 1% performance overhead) for your own event types. This allows you to record and analyze higher-level events, using the language of your application-specific domain.\n Taking my day job project Debezium as an example (an open-source platform for change data capture for a variety of databases), we could for instance produce events such as \"Snapshot started\", \"Snapshotting of table 'Customers' completed\", \"Captured change event for transaction log offset 123\" etc. Users could send us recordings with these events and we could dive into them, in order to identify bugs or performance issues.\n In the following let\u0026#8217;s consider a less complex and hence better approachable example, though. We\u0026#8217;ll implement an event for measuring the duration of REST API calls. The Todo service from my recent blog post on Quarkus Qute will serve as our guinea pig. It is based on the Quarkus stack and provides a simple REST API based on JAX-RS. As always, you can find the complete source code for this blog post on GitHub.\n Event types are implemented by extending the jdk.jfr.Event class; It already provides us with some common attributes such as a timestamp and a duration. In sub-classes you can add application-specific payload attributes, as well as some metadata such as a name and category which will be used for organizing and displaying events when looking at them in JMC.\n Which attributes to add depends on your specific requirements; you should aim for the right balance between capturing all the relevant information that will be useful for analysis purposes later on, while not going overboard and adding too much, as that could cause record files to become too large, in particular for events that are emitted with a high frequency. Also retrieval of the attributes should be an efficient operation, so to avoid any unneccessary overhead.\n Here\u0026#8217;s a basic event class for monitoring our REST API calls:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 @Name(JaxRsInvocationEvent.NAME) (1) @Label(\"JAX-RS Invocation\") @Category(\"JAX-RS\") @Description(\"Invocation of a JAX-RS resource method\") @StackTrace(false) (2) public class JaxRsInvocationEvent extends Event { static final String NAME = \"dev.morling.jfr.JaxRsInvocation\"; @Label(\"Resource Method\") (3) public String method; @Label(\"Media Type\") public String mediaType; @Label(\"Java Method\") public String javaMethod; @Label(\"Path\") public String path; @Label(\"Query Parameters\") public String queryParameters; @Label(\"Headers\") public String headers; @Label(\"Length\") @DataAmount (4) public int length; @Label(\"Response Headers\") public String responseHeaders; @Label(\"Response Length\") public int responseLength; @Label(\"Response Status\") public int status; }      1 The @Name, @Category, @Description and @Label annotations define some meta-data, e.g. used for controlling the appearance of these events in the JMC UI   2 JAX-RS invocation events shouldn\u0026#8217;t contain a stacktrace by default, as that\u0026#8217;d only increase the size of Flight Recordings without adding much value   3 One payload attribute is defined for each relevant property such as HTTP method, media type, the invoked path etc.   4 @DataAmount tags this attribute as a data amount (by default in bytes) and will be displayed accordingly in JMC; there are many other similar annotations in the jdk.jfr package, such as @MemoryAddress, @Timestamp and more    Having defined the event class itself, we must find a way for emitting event instances at the right point in time. In the simplest case, e.g. suitable for events related to your application logic, this might happen right in the application code itself. For more \"technical\" events it\u0026#8217;s a good idea though to keep the creation of Flight Recorder events separate from your business logic, e.g. by using mechanisms such as servlet filters, interceptors and similar, which allow to inject cross-cutting logic into the call flow of your application.\n You also might employ byte code instrumentation at build or runtime for this purpose. The JMC Agent project aims at providing a configurable Java agent that allows to dynamically inject code for emitting JFR events into running programs. Via the EventFactory class, the JFR API also provides a way for defining event types dynamically, should their payload attributes only be known at runtime.\n For monitoring a JAX-RS based REST API, the ContainerRequestFilter and ContainerResponseFilter contracts come in handy, as they allow to hook into the request handling logic before and after a REST request gets processed:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @Provider (1) public class FlightRecorderFilter implements ContainerRequestFilter, ContainerResponseFilter { @Override (2) public void filter(ContainerRequestContext requestContext) throws IOException { JaxRsInvocationEvent event = new JaxRsInvocationEvent(); if (!event.isEnabled()) { (3) return; } event.begin(); (4) requestContext.setProperty(JaxRsInvocationEvent.NAME, event); (5) } @Override (6) public void filter(ContainerRequestContext requestContext, ContainerResponseContext responseContext) throws IOException { JaxRsInvocationEvent event = (JaxRsInvocationEvent) requestContext .getProperty(JaxRsInvocationEvent.NAME); if (event == null || !event.isEnabled()) { return; } event.end(); (7) event.path = String.valueOf(requestContext.getUriInfo().getPath()); if (event.shouldCommit()) { (8) event.method = requestContext.getMethod(); event.mediaType = String.valueOf(requestContext.getMediaType()); event.length = requestContext.getLength(); event.queryParameters = requestContext.getUriInfo() .getQueryParameters().toString(); event.headers = requestContext.getHeaders().toString(); event.javaMethod = getJavaMethod(requestContext); event.responseLength = responseContext.getLength(); event.responseHeaders = responseContext.getHeaders().toString(); event.status = responseContext.getStatus(); event.commit(); (9) } } private String getJavaMethod(ContainerRequestContext requestContext) { String propName = \"org.jboss.resteasy.core.ResourceMethodInvoker\"; ResourceMethodInvoker invoker = (ResourceMethodInvoker)requestContext.getProperty(propName); return invoker.getMethod().toString(); } }      1 Allows the filter to be picked up automatically by the JAX-RS implementation   2 Will be invoked before the request is processed   3 Nothing to do if the event type is not enabled for recordings currently   4 Begin the timing of the event   5 Store the event in the request context, so it can be obtained again later on   6 Will be invoked after the request has been processed   7 End the timing of the event   8 The event should be committed if it is enabled and its duration is within the threshold configured for it; in that case, populate all the payload attributes of the event based on the values from the request and response contexts   9 Commit the event with Flight Recorder    With that, our event class is pretty much ready to be used. There\u0026#8217;s only one more thing to do, and that is registering the new type with the Flight Recorder system. A Quarkus application start-up lifecycle method comes in handy for that:\n 1 2 3 4 5 6 7 @ApplicationScoped public class Metrics { public void registerEvent(@Observes StartupEvent se) { FlightRecorder.register(JaxRsInvocationEvent.class); } }    Note this step isn\u0026#8217;t strictly needed, the event type can also be used without explicit registration. But doing so will later on allow to apply specific settings for the event in Mission Control (see below), also if no event of this type has been emitted yet.\n   Creating JFR Recordings Now let\u0026#8217;s capture some JAX-RS API events using Flight Recorder and inspect them in Mission Control.\n To do so, make sure to have Mission Control installed. Just as with OpenJDK, there are different builds for Mission Control to choose from. If you\u0026#8217;re in the Fedora/RHEL universe, there\u0026#8217;s a repository package which you can install, e.g. like this for the Fedora JMC package:\n 1 sudo dnf module install jmc:7/default    Alternatively, you can download builds for different platforms from Oracle; some more info about these builds can be found in this blog post by Marcus Hirt. There\u0026#8217;s also the Liberica Mission Control build by BellSoft and Zulu Mission Control by Azul. The AdoptOpenJDK provides snapshot builds of JMC 8 as well as an Eclipse update site for installing JMC into an existing Eclipse instance.\n If you\u0026#8217;d like to follow along and run these steps yourself, check out the source code from GitHub and then perform the following commands:\n 1 2 cd example-service \u0026amp;\u0026amp; mvn clean package \u0026amp;\u0026amp; cd .. docker-compose up --build    This builds the project using Maven and spins up the following services using Docker Compose:\n   example-service: The Todo example application\n  todo-db: The Postgres database used by the Todo service\n  prometheus and grafana: For monitoring live events later on\n   Then go to http://localhost:8080/todo, where you should see the Todo web application:\n   Now fire up Mission Control. The example service run via Docker Compose is configured so you can connect to it on localhost. In the JVM Browser, create a new connection with host \"localhost\" and port \"1898\". Hit \"Test connection\", which should yield \"OK\", then click \"Finish\".\n   Create a new recording by expanding the localhost:1898 node in the JVM Explorer, right-clicking on \"Flight Recorder\" and choosing \"Start Flight Recording\u0026#8230;\u0026#8203;\". Confirm the default settings, which will create a recording with a duration of one minute. Go back to the Todo web application and perform a few tasks like creating some new todos, editing and deleting them, or filtering the todo list.\n Either wait for the recording to complete or stop it by right-clicking on the recording name and selecting \"Stop\". Once the recording is done, it will be opened automatically. Now you could dive into all the logged events for the OS, the JVM etc, but as we\u0026#8217;re interested in our custom JAX-RS events, Choose \"Event Browser\" in the outline view and expand the \"JAX-RS\" category. You will see the events for all your REST API invocations, including information such as duration of the request, the HTTP method, the resource path and much more:\n   In a real-world use case, you could now use this information for instance to identify long-running requests and correlate these events with other data points in the Flight Recording, such as method profiling and memory allocation data, or sub-optimal SQL statements in your database.\n     If your application is running in production, it might not be feasible to connect to it via Mission Control from your local workstation. The jcmd utility comes in handy in that case; part of the JDK, you can use it to issue diagnostic commands against a running JVM.\n Amongst many other things, it allows you to start and stop Flight Recordings. On the environment with your running application, first run jcmd -l, which will show you the PIDs of all running Java processes. Having identified the PID of the process you\u0026#8217;d like to examine, you can initiate a recording like so:\n 1 2 jcmd \u0026lt;PID\u0026gt; JFR.start delay=5s duration=30s \\ name=MyRecording filename=my-recording.jfr    This will start a recording of 30 seconds, beginning in 5 seconds from now. Once the recording is done, you could copy the file to your local machine and load it into Mission Control for further analysis. To learn more about creating Flight Recordings via jcmd, refer to this great cheat sheet.\n     Another useful tool in the belt is the jfr command, which was introduced in JDK 12. It allows you to filter and examine the binary Flight Recording files. You also can use it to extract parts of a recording and convert them to JSON, allowing them to be processed with other tools. E.g. you could convert all the JAX-RS events to JSON like so:\n 1 jfr print --json --categories JAX-RS my-recording.jfr      Event Settings Sometimes it\u0026#8217;s desirable to configure detailed behaviors of a given event type. For the JAX-RS invocation event it might for instance make sense to only log invocations of particular paths in a specific recording, allowing for a smaller recording size and keeping the focus on a particular subset of all invocations. JFR supports this by the notion of event settings. Such settings can be specified when creating a recording; based on the active settings, particular events will be included or excluded in the recording.\n Inspired by the JavaDoc of @SettingDefinition let\u0026#8217;s see what\u0026#8217;s needed to enhance JaxRsInvocationEvent with that capability. The first step is to define a subclass of jdk.jfr.SettingControl, which serves as the value holder for our setting:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public class PathFilterControl extends SettingControl { private Pattern pattern = Pattern.compile(\".*\"); (1) @Override (2) public void setValue(String value) { this.pattern = Pattern.compile(value); } @Override (3) public String combine(Set\u0026lt;String\u0026gt; values) { return String.join(\"|\", values); } @Override (4) public String getValue() { return pattern.toString(); } (5) public boolean matches(String s) { return pattern.matcher(s).matches(); } }      1 A regular expression pattern that\u0026#8217;ll be matched against the path of incoming events; by default all paths are included (.*)   2 Invoked by the JFR runtime to set the value for this setting   3 Invoked when multiple recordings are running at the same time, combining the settings values   4 Invoked by the runtime for instance when getting the default value of the setting   5 Matches the configured setting value against a particular path    On the event class itself a method with the following characteristics must be declared which will receive the setting by the JFR runtime:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 class JaxRsInvocationEvent extends Event { @Label(\"Path\") public String path; // other members... @Label(\"Path Filter\") @SettingDefinition (1) protected boolean pathFilter(PathFilterControl pathFilter) { (2) return pathFilter.matches(path); } }      1 Tags this as a setting   2 The method must be public, take a SettingControl type as its single parameter and return boolean    This method will be invoked by the JFR runtime during the shouldCommit() call. It passes in the setting value of the current recording so it can applied to the path value of the given event. In case the filter returns true, the event will be added to the recording, otherwise it will be ignored.\n We also could use such setting to control the inclusion or exclusion of specific event attributes. For that, the setting definition method would always have to return true, but depending on the actual setting it might set particular attributes of the event class to null. For instance this might come in handy if we wanted to log the entire request/response body of our REST API. Doing this all the time might be prohibitive in terms of recording size, but it might be enabled for a particlar short-term recording for analyzing some bug.\n Now let\u0026#8217;s see how the path filter can be applied when creating a new recording in Mission Control. The option is a bit hidden, but here\u0026#8217;s how you can enable it. First, create a new Flight Recording, then choose \"Template Manager\" in the dialogue:\n   Duplicate the \"Continuous\" template and edit it:\n   Click \"Advanced\":\n   Expand \"JAX-RS\" \u0026#8594; \"JAX-RS Invocation\" and put .*(new|edit).* into the Path Filter control:\n   Now close the last two dialogues. In the \"Start Flight Recording\" dialogue make sure to select your new template under \"Event Settings\"; although you\u0026#8217;ve edited it before, it won\u0026#8217;t be selected automatically. I lost an hour or so wondering why my settings were not applied\u0026#8230;\u0026#8203; .\n Lastly, click \"Finish\" to begin the recording:\n   Perform some tasks in the Todo web app and stop the recording. You should see only the REST API calls for the new and edit operations, whereas no events should be shown for the list and delete operations of the API.\n     In order to apply specific settings when creating a recording on the CLI using jcmd, edit the settings as described above. Then go to the Template Manager and export the profile you\u0026#8217;d like to use. When starting the recording via jcmd, specify the settings file via the settings=/path/to/settings.jfc parameter.\n       JFR Event Streaming Flight Recorder files are great for analyzing performance characteristics in an \"offline\" approach: you can take recordings in your production environment and ship them to your work station or a remote support team, without requiring live access to the running application. This is also an interesting mode for open-source projects, where maintainers typically don\u0026#8217;t have access to running applications of their users. Exchanging Flight Recordings (limited to a sensible subset of information, so to avoid exposure of confidential internals) might allow open source developers to gain insight into characteristics of their libraries when deployed to production at their users.\n But there\u0026#8217;s another category of use cases for event data sourced from applications, the JVM and the operating system, where the recording file approach doesn\u0026#8217;t quite fit: live monitoring and alerting of running applications. E.g. operations teams might want to set up dashboards showing the most relevant application metrics in \"real-time\", without having to create any recording files first. A related requirement is alerting, so to be notified when metrics reach a certain threshold. For instance it might be desirable to be alterted if the request duration of our JAX-RS API goes beyond a defined value such as 100 ms.\n This is where JEP 349 (\"JFR Event Streaming\") comes in. It\u0026#8217;ll be part of Java 14 and its stated goal is to \"provide an API for the continuous consumption of JFR data on disk, both for in-process and out-of-process applications\". That\u0026#8217;s exactly what we need for our monitoring/dashboarding use case. Using the Streaming API, Flight Recorder events of the running application can be exposed to external consumers, without having to explicitly load any recording files.\n Now it may be prohibitively expensive to stream each and every event with all its detailed information to remote clients. But that\u0026#8217;s not needed for monitoring purposes anyways. Instead, we can expose metrics based on our events, such as the total number and frequency of REST API invocations, or the average and 99th percentile duration of the calls.\n   MicroProfile Metrics The following shows a basic implementation of exposing these metrics for the JAX-RS API events to Prometheus/Grafana, where they can be visualized using a dashboard. Being based on Quarkus, the Todo web application can leverage all the MicroProfile APIs. On of them is the MicroProfile Metrics API, which defines a \"unified way for Microprofile servers to export Monitoring data (\"Telemetry\") to management agents\".\n While the MicroProfile Metrics API is used in an annotation-driven fashion often-times, it also provides a programmatic API for registering metrics. This can be leveraged to expose metrics based on the JAX-RS Flight Recorder events:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 @ApplicationScoped public class Metrics { @Inject (1) MetricRegistry metricsRegistry; private RecordingStream recordingStream; (2) public void onStartup(@Observes StartupEvent se) { recordingStream = new RecordingStream(); (3) recordingStream.enable(JaxRsInvocationEvent.NAME); recordingStream.onEvent(JaxRsInvocationEvent.NAME, event -\u0026gt; { (4) String path = event.getString(\"path\") .replaceAll(\"(\\\\/)([0-9]+)(\\\\/?)\", \"$1{param}$3\"); (5) String method = event.getString(\"method\"); String name = path + \"-\" + method; Metadata metadata = metricsRegistry.getMetadata().get(name); if (metadata == null) { metricsRegistry.timer(Metadata.builder() (6) .withName(name) .withType(MetricType.TIMER) .withDescription(\"Metrics for \" + path + \" (\" + method + \")\") .build()).update(event.getDuration().toNanos(), TimeUnit.NANOSECONDS); } else { (7) metricsRegistry.timer(name).update(event.getDuration() .toNanos(), TimeUnit.NANOSECONDS); } }); recordingStream.startAsync(); (8) } public void stop(@Observes ShutdownEvent se) { recordingStream.close(); (9) try { recordingStream.awaitTermination(); } catch (InterruptedException e) { throw new RuntimeException(e); } } }      1 Inject the MicroProfile Metrics registry   2 A stream providing push access to JFR events   3 Initialize the stream upon application start-up, so it includes the JAX-RS invocation events   4 For each JaxRsInvocationEvent this callback will be invoked   5 To register a corresponding metric, any path parameters are replaced with a constant placeholder, so that e.g. all invocations of the todo/{id}/edit path are exposed via one single metric instead of having separate ones for Todo 1, Todo 2 etc.   6 If the metric for the specific path hasn\u0026#8217;t been registered yet, then do so; it\u0026#8217;s a metric of type TIMER, allowing metric consumers to track the duration of calls of that particular path   7 If the metric for the path has been registered before, update its value with the duration of the incoming event   8 Start the stream asynchronously, not blocking the onStartup() method   9 Close the JFR event stream upon application shutdown    When connecting to the running application using JMC now, you\u0026#8217;ll see a continuous recording, which serves as the basis for the event stream. It only contains events of the JaxRsInvocationEvent type.\n MicroProfile Metrics exposes any application-provided metrics in the Prometheus format under the /metrics/application endpoint; for each operation of the REST API, e.g. POST to /todo/{id}/edit, the following metrics are provided:\n   request rate per second, minute, five minutes and 15 minutes\n  min, mean and max duration as well as standard deviation\n  total invocation count\n  duration of 75th, 95th, 99th etc. percentiles\n     Once the endpoint is provided, it\u0026#8217;s not difficult to set up a scraping process for ingesting the metrics into the Prometheus time-series database. You can find the required Prometheus configuration in the accompanying source code repository.\n While Prometheus provides some visualization capabilities itself, it is often used together with Grafana, which allows to build nicely looking dashboards via a rather intuitive UI. Here\u0026#8217;s an example dashboard showing the duration and invocation numbers for the different methods in the Todo REST API:\n   Again you can find the complete configuration for Grafana including the definition of that dashboard in the example repo. It will automatically be loaded when using the Docker Compose set-up shown above. Based on that you could easily expand the dashboard for other metrics and set up alerts, too.\n Combining the monitoring of live key metrics with the deep insights possible via detailed JFR recordings enable a very powerful workflow for analysing performance issues in production:\n   When setting up the continuous recording that serves as the basis for the metrics, have it contain all the event types you\u0026#8217;d need to gain insight into GC or memory issues etc.; specify a maximum size via RecordingStream#setMaxSize(), so to avoid an indefinitely growing recording; you\u0026#8217;ll probably need to experiment a bit to find the right trade-off between number of enabled events, duration that\u0026#8217;ll be covered by the recording and the required disk space\n  Only expose a relevant subset of the events as metrics to Prometheus/Grafana, such as the JAX-RS API invocation events in our example\n  Set up an alert in Grafana on the key metrics, e.g. mean duration of the REST calls, or 99th percentile thereof\n  If the alert triggers, take a dump of the last N minutes of the continuous recording via JMC or jcmd (using the JFR.dump command), and analyze that detailed recording to understand what was happening in the time leading to the alert\n     Summary and Related Work Flight Recorder and Mission Control are excellent tools providing deep insight into the performance characteristics of Java applications. While there\u0026#8217;s a large amount of data and highly valuable information provided out the box, JFR and JMC also allow for the recording of custom, application-specific events. With its low overhead, JFR can be enabled on a permanent basis in production environments. Combined with the Event Streaming API introduced in Java 14, this opens up an attractive, very performant alternative to other means of capturing analysis information at application runtime, such as logging libraries. Providing live key metrics derived from JFR events to tools such as Prometheus and Grafana enables monitoring and alerting in \"real-time\".\n For many enterprises that are still on Java 11 or even 8, it\u0026#8217;ll still be far out into the future until they might adopt the streaming API. But with more and more companies joining the OpenJDK efforts, it might be a possiblity that this useful feature gets backported to earlier LTS releases, just as the open-sourced version of Flight Recorder itself got backported to Java 8.\n There are quite a few posts and presentations about JFR and JMC available online, but many of them refer to older versions of those tools, before they got open-sourced. Here are some up-to-date resources which I found very helpful:\n   Continuous Monitoring with JDK Flight Recorder: a talk from QCon SF 2019 by Mikael Vidstedt\n  Flight Recorder \u0026amp; Mission Control at Code One 2019: a compilation of several great sessions on these two tools at last year\u0026#8217;s Code One, put together by Marcus Hirt\n  Digging Into Sockets With Java Flight Recorder: blog post by Petr Bouda on identifying performance bottlenecks with JFR in a Netty-based web application\n   Lastly, the Red Hat OpenJDK team is working on some very interesting projects around JFR and JMC, too. E.g. they\u0026#8217;ve built a datasource for Grafana which lets you examine the events of a JFR file. They also work on tooling to simplify the usage of JFR in container-based environments such as Kubernetes and OpenShift, including a K8s Operator for controlling Flight Recordings and a web-based UI for managing JFR in remote JVMs. Should you happen to be at the FOSDEM conference in Brussels on the next weekend, be sure to not miss the JMC \u0026amp; JFR - 2020 Vision session by Red Hat engineer Jie Kang.\n If you\u0026#8217;d like to experiment with JDK Flight Recorder and JDK Mission Control based on the Todo web application yourself, you can find the complete source code for this post on GitHub.\n Many thanks to Mario Torre and Jie Kang for reviewing an early draft of this post.\n  ","id":26,"publicationdate":"Jan 29, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe \u003ca href=\"https://openjdk.java.net/jeps/328\"\u003eJDK Flight Recorder\u003c/a\u003e (JFR) is an invaluable tool for gaining deep insights into the performance characteristics of Java applications.\nOpen-sourced in JDK 11, JFR provides a low-overhead framework for collecting events from Java applications, the JVM and the operating system.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIn this blog post we\u0026#8217;re going to explore how custom, application-specific JFR events can be used to monitor a REST API, allowing to track request counts, identify long-running requests and more.\nWe\u0026#8217;ll also discuss how the JFR \u003ca href=\"https://openjdk.java.net/jeps/349\"\u003eEvent Streaming API\u003c/a\u003e new in Java 14 can be used to export live events,\nmaking them available for monitoring and alerting via tools such as Prometheus and Grafana.\u003c/p\u003e\n\u003c/div\u003e","tags":["java","monitoring","microprofile","jakartaee","quarkus"],"title":"Monitoring REST APIs with Custom JDK Flight Recorder Events","uri":"https://www.morling.dev/blog/rest-api-monitoring-with-custom-jdk-flight-recorder-events/"},{"content":"","id":27,"publicationdate":"Jan 29, 2020","section":"tags","summary":"","tags":null,"title":"quarkus","uri":"https://www.morling.dev/tags/quarkus/"},{"content":"","id":28,"publicationdate":"Jan 29, 2020","section":"tags","summary":"","tags":null,"title":"Tags","uri":"https://www.morling.dev/tags/"},{"content":"","id":29,"publicationdate":"Jan 20, 2020","section":"tags","summary":"","tags":null,"title":"bean-validation","uri":"https://www.morling.dev/tags/bean-validation/"},{"content":"Record types are one of the most awaited features in Java 14; they promise to \"provide a compact syntax for declaring classes which are transparent holders for shallowly immutable data\". One example where records should be beneficial are data transfer objects (DTOs), as e.g. found in the remoting layer of enterprise applications. Typically, certain rules should be applied to the attributes of such DTO, e.g. in terms of allowed values. The goal of this blog post is to explore how such invariants can be enforced on record types, using annotation-based constraints as provided by the Bean Validation API.\n Record Invariants and Bean Validation Records (a preview feature as of Java 14) help to cut down the ceremony when defining plain data holder objects. In a nutshell, you solely need to declare the attributes that should make up the state of the record type (\"components\" in terms of JEP 359), and quite a few things you\u0026#8217;d otherwise have to implement by hand will be created for you automatically:\n   a private final field and a corresponding read accessor for each component\n  a constructor for passing in all component values\n  toString(), equals() and hashCode() methods.\n   As an example, here\u0026#8217;s a record Car with three components:\n 1 2 3 public record Car(String manufacturer, String licensePlate, int seatCount) { }    Now let\u0026#8217;s assume a few class invariants should be applied to this record (inspired by an example from the Hibernate Validator reference guide):\n   manufacturer is a non-blank string\n  license plate is never null and has a length of 2 to 14 characters\n  seatCount is at least 2\n   Class invariants like these are specific conditions or rules applying to the state of a class (as manifesting in its fields), which always are guaranteed to be satisfied for the lifetime of an instance of the class.\n The Bean Validation API defines a way for expressing and validating constraints using Java annotations. By putting constraint annotations to the components of a record type, it\u0026#8217;s a perfect means of describing the invariants from above:\n 1 2 3 4 5 public record Car( @NotBlank String manufacturer, @NotNull @Size(min = 2, max = 14) String licensePlate, @Min(2) int seatCount) { }    Of course declaring constraints using annotations by itself won\u0026#8217;t magically enforce these invariants. In order to do so, the javax.validation.Validator API must be invoked at suitable points in the object lifecycle, so to avoid any of the invariants to be violated. As records are immutable, it is sufficient to validate the constraints once when creating a new Car instance. If no constraints are violated, the created instance is guaranteed to always satisfy its invariants.\n   Implementation The key question now is how to validate the invariants while constructing new Car instances. This is where Bean Validation\u0026#8217;s API for method validation comes in: it allows to validate pre- and post-conditions that should be satisfied when a Java method or constructor gets invoked. Pre-conditions are expressed by applying constraints to method and constructor parameters, whereas post-conditions are expressed by putting constraints to a method or constructor itself.\n This can be leveraged for enforcing record invariants: as it turns out, any annotations on the components of a record type are also copied to the corresponding parameters of the generated constructor. I.e. the Car record implicitly has a constructor which looks like this:\n 1 2 3 4 5 6 7 8 9 public Car( @NotBlank String manufacturer, @NotNull @Size(min = 2, max = 14) String licensePlate, @Min(2) int seatCount) { this.manufacturer = manufacturer; this.licensePlate = licensePlate; this.seatCount = seatCount; }    That\u0026#8217;s exactly what we need: by validating these parameter constraints upon instantiation of the Car class, we can make sure that only valid objects can ever be created, ensuring that the record type\u0026#8217;s invariants are always guaranteed.\n What\u0026#8217;s missing is a way for automatically validating them upon constructor invocation. The idea for that is to enhance the byte code of the implicit Car constructor so that it passes the incoming parameter values to Bean Validation\u0026#8217;s ExecutableValidator#validateConstructorParameters() method and raises a constraint violation exception in case of any invalid parameter values.\n We\u0026#8217;re going to use the excellent ByteBuddy library for this job. Here\u0026#8217;s a slightly simplified implementation for invoking the executable validator (you can find the complete source code of this example in this GitHub repository):\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 public class ValidationInterceptor { private static final Validator validator = Validation (1) .buildDefaultValidatorFactory() .getValidator(); public static \u0026lt;T\u0026gt; void validate(@Origin Constructor\u0026lt;T\u0026gt; constructor, @AllArguments Object[] args) { (2) Set\u0026lt;ConstraintViolation\u0026lt;T\u0026gt;\u0026gt; violations = validator (3) .forExecutables() .validateConstructorParameters(constructor, args); if (!violations.isEmpty()) { String message = violations.stream() (4) .sorted(ValidationInterceptor::compare) .map(cv -\u0026gt; getParameterName(cv) + \" - \" + cv.getMessage()) .collect(Collectors.joining(System.lineSeparator())); throw new ConstraintViolationException( (5) \"Invalid instantiation of record type \" + constructor.getDeclaringClass().getSimpleName() + System.lineSeparator() + message, violations); } } private static int compare(ConstraintViolation\u0026lt;?\u0026gt; o1, ConstraintViolation\u0026lt;?\u0026gt; o2) { return Integer.compare(getParameterIndex(o1), getParameterIndex(o2)); } private static String getParameterName(ConstraintViolation\u0026lt;?\u0026gt; cv) { // traverse property path to extract parameter name } private static int getParameterIndex(ConstraintViolation\u0026lt;?\u0026gt; cv) { // traverse property path to extract parameter index } }      1 Obtain a Bean Validation Validator instance   2 The @Origin and @AllArguments annotations are the hint to ByteBuddy that the invoked constructor and parameter values should be passed to this method from within the enhanced constructor   3 Validate the passed constructor arguments using Bean Validation   4 If there\u0026#8217;s at least one violated constraint, create a message comprising all constraint violation messages, ordered by parameter index   5 Raise a ConstraintViolationException, containing the message created before as well as all the constraint violations    Having implemented the validation interceptor, the code of the record constructor must be enhanced by ByteBuddy, so that it invokes the inceptor. ByteBuddy provides different ways for doing so, e.g. at application start-up using a Java agent. For this example, we\u0026#8217;re going to employ build-time enhancement via the ByteBuddy Maven plug-in. The enhancement logic itself is implemented in a custom net.bytebuddy.build.Plugin:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 public class ValidationWeavingPlugin implements Plugin { @Override public boolean matches(TypeDescription target) { (1) return target.getDeclaredMethods() .stream() .anyMatch(m -\u0026gt; m.isConstructor() \u0026amp;\u0026amp; hasConstrainedParameter(m)); } @Override public Builder\u0026lt;?\u0026gt; apply(Builder\u0026lt;?\u0026gt; builder, TypeDescription typeDescription, ClassFileLocator classFileLocator) { return builder.constructor(this::hasConstrainedParameter) (2) .intercept(SuperMethodCall.INSTANCE.andThen( MethodDelegation.to(ValidationInterceptor.class))); } private boolean hasConstrainedParameter(MethodDescription method) { return method.getParameters() (3) .asDefined() .stream() .anyMatch(p -\u0026gt; isConstrained(p)); } private boolean isConstrained( ParameterDescription.InDefinedShape parameter) { (4) return !parameter.getDeclaredAnnotations() .asTypeList() .filter(hasAnnotation(annotationType(Constraint.class))) .isEmpty(); } @Override public void close() throws IOException { } }      1 Determines whether a type should be enhanced or not; this is the case if there\u0026#8217;s at least one constructor that has one more more constrained parameters   2 Applies the actual enhancement: into each constrained constructor the call to ValidationInterceptor gets injected   3 Determines whether a method or constructor has at least one constrained parameter   4 Determines whether a parameter has at least one constraint annotation (an annotation meta-annotated with @Constraint; for the sake of simplicity the case of constraint inheritance is ignored here)    The next step is to configure the ByteBuddy Maven plug-in in the pom.xml of the project:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;net.bytebuddy\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;byte-buddy-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${version.bytebuddy}\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;transform\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;transformations\u0026gt; \u0026lt;transformation\u0026gt; \u0026lt;plugin\u0026gt; dev.morling.demos.recordvalidation.implementation.ValidationWeavingPlugin \u0026lt;/plugin\u0026gt; \u0026lt;/transformation\u0026gt; \u0026lt;/transformations\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt;    This plug-in runs in the process-classes phase by default, so it can access and enhance the class files generated during compilation. If you were to build the project now, you could use the javap tool to examine the byte code of the Car class,and you\u0026#8217;d see that the implicit constructor of that class contains an invocation of the ValidationInterceptor#validate() method.\n As an example, let\u0026#8217;s consider the following attempt to instantiate a Car object, which violates the invariants of that record type:\n 1 Car invalid = new Car(\"\", \"HH-AB-123\", 1);    A constraint violation like this will be thrown immediately:\n 1 2 3 4 5 javax.validation.ConstraintViolationException: Invalid instantiation of record type Car manufacturer - must not be blank seatCount - must be greater than or equal to 2 at dev.morling.demos.recordvalidation.RecordValidationTest.canValidate(RecordValidationTest.java:20)    If all constraints are satisfied, no exception will be thrown and the caller obtains the new Car instance, whose invariants are guaranteed to be met for the remainder of the object\u0026#8217;s lifetime.\n   Advantages Having shown how Bean Validation can be leveraged to enforce the invariants of Java record types, it is time to reflect: is this this approach worth the additional complexity incurred by adding a library such as Bean Validation and hooking it up using byte code enhancement? After all, you could also validate incoming parameter values using methods such as Objects#requireNonNull().\n As so often, you need to make such decision based on your specific requirements and needs. Here are some advantages I can see about the Bean Validation approach:\n   Invariants become part of the API: Constraint annotations on public API members such as the implicit record constructor are easily discoverable by users of such type; they are listed in generated JavaDoc, you can see them when hovering over an invocation in your IDE (once records are supported); when used on the DTOs of a REST layer, the invariants could also be added to automatically generated API documentation. All this makes it easy for users of the type to understand the invariants and also avoids potential inconsistencies between a manual validation implementation and corresponding hand-written documentation\n  Providing constraint metadata: The Bean Validation constraint meta-data API can be used to obtain information about the constraints of Java types; for instance this can be used to implement client-side validation of constraints in a web application\n  Less code: Putting constraint annotations directly to the record components themselves avoids the need for implementing these checks manually in an explicit canonical constructor\n  I18N support: Bean Validation provides means of internationalizing constraint violation messages; if your record types are instantiated based on user input (e.g. when using them as data types in a REST API), this allows for localized error messages in the UI\n  Returning all constraints at once: For UIs it\u0026#8217;s typically beneficial to return all the constraint violations at once instead of showing them one by one; while doable in a hand-written implementation, it requires a bit of effort, whereas you get this \"for free\" when using Bean Validation which always returns a set of all the violations\n  Lots of ready-made constraints: Bean Validation comes with a range of constraints out of the box; in addition libraries such as Hibernate Validator and others provide many more ready-to-use constraints, coming in handy for instance when implementing domain-specific value types with complex validation rules:\n1 2 3 public record EmailAddress( @Email @NotNull @Size(min=1, max=250) String value) { }      Support for validation groups: Bean Validation\u0026#8217;s concept of validation groups allows you to validate only sub-sets of constraints in specific contexts; e.g. based on location and applying legal requirements\n  Dynamic constraint definition: Using Hibernate Validator, constraints can also be declared dynamically using a fluent API. This can be very useful when your validation requirements vary at runtime, e.g. if you need to apply different constraint configurations for different tenants.\n     Limitations One area where this current proof-of-concept implementation falls a bit short is the validation of invariants that apply to multiple components. For instance consider a record type representing an interval with a begin and an end attribute, where you\u0026#8217;d like to enforce the invariant that end is larger than begin.\n Bean Validation addresses this sort of requirement via class-level constraints and, for method and constructor validation, cross-parameter constraints. Class-level constraints are not really suitable for our purposes, because we want to validate the invariants before an object instance is created.\n Cross-parameter constraints on the other hand are exactly what we\u0026#8217;d need. As they must be given on a constructor or method, the canonical constructor of a record must be explicitly declared in this case. Using Hibernate Validator\u0026#8217;s @ParameterScriptAssert constraint, the invariant from above could be expressed like so:\n 1 2 3 4 5 6 public record Interval(int begin, int end) { @ParameterScriptAssert(lang=\"javascript\", script=\"end \u0026gt; begin\") public Interval { } }    This works as expected, but there\u0026#8217;s one caveat: any annotations from the record components are not propagated to the corresponding parameters of the canoncial constructor in this case. This means that any constraints given on the individual components would be lost. Right now it\u0026#8217;s not quite clear to me whether that\u0026#8217;s an intended behavior or rather a bug in the current record implementation.\n If indeed it is intentional, than there\u0026#8217;d be no way other than specifying the constraints explicitly on the parameters of a fully manually implemented constructor:\n 1 2 3 4 5 6 7 8 public record Interval(int begin, int end) { @ParameterScriptAssert(lang=\"javascript\", script=\"end \u0026gt; begin\") public Interval(@Positive int begin, @Positive int end) { this.begin = begin; this.end = end; } }    This works, but of course we\u0026#8217;re losing a bit of the conciseness promised by records.\n Update, Jan 20, 2020, 20:57: Turns out, the current behavior indeed is not intended (see JDK-8236597) and in a future Java version the shorter version of the code shown above should work.\n   Wrap-Up In this blog post we\u0026#8217;ve explored how invariants on Java 14 record types can be enforced using the Bean Validation API. With just a bit of byte code magic the task gets manageable: by validating invariants expressed by constraint annotations on record components right at instantiation time, only valid record instances will ever be exposed to callers. Key for that is the fact that any annotations from record components are automatically propagated to the corresponding parameters of the canonical record constructor. That way they can be validated using Bean Validation\u0026#8217;s method validation API. It remains to be seen, whether invariants based on multiple record components also can be enforced as easily.\n From the perspective of the Bean Validation specification, it\u0026#8217;ll surely make sense to explore support for record types. While not as powerful as enforcing invariants at construction time via byte code enhancement, it might also be useful to support the validation of component values via their read accessors. For that, the notion of \"properties\" would have to be relaxed, as the read accessors of records don\u0026#8217;t have the JavaBeans get prefix currently expected by Bean Validation. It also should be considered to expand the Bean Validation metadata API accordingly.\n I would also be very happy to learn about your thoughts around this topic. While Bean Validation 3.0 (as part of Jakarta EE 9) in all likelyhood won\u0026#8217;t bring any changes besides the transition to the jakarta.* package namespace, this may be an area where we could evolve the specification for Jakarta EE 10.\n If you\u0026#8217;d like to experiment with the validation of record types yourself, you can find the complete source code on GitHub.\n   ","id":30,"publicationdate":"Jan 20, 2020","section":"blog","summary":"Record types are one of the most awaited features in Java 14; they promise to \"provide a compact syntax for declaring classes which are transparent holders for shallowly immutable data\". One example where records should be beneficial are data transfer objects (DTOs), as e.g. found in the remoting layer of enterprise applications. Typically, certain rules should be applied to the attributes of such DTO, e.g. in terms of allowed values. The goal of this blog post is to explore how such invariants can be enforced on record types, using annotation-based constraints as provided by the Bean Validation API.","tags":["bean-validation","jakartaee"],"title":"Enforcing Java Record Invariants With Bean Validation","uri":"https://www.morling.dev/blog/enforcing-java-record-invariants-with-bean-validation/"},{"content":"When Java 9 was introduced in 2017, it was the last major version published under the old release scheme. Since then, a six month release cadence has been adopted. This means developers don\u0026#8217;t have to wait years for new APIs and language features, but they can get their hands onto the latest additions twice a year. In this post I\u0026#8217;d like to describe how you can try out new language features such as Java 13 text blocks in the test code of your project, while keeping your main code still compatible with older Java versions.\n One goal of the increased release cadence is to shorten the feedback loop for the OpenJDK team: have developers in the field try out new functionality early on, collect feedback based on that, adjust as needed. To aid with that process, the JDK has two means of publishing preliminary work before new APIs and language features are cast in stone:\n   Incubator JDK modules\n  Preview language and VM features\n   An example for the former is the new HTTP client API, which was an incubator module in JDK 9 and 10, before it got standardized as a regular API in JDK 11. Examples for preview language features are switch expressions (added as a preview feature in Java 12) and text blocks (added in Java 13).\n Now especially text blocks are a feature which many developers have missed in Java for a long time. They are really useful when embedding other languages, or just any kind of longer text into your Java program, e.g. multi-line SQL statements, JSON documents and others. So you might want to go and use them as quickly as possible, but depending on your specific situation and requirements, you may no be able to move to Java 13 just yet.\n In particular when working on libraries, compatibility with older Java versions is a high priority in order to not cut off a large number of potential users. E.g. in the JetBrains Developer Ecosystem Survey from early 2019, 83% of participants said that Java 8 is a version they regularly use. This matches with what I\u0026#8217;ve observed myself during conversations e.g. at conferences. Now this share may have reduced a bit since then (I couldn\u0026#8217;t find any newer numbers), but at this point in time it still seems save to say that libraries should support Java 8 to not limit their audience in a signficant way.\n So while building on Java 13 is fine, requiring it at runtime for libraries isn\u0026#8217;t. Does this mean as a library author you cannot use text blocks then for many years to come? For your main code (i.e. the one shipped to users) it indeed does mean that, but things look different when it comes to test code.\n An Example One case where text blocks come in extremely handy is testing of REST APIs, where JSON requests need to created and responses may have to be compared to a JSON string with the expected value. Here\u0026#8217;s an example of using text blocks in a test of a Quarkus-based REST service, implemented using RESTAssured and JSONAssert:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @QuarkusTest public class TodoResourceTest { @Test public void canPostNewTodoAndReceiveId() throws Exception { given() .when() .body(\"\"\" (1) { \"title\" : \"Learn Java\", \"completed\" : false } \"\"\" ) .contentType(ContentType.JSON) .post(\"/hello\") .then() .statusCode(201) .body(matchesJson(\"\"\" (2) { \"id\" : 1, \"title\" : \"Learn Java\", \"completed\" : false } \"\"\") ); } }      1 Text block with the JSON request to send   2 Text block with the expected JSON response    Indeed that\u0026#8217;s much nicer to read, e.g. when comparing the request JSON to the code you\u0026#8217;d typically write without text blocks. Concatenating multiple lines, escaping quotes and explicitly specifying line breaks make this quite cumbersome:\n 1 2 3 4 5 6 .body( \"{\\n\" + \" \\\"title\\\" : \\\"Learn Java 13\\\",\\n\" + \" \\\"completed\\\" : false\\n\" + \"}\" )    Now let\u0026#8217;s see what\u0026#8217;s needed in terms of configuration to enable usage of Java 13 text blocks for tests, while keeping the main code of a project compatible with Java 8.\n   Configuration Two options of the Java compiler javac come into play here:\n   --release: specifies the Java version to compile for\n  --enable-preview: allows to use language features currently in \"preview\" status such as text blocks as of Java 13/14\n       The --release option was introduced in Java 9 and should be preferred over the more widely known pair of --source and --target. The reason being that --release will prevent any accidental usage of APIs only introduced in later versions.\n E.g. say you were to write code such as List.of(\"Foo\", \"Bar\"); the of() methods on java.util.List were only introduced in Java 9, so compiling with --release 8 will raise a compilation error in this case. When using the older options, this situation wouldn\u0026#8217;t be detected at compile time, making the problem only apparent when actually running the application on the older Java version.\n     Build tools typically allow to use different configurations for the compilation of main and test code. E.g. here is what you\u0026#8217;d use for Maven (you can find the complete source code of the example in this GitHub repo):\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 ... \u0026lt;properties\u0026gt; ... \u0026lt;maven.compiler.release\u0026gt;8\u0026lt;/maven.compiler.release\u0026gt; (1) ... \u0026lt;/properties\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; ... \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.8.1\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;default-testCompile\u0026lt;/id\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;release\u0026gt;13\u0026lt;/release\u0026gt; (2) \u0026lt;compilerArgs\u0026gt;--enable-preview\u0026lt;/compilerArgs\u0026gt; (3) \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; ... \u0026lt;/plugins\u0026gt; ... \u0026lt;/build\u0026gt; ...      1 Compile for release 8 by default, i.e. the main code   2 Compile test code for release 13   3 Also pass the --enable-preview option when compiling the test code    Also at runtime preview features must be explicitly enabled. Therefore the java command must be accordingly configured when executing the tests, e.g. like so when using the Maven Surefire plug-in:\n 1 2 3 4 5 6 7 8 9 ... \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-surefire-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.22.1\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;argLine\u0026gt;--enable-preview\u0026lt;/argLine\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; ...    With this configuration in place, text blocks can now be used in tests as the one above, but not in the main code of the program. Doing so would result in a compilation error.\n Note your IDE might still let you do this kind of mistake. At least Eclipse chose for me the maximum of main (8) and test code (13) release levels when importing the project. But running the build on the command line via Maven or on your CI server will detect this situation.\n As Java 13 now is required to build this code base, it\u0026#8217;s a good idea to make this prerequisite explicit in the build process itself. The Maven enforcer plug-in comes in handy for that, allowing to express this requirement using its Java version rule:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ... \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-enforcer-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0-M3\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;enforce-java\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;enforce\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;rules\u0026gt; \u0026lt;requireJavaVersion\u0026gt; \u0026lt;version\u0026gt;[13,)\u0026lt;/version\u0026gt; \u0026lt;/requireJavaVersion\u0026gt; \u0026lt;/rules\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; ...    The plug-in will fail the build when being run on a version before Java 13.\n   Should You Do This? Having seen how you can use preview features in test code, the question is: should you actually do this? A few things should be kept in mind for answering that. First of all, preview features are really that, a preview. This means that details may change in future Java revisions. Or, albeit unlikely, such feature may even be dropped altogether, should the JDK team arrive at the conclusion that it is fundamentally flawed.\n Another important factor is the minimum Java language version supported by the JDK compiler. As of Java 13, the oldest supported release is 7; i.e. using JDK 13, you can produce byte code that can be run with Java versions as old as Java 7. In order to keep the Java compiler maintainable, support for older versions is dropped every now and then. Right now, there\u0026#8217;s no formal process in place which would describe when support for a specific version is going to be removed (defining such policy is the goal of JEP 182).\n As per JDK developer Joe Darcy, \"there are no plans to remove support for --release 7 in JDK 15\". Conversely, this means that support for release 7 theoretically could be removed in JDK 16 and support for release 8 could be removed in JDK 17. In that case you\u0026#8217;d be caught between a rock and a hard place: Once you\u0026#8217;re on a non-LTS (\"long-term support\") release like JDK 13, you\u0026#8217;ll need to upgrade to JDK 14, 15 etc. as soon as they are out, in order to not be cut off from bug fixes and security patches. Now while doing so, you\u0026#8217;d be forced to increase the release level of your main code, once support for release 8 gets dropped, which may not desirable. Or you\u0026#8217;d have to apply some nice awk/sed magic to replace all those shiny text blocks with traditional concatenated and escaped strings, so you can go back to the current LTS release, Java 11. Not nice, but surely doable.\n That being said, this all doesn\u0026#8217;t seem like a likely scenario to me. JEP 182 expresses a desire \"that source code 10 or more years old should still be able to be compiled\"; hence I think it\u0026#8217;s save to assume that JDK 17 (the next release planned to receive long-term support) will still support release 8, which will be seven years old when 17 gets released as planned in September 2021. In that case you\u0026#8217;d be on the safe side, receiving update releases and being able to keep your main code Java 8 compatible for quite a few years to come.\n Needless to say, it\u0026#8217;s a call that you need to make, deciding for yourself wether the benefits of using new language features such as text blocks is worth it in your specific situation or not.\n  ","id":31,"publicationdate":"Jan 13, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eWhen Java 9 was introduced in 2017,\nit was the last major version published under the old release scheme.\nSince then, a \u003ca href=\"https://www.infoq.com/news/2017/09/Java6Month/\"\u003esix month release cadence\u003c/a\u003e has been adopted.\nThis means developers don\u0026#8217;t have to wait years for new APIs and language features,\nbut they can get their hands onto the latest additions twice a year.\nIn this post I\u0026#8217;d like to describe how you can try out new language features such as \u003ca href=\"http://openjdk.java.net/jeps/355\"\u003eJava 13 text blocks\u003c/a\u003e in the test code of your project,\nwhile keeping your main code still compatible with older Java versions.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Using Java 13 Text Blocks (Only) for Your Tests","uri":"https://www.morling.dev/blog/using-java-13-text-blocks-for-tests/"},{"content":"One of the long-awaited features in Quarkus was support for server-side templating: until recently, Quarkus supported only client-side web frameworks which obtain there data by calling a REST API on the backend. This has changed with Quarkus 1.1: it comes with a brand-new template engine named Qute, which allows to build web applications using server-side templates.\n When looking at frameworks for building web applications, there\u0026#8217;s two large categories:\n   client-side solutions based on JavaScript such as React, vue.js or Angular\n  server-side frameworks such as Spring Web MVC, JSF or MVC 1.0 (in the Java world)\n   Both have their indivdual strengths and weaknesses and it\u0026#8217;d be not very wise to always prefer one over the other. Instead, the choice should be based on specific requirements (e.g. what kind of interactivity is needed) and prerequisites (e.g. the skillset of the team building the application).\n Being mostly experienced with Java, server-side solutions are appealing to me, as they allow me to use the language I know and tooling (build tools, IDEs) I\u0026#8217;m familiar and most productive with. So when Qute was announced, it instantly caught my attention and I had to give it a test ride. In this post I want to share some of the experiences I made.\n Note this isn\u0026#8217;t a comprehensive tutorial for building web apps with Qute, instead, I\u0026#8217;d like to discuss a few things that stuck out to me. You can find a complete working example here on GitHub. It implements a basic CRUD application for managing personal todos, persisted in a Postgres database. Here\u0026#8217;s a video that shows the demo in action:\n    The Basics The Qute engine is based on RESTEasy/JAX-RS. As such, Qute web applications are implemented by defining resource types with methods answering to specific HTTP verbs and accept headers. The only difference being, that HTML pages are returned instead of JSON as in your typical REST-ful data API. The individual pages are created by processing template files. Here\u0026#8217;s a basic example for returning all the Todo records in our application:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 @Path(\"/todo\") public class TodoResource { @Inject Template todos; @GET (1) @Consumes(MediaType.TEXT_HTML) (2) @Produces(MediaType.TEXT_HTML) public TemplateInstance listTodos() { return todos.data(\"todos\", Todo.findAll().list()); (3) } }      1 Processes HTTP GET requests for /todo   2 This method consumes and produces the text/html media type   3 Obtain all todos from the database and feed them to the todos template    The Todo class is as JPA entity implemented via Hibernate Panache:\n 1 2 3 4 5 6 7 @Entity public class Todo extends PanacheEntity { public String title; public int priority; public boolean completed; }    Panache is a perfect fit for this kind of CRUD applications. It helps with common tasks such as id mapping, and by means of the active record pattern you get query methods like findAll() \"for free\".\n To produce an HTML page for displaying the result list, the todos template is used. Templates are located under src/main/resources/templates. As you would expect it, changes to template files are immediatly picked up when running Quarkus in Dev Mode. By default, the template name is derived from the field name of the injected Template instance, i.e. in this case the src/main/resources/templates/todos.html template will be used. It could look like this:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\"en\"\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\"utf-8\"\u0026gt; \u0026lt;!-- CSS ... --\u0026gt; \u0026lt;link rel=\"stylesheet\" href=\"...\"\u0026gt; \u0026lt;title\u0026gt;My Todos\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\"container\"\u0026gt; \u0026lt;h1\u0026gt;My Todos\u0026lt;/h1\u0026gt; \u0026lt;table class=\"table table-striped table-bordered\"\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th scope=\"col\" class=\"fit\"\u0026gt;Id\u0026lt;/th\u0026gt; \u0026lt;th scope=\"col\" \u0026gt;Title\u0026lt;/th\u0026gt; \u0026lt;th scope=\"col\" class=\"fit\"\u0026gt;Priority\u0026lt;/th\u0026gt; \u0026lt;th scope=\"col\" class=\"fit\"\u0026gt;Completed\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; {#if todos.size == 0} (1) \u0026lt;tr\u0026gt; \u0026lt;td colspan=\"4\"\u0026gt;No data found.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; {#else} {#for todo in todos} (2) \u0026lt;tr\u0026gt; \u0026lt;th scope=\"row\"\u0026gt;#{todo.id}\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt; {todo.title} (3) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; {todo.priority} (4) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; (5) \u0026lt;div class=\"custom-control custom-checkbox\"\u0026gt; \u0026lt;input type=\"checkbox\" class=\"custom-control-input\" disabled id=\"completed-{todo.id}\" {#if todo.completed}checked{/if}\u0026gt; \u0026lt;label class=\"custom-control-label\" for=\"completed-{todo.id}\"\u0026gt;\u0026lt;/label\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; {/for} {/if} \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;      1 If the injected todos list is empty, display a placeholder row   2 Otherwise, iterate over the todos list and add a table row for each one   3 Table cell for title   4 Table cell for priority   5 Table cell for completion status, rendered as a checkbox    If you\u0026#8217;ve worked with other templating engine before, this will look very familiar to you. You can refer to injected objects and their properties to display their values, have conditional logic, iterate over collections etc. A very nice aspect about Qute templates is that they are processed at build time, following the Quarkus notion of \"compile-time boot\". This means if there is an error in a template such as unbalanced control keywords, you\u0026#8217;ll find out about this at build time instead of only at runtime.\n The reference documentation describes the syntax and all options in depth. Note that things are still in flux here, e.g. I couldn\u0026#8217;t work with boolean operators in conditions.\n   Combining HTML and Data APIs Thanks to HTTP content negotiation, you can easily combine resource methods for returning HTML and JSON for API-style consumers in a single endpoint. Just add another resource method for handling the required media type, e.g. \"application/json\":\n 1 2 3 4 5 6 @GET @Produces(MediaType.APPLICATION_JSON) @Consumes(MediaType.APPLICATION_JSON) public List\u0026lt;Todo\u0026gt; listTodosJson() { return Todo.findAll().list(); }    A standard HTTP request issued by a web browser would now be answered with the HTML page, whereas an AJAX request with the \"application/json\" accept header (or a manual invocation via curl) would yield the JSON representation. I really like that idea of considering HTML and JSON-based representations as two different \"views\" of the same API essentially.\n   Template Organization If a web application has multiple pages or \"views\", chances are there are many similarities between those. E.g. there might be a common header and footer for all pages, or one and the same form is used on multiple pages.\n To avoid duplication in the templates in such cases, Qute supports the notion of includes. E.g. let\u0026#8217;s say there\u0026#8217;s a common form for creating new and editing existing todos. This can be put into its own template:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 (1) \u0026lt;form action=\"/todo/{#if update}{todo.id}/edit{#else}new{/if}\" method=\"POST\" name=\"todoForm\" enctype=\"multipart/form-data\"\u0026gt; \u0026lt;div class=\"form-row align-items-center\"\u0026gt; \u0026lt;div class=\"col-sm-3 my-1\"\u0026gt; \u0026lt;label class=\"sr-only\" for=\"title\"\u0026gt;Title\u0026lt;/label\u0026gt; (2) \u0026lt;input type=\"text\" name=\"title\" class=\"form-control\" id=\"title\" placeholder=\"Title\" required autofocus {#if update}value=\"{todo.title}\"{/if}\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"col-auto my-1\"\u0026gt; \u0026lt;select class=\"custom-select\" name=\"priority\"\u0026gt; \u0026lt;option disabled value=\"\"\u0026gt;Priority\u0026lt;/option\u0026gt; {#for prio in priorities} \u0026lt;option value=\"{prio}\" {#if todo.priority == prio}selected{/if}\u0026gt;{prio}\u0026lt;/option\u0026gt; {/for} \u0026lt;/select\u0026gt; \u0026lt;/div\u0026gt; (3) {#if update} \u0026lt;div class=\"col-auto my-1\"\u0026gt; \u0026lt;div class=\"form-check\"\u0026gt; \u0026lt;input type=\"checkbox\" name=\"completed\" class=\"form-check-input\" id=\"completed\" {#if todo.completed}checked{/if}\u0026gt; \u0026lt;label class=\"form-check-label\" for=\"completed\"\u0026gt;Completed\u0026lt;/label\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {/if} (4) \u0026lt;button type=\"submit\" class=\"btn btn-primary\"\u0026gt;{#if update}Update{#else}Create{/if}\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt;      1 Post to different path for update and create   2 Display existing title and priority in case of an update   3 Show checkbox for completion status in case of an update   4 Choose button caption depending on use case    In order to display this form right under the table with all todos, the template can simply be included like so:\n 1 2 \u0026lt;h2\u0026gt;New Todo\u0026lt;/h2\u0026gt; {#include todo-form.html}{/include}    It\u0026#8217;s also possible to extract the outer shell of multiple pages into a shared template (\"template inheritance\"). This allows to extract common headers and footers into one single template with placeholders for the inner parts.\n For that, create a template with the common outer structure:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\"en\"\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\"utf-8\"\u0026gt; \u0026lt;!-- CSS ... --\u0026gt; \u0026lt;link rel=\"stylesheet\" href=\"...\"\u0026gt; \u0026lt;title\u0026gt;{#insert title}Default Title{/}\u0026lt;/title\u0026gt; (1) \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\"container\"\u0026gt; \u0026lt;h1\u0026gt;{#insert title}Default Title{/}\u0026lt;/h1\u0026gt; (1) {#insert contents}No contents!{/} (2) \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;      1 Derived templates define a section title which will be inserted here   2 Derived templates define a section contents which will be inserted here    Other templates can then extend the base one, e.g. like so for the \"Edit Todo\" page:\n 1 2 3 4 5 6 {#include base.html} (1) {#title}Edit Todo #{todo.id}{/title} (2) {#contents} (3) {#include todo-form.html}{/include} (4) {/contents} {/include}      1 Include the base template   2 Define the title section   3 Define the contents section   4 Include the template for displaying the todo form    As so often, a balance needs to be found between extracting common parts and still being able to comprehend the overall structure without having to pursue a large number of template references. But in any case with includes and inserts Qute puts the neccessary tools into your hands.\n   Error Handling For a great user experience robust error handling is a must. E.g. might happen that a user loads the \"Edit Todo\" dialog and while they\u0026#8217;re in the process of editing, that record gets deleted by someone else. When saving, a proper error message should be displayed to the first user. Here\u0026#8217;s the resource method implementation for that:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @POST @Consumes(MediaType.MULTIPART_FORM_DATA) @Transactional @Path(\"/{id}/edit\") public Object updateTodo( @PathParam(\"id\") long id, @MultipartForm TodoForm todoForm) { Todo loaded = Todo.findById(id); (1) if (loaded == null) { (2) return error.data(\"error\", \"Todo with id \" + id + \" has been deleted after loading this form.\"); } loaded = todoForm.updateTodo(loaded); (3) return Response.status(301) (4) .location(URI.create(\"/todo\")) .build(); }      1 Load the todo record to be updated   2 If it doesn\u0026#8217;t exist, render the \"error\" template   3 Otherwise, update the record; as loaded is an attached entity, no call to persist is needed   4 redirect the user to the main page, avoiding issues with reloading etc. (post-redirect-get pattern)    Note that TemplateInstance as returned from the Template#data() method doesn\u0026#8217;t extend the JAX-RS Response class. Therefore the return type of the method must be declared as Object in this case.\n   Search Thanks to Hibernate Panache it\u0026#8217;s quite simple to refine the todo list and only return those whose title matches a given search term. Also ordering the list in some meaningful way would be nice. All we need is an optional query parameter for specifying the search term and a custom query method:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @GET @Consumes(MediaType.TEXT_HTML) @Produces(MediaType.TEXT_HTML) public TemplateInstance listTodos(@QueryParam(\"filter\") String filter) { return todos.data(\"todos\", find(filter)); } @GET @Produces(MediaType.APPLICATION_JSON) @Consumes(MediaType.APPLICATION_JSON) public List\u0026lt;Todo\u0026gt; listTodosJson(@QueryParam(\"filter\") String filter) { return find(filter); } private List\u0026lt;Todo\u0026gt; find(String filter) { Sort sort = Sort.ascending(\"completed\") (1) .and(\"priority\", Direction.Descending) .and(\"title\", Direction.Ascending); if (filter != null \u0026amp;\u0026amp; !filter.isEmpty()) { (2) return Todo.find(\"LOWER(title) LIKE LOWER(?1)\", sort, \"%\" + filter + \"%\").list(); } else { return Todo.findAll(sort).list(); (3) } }      1 First sort by completion status, then priority, then by title   2 If a filter is given, apply the search term lower-cased and with wildcards, i.e. using a WHERE clause such as where lower(todo0_.title) like lower(%searchterm%)   3 Otherwise, return all todos    To enter the search term, a form is added next to the table of todos:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 (1) \u0026lt;form action=\"/todo\" method=\"GET\" name=\"search\"\u0026gt; \u0026lt;div class=\"form-row align-items-center\"\u0026gt; \u0026lt;div class=\"col-sm-3 my-1\"\u0026gt; \u0026lt;label class=\"sr-only\" for=\"filter\"\u0026gt;Search\u0026lt;/label\u0026gt; (2) \u0026lt;input type=\"text\" name=\"filter\" class=\"form-control\" id=\"filter\" placeholder=\"Search By Title\" required {#if filtered}value=\"{filter}\"{/if}\u0026gt; \u0026lt;/div\u0026gt; (3) \u0026lt;input class=\"btn btn-primary\" value=\"Search\" type=\"submit\"\u0026gt;\u0026amp;nbsp; \u0026lt;a class=\"btn btn-secondary {#if !filtered}disabled{/if}\" href=\"/todo\" role=\"button\"\u0026gt;Clear Filter\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt;      1 Invoke this page with the entered search as query parameter   2 Input for the search term; show the previously entered term, if any   3 A button for clearing the result list if a search term has been entered; otherwise the button will be disabled      Smoother User Experience via Unpoly The last thing I wanted to explore is how the usability and performance of the application can be improved by means of some client-side enhancements. By default, a web app rendered on the server-side like ours requires full page loads when going from one page to the other. This is where single page applications (SPAs) implemented with client-side frameworks shine: just parts of the document object model tree in the browser will be replaced e.g. when loading a result list via AJAX, resulting in a much smoother and faster user experience.\n Does this mean we have to give up on server-side rendering altogether if we\u0026#8217;re after this kind of UX? Luckily not, as small helper libraries such as Unpoly, Intercooler or Turbolinks can be leveraged to replace just page fragments instead of requiring full page loads. This results in a smooth SPA-like user experience without having to opt into the full client-side programming model. For the Todo example I\u0026#8217;ve obtained great results using Unpoly. After importing its JavaScript file, all that\u0026#8217;s needed is to add the up-target attribute to links or forms.\n E.g. here\u0026#8217;s the form for entering the search term with that modification:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 (1) \u0026lt;form action=\"/todo\" method=\"GET\" name=\"search\" up-target=\".container\"\u0026gt; \u0026lt;div class=\"form-row align-items-center\"\u0026gt; \u0026lt;div class=\"col-sm-3 my-1\"\u0026gt; \u0026lt;label class=\"sr-only\" for=\"filter\"\u0026gt;Search\u0026lt;/label\u0026gt; \u0026lt;input type=\"text\" name=\"filter\" class=\"form-control\" id=\"filter\" placeholder=\"Search By Title\" required {#if filtered}value=\"{filter}\"{/if}\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;input class=\"btn btn-primary\" value=\"Search\" type=\"submit\"\u0026gt;\u0026amp;nbsp; (2) \u0026lt;a class=\"btn btn-secondary {#if !filtered}disabled{/if}\" href=\"/todo\" role=\"button\" up-target=\".container\"\u0026gt;Clear Filter\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt;      1 When receiving the result of the form submission, replace the \u0026lt;div\u0026gt; with CSS class container of the current page with the one from the response   2 Do the same when following the \"Clear Filter\" link    The magic trick of Unpoly is that links and forms with the up-target attribute are intercepted by Unpoly and executed via AJAX calls. The specified fragments from the result page are then used to replace parts of the already loaded page, instead of having the browser load the full response page. The result is the fast user experience shown in the video above.\n Unpoly also allows to show page fragments in modal dialogs, allowing to remain on the same page also when showing forms such as the one for editing a todo:\n   Note that if JavaScript is disabled, the application gracefully falls back to full page loads. I.e. it will still be fully functional, just with a slightly degraded user experience. The same would happen when accessing the edit dialog directly via its URL or when opening the \"Edit\" link in a new tab or window:\n     Bonus: Using WebJars In a thread on Twitter James Ward brought up the idea of pulling in required resources such as Bootstrap via WebJars instead of getting them from a CDN. WebJars is a useful utility for obtaining all sorts of client-side libraries with Java build tools such as Maven or Gradle.\n For Bootstrap, the following dependency must be added to the Maven pom.xml file:\n 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.webjars\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;bootstrap\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.4.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;    The Bootstrap CSS can then be included within the base.html template like so:\n 1 2 3 4 5 6 7 ... \u0026lt;head\u0026gt; ... \u0026lt;link rel=\"stylesheet\" href=\"/webjars/bootstrap/4.4.1/css/bootstrap.min.css\"\u0026gt; ... \u0026lt;/head\u0026gt; ...    This is all that\u0026#8217;s needed in order to use Bootstrap via WebJars. Note this will work on the JVM and also with a native binary via GraalVM: WebJars resources are located under META-INF/resources, and Quarkus automatically adds all resources from there when building a native image.\n   Wrap Up This concludes my quick tour through server-side web applications with Quarkus and its new Qute extension. Where only web applications based on REST APIs called by client-side web applications were supported before, Qute is a great addition to the list of Quarkus extensions, allowing to choose different architecture styles based on your needs and preferences.\n Note that Qute currently is in \"Experimental\" state, i.e. it\u0026#8217;s a great time to give it a try and share your feedback, but be prepared for possible immaturities and potential changes down the road. E.g. I noticed that complex boolean expressions in template conditions aren\u0026#8217;t support yet. Also it would be great to get build-time feedback upon invalid variable references in templates.\n To learn more, refer to the Qute guide and its reference documentation. You can find the complete source code of the Todo example including instructions for building and running in this GitHub repo.\n  ","id":32,"publicationdate":"Jan 3, 2020","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eOne of the long-awaited features in Quarkus was support for server-side templating:\nuntil recently, Quarkus supported only client-side web frameworks which obtain there data by calling a REST API on the backend.\nThis has changed with \u003ca href=\"https://quarkus.io/blog/quarkus-1-1-0-final-released/\"\u003eQuarkus 1.1\u003c/a\u003e: it comes with a brand-new template engine named \u003ca href=\"https://quarkus.io/guides/qute\"\u003eQute\u003c/a\u003e,\nwhich allows to build web applications using server-side templates.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Quarkus Qute – A Test Ride","uri":"https://www.morling.dev/blog/quarkus-qute-test-ride/"},{"content":"As a software engineer, I like to automate tedious tasks as much as possible. The deployment of this website is no exception: it is built using the Hugo static site generator and hosted on GitHub Pages; so wouldn\u0026#8217;t it be nice if the rendered website would automatically be published whenever an update is pushed to its source code repository?\n With the advent of GitHub Actions, tasks like this can easily be implemented without having to rely on any external CI service. Instead, many ready-made actions can be obtained from the GitHub marketplace and easily be configured as per our needs. E.g. triggered by a push to a specified branch in a GitHub repository, they can execute tasks like project builds, tests and many others, running in virtual machines based on Linux, Windows and even macOS. So let\u0026#8217;s see what\u0026#8217;s needed for building a Hugo website and deploying it to GitHub Pages.\n GitHub Actions To the Rescue Using my favourite search engine, I came across two GitHub actions which do everything we need:\n   GitHub Actions for Hugo\n  GitHub Actions for GitHub Pages\n   There are multiple alternatives for GitHub Pages deployment. I chose this one basically because it seems to be the most popular one (as per number of GitHub stars), and because it\u0026#8217;s by the same author as the Hugo one, so they should nicely play together.\n   Registering a Deploy Key In order for the GitHub action to deploy the website, a GitHub deploy key must be registered.\n To do so, create a new SSH key pair on your machine like so:\n ssh-keygen -t rsa -b 4096 -C \"$(git config user.email)\" -f gh-pages -N \"\"   This will create two files, the public key (gh-pages.pub) and the private key (gh-pages). Go to https://github.com/\u0026lt;your-user-or-organisation\u0026gt;/\u0026lt;your-repo\u0026gt;/settings/keys and click \"Add deploy key\". Paste in the public part of your key pair and check the \"Allow write access\" box.\n Now go to https://github.com/\u0026lt;your-user-or-organisation\u0026gt;/\u0026lt;your-repo\u0026gt;/settings/secrets and click \"Add new secret\". Choose ACTIONS_DEPLOY_KEY as the name and paste the private part of your key pair into the \"Value\" field.\n The key will be stored in an encrypted way as per GitHub\u0026#8217;s documentation Nevertheless I\u0026#8217;d recommend to use a specific key pair just for this purpose, instead of re-using any other key pair. That way, impact will be reduced to this particular usage, should the private key get leaked somehow.\n   Defining the Workflow With the key in place, it\u0026#8217;s time to set up the actual GitHub Actions workflow. This is simply done by creating the file .github/workflows/gh-pages-deployment.yml in your repository with the following contents. GitHub Actions workflows are YAML files, because YOLO ;)\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 name: GitHub Pages on: (1) push: branches: - master jobs: build-deploy: runs-on: ubuntu-18.04 steps: - uses: actions/checkout@v1 (2) with: submodules: true - name: Install Ruby Dev (3) run: sudo apt-get install ruby-dev - name: Install AsciiDoctor and Rouge run: sudo gem install asciidoctor rouge - name: Setup Hugo (4) uses: peaceiris/actions-hugo@v2 with: hugo-version: '0.62.0' - name: Build (5) run: hugo - name: Deploy (6) uses: peaceiris/actions-gh-pages@v2 env: ACTIONS_DEPLOY_KEY: ${{ secrets.ACTIONS_DEPLOY_KEY }} PUBLISH_BRANCH: gh-pages PUBLISH_DIR: ./public      1 Run this action whenever changes are pushed to the master branch   2 The first step in the job: check out the source code   3 Install AsciiDoctor (in case you use Hugo with AsciiDoc files, like I do) and Rouge, a Ruby gem for syntax highlighting; I\u0026#8217;m installing the gems instead of Ubuntu packages in order to get current versions   4 Set up Hugo via the aforementioned GitHub Actions for Hugo   5 Run the hugo command; here you could add parameters such as -F for also building future posts   6 Deploy the website to GitHub pages; the contents of Hugo\u0026#8217;s build directory public will be pushed to the gh-pages branch of the upstream repository, using the deploy key configured before    And that\u0026#8217;s all we need; once the file is committed and pushed to the upstream repository, the deployment workflow will be executed upon each push to the master branch.\n You can find the complete workflow definition used for publishing this website here. Also check out the documentation of GitHub Actions for Hugo and GitHub Actions for GitHub Pages to learn more about their capabilities and the options they offer.\n  ","id":33,"publicationdate":"Dec 26, 2019","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eAs a software engineer, I like to automate tedious tasks as much as possible.\nThe deployment of this website is no exception:\nit is built using the \u003ca href=\"https://gohugo.io/\"\u003eHugo\u003c/a\u003e static site generator and hosted on \u003ca href=\"https://pages.github.com/\"\u003eGitHub Pages\u003c/a\u003e;\nso wouldn\u0026#8217;t it be nice if the rendered website would automatically be published whenever an update is pushed to its source code repository?\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Automatically Deploying a Hugo Website via GitHub Actions","uri":"https://www.morling.dev/blog/automatically-deploying-hugo-website-via-github-actions/"},{"content":"This page gives an overview over some talks I have done over the last years. I have spoken at large conferences such as QCon San Francisco, Devoxx and JavaOne, local meet-ups as well as company-internal events, covering topics such as Debezium and Change Data Capture, Bean Validation, NoSQL and more.\n If you\u0026#8217;d like to have me as a speaker at your conference or meet-up, please get in touch.\n 2020   JokerConf (online): Change data capture pipelines with Debezium and Kafka Streams\n  Virtual JUG (joint presentation with Andres Almiray; online): Plug-in Architectures With Layrry and the Java Module System (recording, slides)\n  QConPlus (online): Serverless Search for My Blog With Java, Quarkus, \u0026amp; AWS Lambda\n  JFall (joint presentation with Andres Almiray; online): Plug-in Architectures for Java With Layrry and the Java Module System\n  Java Day Istanbul (online): Change Data Streaming Use Cases With Apache Kafka \u0026amp; Debezium\n  Great International Developer Summit (online): Change Data Capture Pipelines with Debezium and Kafka Streams\n  Kafka Summit (online): Change Data Capture Pipelines with Debezium and Kafka Streams\n  Red Hat Summit Virtual Experience: Data integration patterns for microservices with Debezium and Apache Kafka\n     2019   Nordic Coding, Kiel: Quarkus - Supersonic Subatomic Java\n  Java User Group Paderborn: Change Data Streaming Use Cases mit Debezium und Apache Kafka\n  QCon San Francisco: Practical Change Data Streaming Use Cases With Apache Kafka \u0026amp; Debezium\n  JokerConf, St. Petersburg: Practical change data streaming use cases with Apache Kafka and Debezium\n  JavaZone, Oslo: Change Data Streaming For Microservices With Apache Kafka and Debezium\n  MicroXchg, Berlin: Change Data Streaming Patterns For Microservices With Debezium\n  JavaLand, Brühl\n  Change Data Streaming für Microservices mit Debezium\n  Das Annotation Processing API - Use Cases und Best Practices\n     RivieraDev, Sophia Antipolis: Practical Change Data Streaming Use Cases With Apache Kafka and Debezium\n  Kafka Summit London: Change Data Streaming Patterns For Microservices With Debezium\n  Red Hat Summit, Boston\n  Bridging microservice boundaries with Apache Kafka and Debezium (hands-on lab)\n  Change data streaming patterns for microservices with Debezium\n     Red Hat Modern Integration and Application Development Day, Milano: Data Strategies for Microservices: Change Data Capture with Debezium\n     2018   Devoxx Morocco, Marrakesh\n  Change Data Streaming Patterns for Microservices With Debezium\n  Map me if you can! Painless bean mappings with MapStruct\n     Kafka Summit San Francisco: Change Data Streaming Patterns for Microservices With Debezium\n  VoxxedDays Microservices Paris: Data Streaming for Microservices using Debezium\n  JUG Saxony Day, Dresden: Streaming von Datenbankänderungen mit Debezium\n  Java User Group Darmstadt: Streaming von Datenbankänderungen mit Debezium\n  JavaLand, Brühl: Hibernate - State of the Union; Migrating to Java 9 Modules with ModiTect\n  RivieraDev, Sophia Antipolis: Data Streaming for Microservices using Debezium\n  Red Hat Summit, San Francisco: Running data-streaming applications with Kafka on OpenShift (hands-on lab)\n  Java User Group Münster, Streaming von Datenbankänderungen mit Debezium\n     2017   JavaZone, Oslo: Keeping Your Data Sane with Bean Validation 2.0\n  JavaOne, San Francisco\n  Keeping Your Data Sane with Bean Validation 2.0\n  NoSQL? Have it Your Way!\n     Devoxx Belgium, Antwerp\n  Streaming Database Changes with Debezium\n  Short talks on Bean Validation 2.0 and MapStruct\n     jdk.io, Copenhagen: Keeping Your Data Sane with Bean Validation 2.0\n  RivieraDev, Sophia Antipolis: Keeping Your Data Sane with Bean Validation 2.0\n  JavaLand, Brühl\n  Bean Validation 2.0\n  Hibernate Search and Elasticsearch\n        2016   JavaZone, Oslo: From Hibernate to Elasticsearch in no time\n     ","id":34,"publicationdate":"Dec 26, 2019","section":"","summary":"This page gives an overview over some talks I have done over the last years. I have spoken at large conferences such as QCon San Francisco, Devoxx and JavaOne, local meet-ups as well as company-internal events, covering topics such as Debezium and Change Data Capture, Bean Validation, NoSQL and more.\n If you\u0026#8217;d like to have me as a speaker at your conference or meet-up, please get in touch.\n 2020   JokerConf (online): Change data capture pipelines with Debezium and Kafka Streams","tags":null,"title":"Conferences","uri":"https://www.morling.dev/conferences/"},{"content":"I had the pleasure to do a few podcasts and interviews, mostly around Debezium and change data capture.\n   The InfoQ Podcast, with Wes Reisz: Gunnar Morling on Change Data Capture and Debezium\n  Data Engineering Podcast by Tobias Macey: Episode 114\u0026#8201;\u0026#8212;\u0026#8201;Change Data Capture For All Of Your Databases With Debezium; together with Randall Hauch\n  Adam Bien\u0026#8217;s airhacks.fm podcast: Episode 39\u0026#8201;\u0026#8212;\u0026#8201;Use the Most Productive Stack You Can Get\n  Adam Bien\u0026#8217;s airhacks.fm podcast: Episode 57\u0026#8201;\u0026#8212;\u0026#8201;CDC, Debezium, streaming and Apache Kafka\n  Streaming Audio: a Confluent podcast about Apache Kafka: Change Data Capture with Debezium ft. Gunnar Morling\n  Interview with Thorben Janssen for heise.de (German): Im Gespräch: Gunnar Morling über Debezium und CDC\n  Thoughts On Java: Interview with Gunnar Morling\n   ","id":35,"publicationdate":"Dec 26, 2019","section":"","summary":"I had the pleasure to do a few podcasts and interviews, mostly around Debezium and change data capture.\n   The InfoQ Podcast, with Wes Reisz: Gunnar Morling on Change Data Capture and Debezium\n  Data Engineering Podcast by Tobias Macey: Episode 114\u0026#8201;\u0026#8212;\u0026#8201;Change Data Capture For All Of Your Databases With Debezium; together with Randall Hauch\n  Adam Bien\u0026#8217;s airhacks.fm podcast: Episode 39\u0026#8201;\u0026#8212;\u0026#8201;Use the Most Productive Stack You Can Get","tags":null,"title":"Podcasts and Interviews","uri":"https://www.morling.dev/podcasts/"},{"content":"I have contributed to a wide range of open-source projects over the last years. Here\u0026#8217;s a selection of projects I have been involved with.\n  Debezium  Debezium is a platform for change data capture; it lets you stream changes out of different databases such as Postgres, MySQL, MongoDB and SQL Server into Apache Kafka. I am the current lead of the Debezium project.\n Quarkus  Quarkus is a \"Kubernetes Native Java stack tailored for OpenJDK HotSpot and GraalVM, crafted from the best of breed Java libraries and standards\". My contributions to Quarkus are centered around its extension for Kafka Streams, which I initially created.\n Bean Validation and Hibernate Validator  Bean Validation is a Java specification which lets you express constraints on object models via annotations. Originally developed at the JCP, it\u0026#8217;s now part of the Jakarta EE umbrella at the Eclipse foundation. I have been the spec lead for Bean Validation 2.0 (JSR 380) and the lead of the reference implementation Hibernate Validator.\n Other Hibernate Projects  As part of the Hibernate team, I\u0026#8217;ve contributed to different projects such as Hibernate OGM (an effort to access NoSQL stores with JPA), Hibernate Search (full-text search for domain models based on Apache Lucene and Elasticsearch) and Hibernate ORM.\n MapStruct  MapStruct is a compile-time code generator for bean-to-bean mappings. Based on annotated Java interfaces, MapStruct generates mapping code that it is fully type-safe and very efficient by avoiding any usage of reflection. I was the creator and initial project lead of MapStruct.\n Deptective and ModiTect  ModiTect is a family of Maven and Gradle plug-ins around the Java Module System, e.g. for creating module descriptors and building modular runtime images via jlink. Deptective is a plug-in for the Java compiler (javac) for enforcing package dependencies within Java projects based on a declarative architecture definition.\n   ","id":36,"publicationdate":"Dec 26, 2019","section":"","summary":"I have contributed to a wide range of open-source projects over the last years. Here\u0026#8217;s a selection of projects I have been involved with.\n  Debezium  Debezium is a platform for change data capture; it lets you stream changes out of different databases such as Postgres, MySQL, MongoDB and SQL Server into Apache Kafka. I am the current lead of the Debezium project.\n Quarkus  Quarkus is a \"","tags":null,"title":"Projects","uri":"https://www.morling.dev/projects/"},{"content":"It has been quite a while since the last post on my old personal blog; since then, I\u0026#8217;ve mostly focused on writing about my day-work on the Debezium blog as well as some posts about more general technical topics on the Hibernate team blog.\n Now recently I had some ideas for things I wanted to write about, which didn\u0026#8217;t feel like a good fit for neither of those two. So it was time to re-boot a personal blog. The previous Blogger based one really, really feels outdated by now. Plus, I also wanted to have more control over how things work, and also be able to publish a list of projects I work on, conference talks I gave etc. So I decided to build the site using Hugo, a static site generator, and also use a nice new shiny dev domain. And here we are, welcome to morling.dev!\n Stay tuned for more posts every now and then about anything related to open source, the projects I work on and software engineering in general. Onwards!\n","id":37,"publicationdate":"Dec 26, 2019","section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIt has been quite a while since the last post on my old \u003ca href=\"http://musingsofaprogrammingaddict.blogspot.com/\"\u003epersonal blog\u003c/a\u003e;\nsince then, I\u0026#8217;ve mostly focused on writing about my day-work on the \u003ca href=\"https://debezium.io/blog/\"\u003eDebezium blog\u003c/a\u003e as well as \u003ca href=\"https://in.relation.to/gunnar-morling/\"\u003esome posts\u003c/a\u003e about more general technical topics on the Hibernate team blog.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Time for a New Blog","uri":"https://www.morling.dev/blog/time-for-new-blog/"},{"content":"I am an open-source software engineer, currently working for Red Hat, where I am leading the Debezium project, a platform for change data capture.\n I have been a long-time committer to multiple open-source projects, including Hibernate, MapStruct and Deptective; I also serve as the spec lead for Bean Validation 2.0 (first at the JCP, now under the Jakarta EE umbrella at the Eclipse Foundation).\n Named a Java Champion, I\u0026#8217;m regularly speaking at conferences such as QCon, JavaOne, Red Hat Summit, JavaZone, JavaLand and Kafka Summit.\n Occasionally, I blog about topics related to software engineering.\n ","id":38,"publicationdate":"Dec 25, 2019","section":"","summary":"I am an open-source software engineer, currently working for Red Hat, where I am leading the Debezium project, a platform for change data capture.\n I have been a long-time committer to multiple open-source projects, including Hibernate, MapStruct and Deptective; I also serve as the spec lead for Bean Validation 2.0 (first at the JCP, now under the Jakarta EE umbrella at the Eclipse Foundation).\n Named a Java Champion, I\u0026#8217;m regularly speaking at conferences such as QCon, JavaOne, Red Hat Summit, JavaZone, JavaLand and Kafka Summit.","tags":null,"title":"About Me","uri":"https://www.morling.dev/about/"},{"content":"","id":39,"publicationdate":"Jan 1, 0001","section":"categories","summary":"","tags":null,"title":"Categories","uri":"https://www.morling.dev/categories/"}]